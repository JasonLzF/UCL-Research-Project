{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modified Cooperative Q-learning.ipynb","provenance":[{"file_id":"1cTDMctxpmgfrN9wWHsNRaGv0Y3Ilwnfe","timestamp":1600983517105},{"file_id":"1W_FZiv7NEwXfXaQ3qJs8vVVN0eV0lmyq","timestamp":1597088951231},{"file_id":"14izCxycL3B_SO_r0pF4ULR-mYga4iCpi","timestamp":1597064144116},{"file_id":"13vfEk6YHAm8qWm0MSGDSyR2Ed646w5oU","timestamp":1596973168562},{"file_id":"1DFXV7ESPJfkbKLwC0ZPQn-K_xxEWjj3H","timestamp":1596900183221},{"file_id":"16Hs7pdOIIJ7ShXQmXquFh9WiwPN0do3P","timestamp":1595085101986},{"file_id":"1qkQcyKHLkyk-wcnCebXBHSkK5U5w3eU3","timestamp":1593788872734},{"file_id":"1ZlCvGgwjBrxD-10sZZljGv0s4bx8vYe6","timestamp":1593715134610}],"collapsed_sections":[],"authorship_tag":"ABX9TyNBi4CmB93le4MlXPJfGTJF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Gjm30orCxyTb","colab_type":"code","colab":{}},"source":["import numpy as np\n","from numpy import linalg as LA\n","import random\n","from collections import defaultdict\n","from functools import partial\n","import math\n","import copy\n","\n","# state of each agent (UAV)\n","class LocalState():\n","    def __init__(self, k, l, pos, velocity, r_c, r_s, num_agents, action_num):\n","        self.k = k\n","        self.l = l\n","        self.pos = pos\n","        self.velocity = velocity\n","        self.acc = 0\n","        self.r_c = r_c\n","        self.r_s = r_s\n","        self.action_num = action_num\n","        self.neighbours = []\n","        self.action_n = []\n","        \n","        self.gamma_map = np.zeros(((int)(k), (int)(l))) # coverage map\n","        self.prev_gamma_map = []\n","        self.prev_target_gamma_idx = {}\n","        self.target_gamma_idx = {} # indice of agents' gamma points\n","        for i in range(num_agents):\n","            # if i == agent_id:\n","            #     continue\n","            self.target_gamma_idx[i] = [-1, -1]\n","\n","        self.Q = defaultdict(partial(np.zeros, self.action_num)) # Q table\n","        # self.Q = defaultdict(partial(np.random.rand * 0.1, self.action_num))\n","        self.Qts = {} # mappingm between state-action and the mapping of next state and corresponding action space\n","\n","        self.tra_map = np.zeros(((int)(k), (int)(l)))\n","        self.untraversed_idx = None\n","        self.done = False\n","        self.prev_global_map = []\n","        self.prev_count_map = []\n","        \n","\n","class Environment():\n","    def __init__(self, m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents):\n","        self.r_c = r_c\n","        self.r_s = r_s\n","        self.d = d\n","        self.h = h\n","        self.w = w\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.c_alpha = c_alpha\n","        self.c1_gamma = c1_gamma\n","        self.c2_gamma = c2_gamma\n","        self.c1_T = c1_T\n","        self.c2_T = c2_T\n","        self.r_ref = r_ref\n","        self.lambda_ = lambda_\n","        self.t_step = t_step\n","        self.c_r = c_r\n","        self.v_max = v_max\n","        self.m = m   # length\n","        self.n = n   # width\n","        self.k = (int)(np.ceil(n / (np.sqrt(2) * self.r_s)))   # rows\n","        self.l = (int)(np.ceil(m / (np.sqrt(2) * self.r_s)))   # columns\n","        self.num_agents = num_agents\n","        self.action_num = action_num\n","        self.epsilon = epsilon   # range to decide if target pos is reached\n","        self.T = 0.3\n","        self.action_gamma_point_mapping = {\n","              1: [0, 0],\n","              2: [0, 1],\n","              3: [-1, 1],\n","              4: [-1, 0],\n","              5: [-1, -1],\n","              6: [0, -1],\n","              7: [1, -1],\n","              8: [1, 0],\n","              9: [1, 1]\n","        }\n","        self.count_map = np.zeros(((int)(self.k), (int)(self.l)))\n","        hrz = ((self.l - 1) * self.m * self.k) / (self.l * self.v_max) + ((self.k - 1) * self.n) / (self.k * self.v_max)\n","        vtc = ((self.k - 1) * self.n * self.l) / (self.k * self.v_max) + ((self.l - 1) * self.m) / (self.l * self.v_max)\n","        self.T_min = np.min([hrz, vtc]) / self.num_agents\n","        self.uavs = [LocalState(self.k, self.l, None, None, self.r_c, self.r_s, self.num_agents, self.action_num) for i in range(self.num_agents)]\n","        self.uavs_pos = []\n","        uavs_gamma_p = [[-1, -1] for _ in range(self.num_agents)]\n","        # ids = [[1, 6], [3, 6], [6, 1]] # untrained_50_170.8\n","        ids = [[6, 3], [5, 1], [1, 6]] # trained_50_142.3 \n","        # ids = [[2, 4], [2, 1], [4, 5]] # trained_50_143.3_1rpt_1diag\n","        for i in range(self.num_agents):\n","            is_same = True\n","            while is_same:\n","                is_same = False\n","                # index = random.randint(1, num_cells - 2)\n","                # row = random.randint(1, self.k - 2)\n","                # col = random.randint(1, self.k - 2)\n","                row = ids[i][0]\n","                col = ids[i][1]\n","                for ag_id in range(self.num_agents):\n","                    if ag_id == i:\n","                        continue\n","                    if (row == uavs_gamma_p[ag_id][0]) and (col == uavs_gamma_p[ag_id][1]):\n","                        is_same = True\n","                        break\n","            uavs_gamma_p[i] = [row, col]\n","            print([row, col])\n","            # self.uavs_pos.append(np.array(self.index_to_pos([row, col])) + np.array([0.5 + np.random.rand(1) / 2, 0.5 + np.random.rand(1) / 2]))\n","            self.uavs_pos.append(np.array(self.index_to_pos([row, col])) + np.array([0.5, 1]))\n","            # self.uavs_pos.append(np.array(self.index_to_pos([row, col])))\n","\n","        self.crt_min_T = 100000000\n","        self.prev_T = 0\n","        self.rpt_T_count = 0\n","        self.divert_points = []\n","        self.update_ac_counts = np.zeros(self.num_agents)\n","\n","        self.cac_xl = 1\n","        self.cac_yl = 1 \n","        self.cac_map = np.zeros(((int)(self.n / self.cac_yl), (int)(self.m / self.cac_xl)))\n","  \n","    def index_to_pos(self, index):\n","        x_l = self.m / self.l\n","        y_l = self.n / self.k\n","        pos_x = (index[1] + 0.5) * x_l\n","        pos_y = self.n - (index[0] + 0.5) * y_l   \n","\n","        return [pos_x, pos_y]\n","\n","    def index_to_pos_gnr(self, index):\n","        # x_l = self.m / self.l\n","        # y_l = self.n / self.k\n","        pos_x = (index[1] + 0.5) * self.cac_xl\n","        pos_y = self.n - (index[0] + 0.5) * self.cac_yl   \n","\n","        return [pos_x, pos_y]\n","\n","    def update_cac_map(self):\n","        for i in range(self.num_agents):\n","            cur_uav = self.uavs[i]\n","            for j in range(self.cac_map.shape[0]):\n","                for k in range(self.cac_map.shape[1]):\n","                    if LA.norm(np.array(cur_uav.pos) - np.array(self.index_to_pos_gnr([j, k]))) <= self.r_s:  \n","                        self.cac_map[j, k] = 1\n","\n","        return np.sum(self.cac_map[:, :]) / (self.cac_map.shape[0] * self.cac_map.shape[1])\n","\n","    def pos_to_index(self, pos):\n","        x_l = self.m / self.l\n","        y_l = self.n / self.k\n","        # col = int(pos[0] / x_l - 0.5)\n","        # row = int((self.n - pos[1]) / y_l - 0.5)\n","        col = pos[0] / x_l - 0.5\n","        row = (self.n - pos[1]) / y_l - 0.5\n","\n","        return [row, col]\n","\n","    def update_Qts(self, agent_id, obs, next_obs, action, action_space):\n","        cur_uav = self.uavs[agent_id]\n","        if obs in cur_uav.Qts:\n","            cur_uav.Qts[obs][action] = {}\n","            cur_uav.Qts[obs][action][next_obs] = action_space\n","        else:\n","            cur_uav.Qts[obs] = [{} for _ in range(self.action_num)]\n","            cur_uav.Qts[obs][action] = {}\n","            cur_uav.Qts[obs][action][next_obs] = action_space\n","\n","    def MAS_update(self, agent_id, action):\n","        agent_ids = list(range(self.num_agents))\n","        del agent_ids[agent_id]\n","        # print(agent_ids)\n","        cur_uav = self.uavs[agent_id]\n","\n","        # target_gamma_idx = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","        # if is_new_target:\n","        #     cur_uav.velocity = [0, 0]\n","        if action != 1:\n","            cur_uav.target_gamma_idx[agent_id] = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","            # cur_uav.velocity = [0, 0]\n","        # print(\"uav \", agent_id, \" \", cur_uav.target_gamma_idx[agent_id])\n","\n","        # update acceleration\n","        cq_alpha = 0\n","        for id in agent_ids:\n","            delta_pos = np.array(self.uavs[id].pos) - np.array(cur_uav.pos)\n","            delta_pos_mag = LA.norm(delta_pos)\n","            if delta_pos_mag <= self.r_c:\n","                z_sigma = np.sqrt(1 + np.square(delta_pos_mag)) - 1\n","                z_p = z_sigma / self.d\n","                if z_p >= 0 and z_p < self.h:\n","                    p_h = 1\n","                elif z_p >= self.h and z_p < 1:\n","                    p_h = 1.0 / 2.0 * (1.0 + math.cos(math.pi * (z_p - self.h) / (1 - self.h)))\n","                else:\n","                    p_h = 0\n","                d_sigma = np.sqrt(1 + np.square(self.d)) - 1\n","                p_sigma = z_sigma - d_sigma\n","                phi_r = p_sigma / np.sqrt(1 + np.square(p_sigma)) - 1\n","                phi = p_h * phi_r\n","                sigma = delta_pos / np.sqrt(1 + np.square(delta_pos_mag))\n","                cq_alpha += phi * sigma\n","                # print(\"agent \", id, \" repulsive force = \", phi * sigma)\n","        cq_alpha = self.c_alpha * cq_alpha\n","        target_gamma_pos = self.index_to_pos(cur_uav.target_gamma_idx[agent_id])\n","        cq_gamma = -self.c1_gamma * (np.array(cur_uav.pos) - np.array(target_gamma_pos)) - self.c2_gamma * np.array(cur_uav.velocity)\n","        cq = cq_alpha + cq_gamma\n","        # cq = cq_gamma\n","        # print(\"cq_alpha = \", cq_alpha)\n","        # print(\"cq_gamma = \", cq_gamma)\n","\n","        # print(\"target_gamma_pos =\", target_gamma_pos)\n","        # print(\"mag of cq_alpha = \", LA.norm(cq_alpha))\n","        # print(\"mag of cq_gamma = \", LA.norm(cq_gamma))\n","        # print(\"mag of cq = \", LA.norm(cq))\n","\n","        # print(\"cur_uav.pos = \", cur_uav.pos)\n","        # print(\"target_gamma_pos = \", target_gamma_pos)\n","        # print(\"cur_uav.velocity = \", cur_uav.velocity)\n","\n","        # print(\"mag of cur_uav.velocity = \", LA.norm(cur_uav.velocity))\n","\n","        cur_uav.acc = cq\n","        # if LA.norm(cq) < 0.00000001:\n","        #     cq = cq_gamma\n","\n","        # update velocity and position\n","        prev_vel = np.array(cur_uav.velocity)\n","        cur_uav.velocity = np.array(cur_uav.velocity) + self.t_step * cq\n","        v_mag = LA.norm(cur_uav.velocity)\n","        if v_mag > self.v_max:\n","            cur_uav.velocity *= self.v_max / v_mag\n","        # cur_uav.pos = np.array(cur_uav.pos) + self.t_step * np.array(cur_uav.velocity)\n","        cur_uav.pos = np.array(cur_uav.pos) + self.t_step * (prev_vel + np.array(cur_uav.velocity)) / 2\n","\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[0].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[1].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[2].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(target_gamma_pos)))\n","\n","        # print(a)\n","            # print(\"target is changed\")\n","        # self.T += self.t_step\n","\n","    def is_target_reached(self, agent_id):\n","        cur_uav = self.uavs[agent_id]\n","        target_pos = self.index_to_pos(cur_uav.target_gamma_idx[agent_id])\n","\n","        return LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) <= self.epsilon\n","\n","    def fuse_maps(self, agent_id, action_n, prev_action_n):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.neighbours = []\n","        for i in range(self.num_agents):\n","            if i == agent_id:\n","                continue\n","            else:\n","                if LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[i].pos)) <= self.r_c: \n","                    cur_uav.neighbours.append(i)\n","                    cur_uav.gamma_map[:, :] = np.add(cur_uav.gamma_map[:, :], self.uavs[i].gamma_map[:, :])\n","                    if action_n[i] == 1:\n","                        x = self.uavs[i].target_gamma_idx[i][0] - self.action_gamma_point_mapping[prev_action_n[i]][0]\n","                        y = self.uavs[i].target_gamma_idx[i][1] - self.action_gamma_point_mapping[prev_action_n[i]][1]\n","                    else:\n","                        x = self.uavs[i].target_gamma_idx[i][0]\n","                        y = self.uavs[i].target_gamma_idx[i][1]\n","                    cur_uav.target_gamma_idx[i] = np.array([x, y])\n","                    # cur_uav.target_gamma_idx[i] = self.uavs[i].target_gamma_idx[i]\n","                    # print(\"neighbour \", i, \" is found\")\n","                    # print(\"communicate\")\n","                    # print(c_maps[i])\n","                    # print(cur_uav.map[:, :, 1])\n","                else:\n","                    cur_uav.target_gamma_idx[i] = [-1, -1]\n","        cur_uav.gamma_map[:, :] = cur_uav.gamma_map[:, :] != 0\n","\n","        target_gamma_idx = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action_n[agent_id]])\n","        # cur_uav.target_gamma_idx[agent_id] = target_gamma_idx\n","        target_pos = self.index_to_pos(target_gamma_idx)\n","        is_target_reached = LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) <= self.epsilon\n","        # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","        # if is_target_reached or (LA.norm(cur_uav.acc) < 0.00001 and LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) < 0.5):\n","        if is_target_reached:\n","            # cur_uav.gamma_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] = 1\n","            # # cur_uav.tra_map = np.zeros((self.k, self.l))\n","            # cur_uav.tra_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] = np.amax(cur_uav.tra_map) + 1\n","            # self.count_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] += 1\n","            cur_uav.gamma_map[target_gamma_idx[0], target_gamma_idx[1]] = 1\n","            # cur_uav.tra_map = np.zeros((self.k, self.l))\n","            cur_uav.tra_map[target_gamma_idx[0], target_gamma_idx[1]] = np.amax(cur_uav.tra_map) + 1\n","            self.count_map[target_gamma_idx[0], target_gamma_idx[1]] += 1\n","            # print()\n","            # is_target_reached = True\n","                # print(\"target is reached\")\n","\n","        return repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","\n","    def update_target_gamma_point(self, agent_id, action):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.target_gamma_idx[agent_id] = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","    \n","    def update_Q_value(self, agent_id, obs, next_obs, action, reward, ep_reward, ep_step, done, last, action_space):\n","        # print(\"update_Q_value\")\n","        cur_uav = self.uavs[agent_id]\n","        s_actions = [3, 5, 7, 9]\n","        prev_Q = cur_uav.Q[obs][action] \n","        use_ngb = False\n","        # if obs not in cur_uav.Q:\n","        #   #  cur_uav.Q[obs] = [a[i] for i in b]\n","        #     for act in s_actions:  \n","        #         cur_uav.Q[obs][act - 1] = -0.3 \n","        # done = self.is_episode_finished()\n","        if done:\n","            it_q = cur_uav.Q[obs][action] + self.alpha * (reward - cur_uav.Q[obs][action])\n","            sum_ngb_q = 0\n","            for ngb in cur_uav.neighbours:\n","                sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward - self.uavs[ngb].Q[obs][action])\n","            # new_Q = self.w * it_q + self.w * sum_ngb_q\n","            new_Q = it_q\n","            # if last:\n","            #     # print(\"uav \", agent_id)\n","            #     # print(\"it_q = \", it_q )\n","            #     # print(\"cur_uav.Q[obs][action] = \", cur_uav.Q[obs][action])\n","            #     # if new_Q > self.r_ref:\n","            #     #     print(\"reward = \", reward)\n","            #     # print(\"action \", action)\n","            #     # if new_Q < 0:\n","            #     #     cur_uav.Q[obs][action] = new_Q\n","            #     if new_Q > cur_uav.Q[obs][action]:\n","            #         cur_uav.Q[obs][action] = new_Q\n","            # else:\n","            #     cur_uav.Q[obs][action] = new_Q\n","            cur_uav.Q[obs][action] = new_Q  \n","            # cur_uav.Q[obs][action] = it_q\n","        else:\n","            # if next_obs not in cur_uav.Q:\n","            # #  cur_uav.Q[obs] = [a[i] for i in b]\n","            #     for act in s_actions:  \n","            #         cur_uav.Q[next_obs][act - 1] = -0.3 \n","            # next_obs = repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","            # print(\"inside\")\n","            # print(repr([cur_uav.gamma_map, cur_uav.target_gamma_idx]))\n","            next_Qs = np.array([cur_uav.Q[next_obs][action - 1] for action in action_space])\n","            it_q = cur_uav.Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(next_Qs) - cur_uav.Q[obs][action])\n","            # print(\"it_q = \", it_q)\n","            sum_ngb_q = 0 \n","            for ngb in cur_uav.neighbours:\n","                # use_ngb = True\n","                # agents = [id_ for id_ in range(self.num_agents)] \n","                # tmp_target_idx = copy.deepcopy(cur_uav.target_gamma_idx)\n","                # tmp_idx = np.copy(tmp_target_idx[agent_id])\n","                # tmp_target_idx[agent_id] = np.copy(self.uavs[ngb].target_gamma_idx[ngb])\n","                # tmp_target_idx[ngb] = tmp_idx\n","                # tmp_obs_ = []\n","                # tmp_obs_.append(repr([cur_uav.prev_gamma_map, tmp_target_idx]))\n","                # agents = np.delete(agents, [agent_id, ngb])\n","                # tmp_idx = np.copy(tmp_target_idx[agent_id])\n","                # tmp_target_idx[agents[0]] = tmp_idx\n","                # tmp_target_idx[agent_id] = np.copy(self.uavs[ngb].target_gamma_idx[ngb])\n","                # tmp_obs_.append(repr([cur_uav.prev_gamma_map, tmp_target_idx]))\n","                # for tmp_obs in tmp_obs_:\n","                #     if tmp_obs in self.uavs[ngb].Qts:\n","                #         if self.uavs[ngb].Qts[tmp_obs][action]:\n","                #             ngb_next_obs = list(self.uavs[ngb].Qts[tmp_obs][action].keys())[0]\n","                #             ngb_action_space = self.uavs[ngb].Qts[tmp_obs][action][ngb_next_obs]\n","                #             next_ngb_Qs = np.array([self.uavs[ngb].Q[ngb_next_obs][action - 1] for action in ngb_action_space])\n","                #             use_ngb = True\n","                #             sum_ngb_q += self.uavs[ngb].Q[tmp_obs][action] + self.alpha * (reward + self.lambda_ * np.max(next_ngb_Qs) - self.uavs[ngb].Q[tmp_obs][action])\n","\n","                # action = cur_uav.action_n[ngb] - 1\n","                if obs in self.uavs[ngb].Qts:\n","                    if self.uavs[ngb].Qts[obs][action]:\n","                        ngb_next_obs = list(self.uavs[ngb].Qts[obs][action].keys())[0]\n","                        ngb_action_space = self.uavs[ngb].Qts[obs][action][ngb_next_obs]\n","                        next_ngb_Qs = np.array([self.uavs[ngb].Q[ngb_next_obs][action - 1] for action in ngb_action_space])\n","                        use_ngb = True\n","                        sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(next_ngb_Qs) - self.uavs[ngb].Q[obs][action])\n","\n","                #     else:\n","                #         next_ngb_Qs = [0]\n","                # else:\n","                #     next_ngb_Qs = [0]\n","                # #       use_ngb = True\n","                # #       sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(next_ngb_Qs) - self.uavs[ngb].Q[obs][action])\n","                # reward, next_x, next_y = self.v_reward_fn(agent_id, ngb)\n","\n","                # sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(next_ngb_Qs) - self.uavs[ngb].Q[obs][action])\n","            # print(\"neighbours \", cur_uav.neighbours)\n","            # print(\"sum_ngb_q = \", sum_ngb_q)\n","            # use_ngb = False\n","            if use_ngb:\n","                new_Q = self.w * it_q + self.w * sum_ngb_q\n","                # print(\"use_ngb\")\n","            else:\n","                new_Q = it_q\n","            # print(\"prev_Q = \", prev_Q)\n","            # print(\"new_Q = \", new_Q)\n","            if last:\n","                # print(\"uav \", agent_id)\n","                # print(\"it_q = \", it_q )\n","                # print(\"cur_uav.Q[obs][action] = \", cur_uav.Q[obs][action])\n","                # if new_Q > self.r_ref:\n","                #     print(\"reward = \", reward)\n","                #     print(\"np.max(cur_uav.Q[next_obs]) = \", np.max(cur_uav.Q[next_obs]))\n","                # print(\"action \", action)\n","                # if new_Q < 0:\n","                #     cur_uav.Q[obs][action] = new_Q\n","                # cur_uav.Q[obs][action] > 0 and \n","\n","                # if ep_step == 1:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif new_Q > cur_uav.Q[obs][action]:\n","                #     cur_uav.Q[obs][action] = new_Q\n","\n","                # if ep_step == 1:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif ep_reward >= np.max(cur_uav.Q[next_obs]):\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif ep_reward < np.max(cur_uav.Q[next_obs]):\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # if (cur_uav.Q[obs][action] > 0) and (new_Q > cur_uav.Q[obs][action]):\n","                #     cur_uav.Q[obs][action] = new_Q\n","                # elif cur_uav.Q[obs][action] == 0:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","\n","                num_forward_branches = np.count_nonzero(cur_uav.Q[next_obs])\n","                # ep_reward += reward\n","                if num_forward_branches > 1:\n","                    if ep_step == 1:\n","                        if new_Q > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = new_Q\n","                    else:\n","                        if ep_reward > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = ep_reward\n","                else:\n","                    if ep_step == 1:\n","                        if new_Q > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = new_Q\n","                    else:\n","                        if ep_reward > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = ep_reward\n","            else:\n","                cur_uav.Q[obs][action] = new_Q \n","            # cur_uav.Q[obs][action] = new_Q\n","            # cur_uav.Q[obs][action] = self.w * it_q + self.w * sum_ngb_q   \n","            # cur_uav.Q[obs][action] = it_q\n","        # print(\"Q = \", cur_uav.Q[obs])     \n","        # print(\"minimum Q = \", np.min(cur_uav.Q[obs]))\n","        # if next_obs in cur_uav.Q:\n","        #     # self.num_repeated_obs += 1\n","        #     print(\"next_obs exists in Q\")\n","        # print(cur_uav.Q[next_obs])\n","        return cur_uav.Q[obs][action] != prev_Q, use_ngb\n","\n","    # def update_Q_value(self, agent_id, obs_n, next_obs_n, action_n, reward_n):\n","    #     # print(\"update_Q_value\")\n","    #     cur_uav = self.uavs[agent_id]\n","    #     done = self.is_episode_finished()\n","    #     if done:\n","    #         it_q = cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] + self.alpha * (reward[agent_id] - cur_uav.Q[obs_n[agent_id]][action_n[agent_id]])\n","    #         sum_ngb_q = 0\n","    #         for ngb in cur_uav.neighbours:\n","    #             sum_ngb_q += self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]] + self.alpha * (reward[ngb] - self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]])\n","    #         cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] = self.w * it_q + self.w * sum_ngb_q \n","    #         # cur_uav.Q[obs][action] = it_q\n","    #     else:\n","    #         # next_obs = repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","    #         # print(\"inside\")\n","    #         # print(repr([cur_uav.gamma_map, cur_uav.target_gamma_idx]))\n","    #         it_q = cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] + self.alpha * (reward[agent_id] + self.lambda_ * np.max(cur_uav.Q[next_obs_n[agent_id]]) - cur_uav.Q[obs_n[agent_id]][action_n[agent_id]])\n","    #         # print(\"it_q = \", it_q)\n","    #         sum_ngb_q = 0\n","    #         for ngb in cur_uav.neighbours:\n","    #             sum_ngb_q += self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]] + self.alpha * (reward[ngb] + self.lambda_ * np.max(self.uavs[ngb].Q[next_obs_n[ngb]]) - self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]])\n","    #         # print(\"neighbours \", cur_uav.neighbours)\n","    #         # print(\"sum_ngb_q = \", sum_ngb_q)\n","    #         cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] = self.w * it_q + self.w * sum_ngb_q \n","\n","    def is_same_target_gamma_point(self, agent_id, action_n):\n","        cur_uav = self.uavs[agent_id]\n","        x = cur_uav.target_gamma_idx[agent_id][0] + self.action_gamma_point_mapping[action_n[agent_id]][0]\n","        y = cur_uav.target_gamma_idx[agent_id][1] + self.action_gamma_point_mapping[action_n[agent_id]][1]\n","        is_same = False\n","        for j in range(self.num_agents):\n","            if j == agent_id:\n","                continue\n","            else:\n","                if LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[j].pos)) <= self.r_c: \n","                    target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    # target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    # target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    if target_x == x and target_y == y:\n","                        is_same = True\n","                        break\n","\n","        return is_same\n","\n","    def action_space(self, agent_id, action_n):\n","        cur_uav = self.uavs[agent_id]\n","        x = cur_uav.target_gamma_idx[agent_id][0]\n","        y = cur_uav.target_gamma_idx[agent_id][1]\n","        action_gamma_point_mapping = {\n","              1: [0, 0],\n","              2: [0, 1],\n","              3: [-1, 1],\n","              4: [-1, 0],\n","              5: [-1, -1],\n","              6: [0, -1],\n","              7: [1, -1],\n","              8: [1, 0],\n","              9: [1, 1]\n","        } \n","        action_space_ = np.array(list(range(1, 10)))\n","        # eliminate actions not possible or end up covering the same place\n","        idx = [] \n","        for action, delta_idx in action_gamma_point_mapping.items():\n","            x_ = x + delta_idx[0]\n","            y_ = y + delta_idx[1]\n","            is_same = False\n","            if x_ < 0 or x_ > self.k - 1 or y_ < 0 or y_ > self.l - 1 or cur_uav.gamma_map[x_, y_] == 1:\n","                # del action_space_[action - 1]\n","                is_same = True\n","                # print(action_space_)\n","                # print(\"action deleted\")\n","                # print(action_space_)\n","            if not is_same:\n","                for j in cur_uav.neighbours:\n","                    # target_x = cur_uav.target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    # target_y = cur_uav.target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    if target_x == x_ and target_y == y_:\n","                        is_same = True\n","                        break\n","                    # if self.uavs[j].untraversed_idx != None:\n","                    #     if x_ == self.uavs[j].untraversed_idx[0] and y_ == self.uavs[j].untraversed_idx[1]:\n","                    #         is_same = True\n","            if is_same:\n","                idx.append(action - 1)\n","        action_space_ = np.delete(action_space_, idx)\n","\n","        # prioritize horizontal and vertical action\n","        # l_actions = [1, 2, 4, 6, 8]\n","        # s_actions = [3, 5, 7, 9]\n","        # hv_ac = []\n","        # diag_ac = []\n","        # for act in action_space_:\n","        #     if act in l_actions:\n","        #         hv_ac.append(act)\n","        #     if act in s_actions:\n","        #         diag_ac.append(act)\n","        # action_space_ = np.array(hv_ac)\n","        # if len(hv_ac) != 0:\n","        #     action_space_ = hv_ac\n","        # else:\n","        #     action_space_ = diag_ac\n","\n","        return np.array(action_space_)  \n","\n","    def select_action(self, agent_id, obs, action_n, explore_start, explore_stop, decay_rate, decay_step, second, rpt_map, exp_exp_tradeoff):\n","        # print(\"new action\")\n","        cur_uav = self.uavs[agent_id]\n","        action_space_ = self.action_space(agent_id, action_n)\n","        o_action_space_ = np.copy(action_space_)\n","        ac_size = action_space_.size\n","        if action_space_.size == 0:   # empty list\n","            explore_probability = -1\n","            # print(\"empty action list\")\n","            # find all untraversed points different to target gamma points of other agents on the gamma map\n","            target_ids = []\n","            target_gamma_idx = cur_uav.target_gamma_idx\n","            for i in range(self.k):\n","                for j in range(self.l):\n","                    is_selected = True\n","                    if cur_uav.gamma_map[i, j] == 1:\n","                        is_selected = False\n","                    if is_selected:\n","                        for k in cur_uav.neighbours:\n","                            # target_x = target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            # target_y = target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            target_x = self.uavs[k].target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            target_y = self.uavs[k].target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            if i == target_x and j == target_y:\n","                                is_selected = False\n","                                break\n","                            # if self.uavs[k].untraversed_idx != None:\n","                            #     if i == self.uavs[k].untraversed_idx[0] and j == self.uavs[k].untraversed_idx[1]:\n","                            #         is_selected = False\n","                    if is_selected == True:\n","                        target_ids.append([i, j])\n","            # find the nearest untraversed point\n","            target = None\n","            dist = 100000000 \n","            for t_idx in target_ids:\n","                # np.array(self.index_to_pos(target_gamma_idx[agent_id]))\n","                tmp_dist = LA.norm(self.index_to_pos(t_idx) - cur_uav.pos)\n","                if tmp_dist < dist:\n","                    dist = tmp_dist\n","                    target = t_idx\n","            if target != None:\n","                # find the nearest gamma point and obtain the corresponding action\n","                dist = 100000000\n","                action_space_ = self.action_gamma_point_mapping\n","                # l_actions = [1, 2, 4, 6, 8]\n","                s_actions = [3, 5, 7, 9]\n","                action_num = 9\n","                for a, delta_idx in action_space_.items():\n","                    # if (a == 1) or (a in s_actions):\n","                    if a == 1:\n","                        # action_num -= 1\n","                        continue\n","                    t_idx_x = target_gamma_idx[agent_id][0] + delta_idx[0]\n","                    t_idx_y = target_gamma_idx[agent_id][1] + delta_idx[1]\n","                    is_selected = True\n","                    if t_idx_x < 0 or t_idx_x > self.k - 1 or t_idx_y < 0 or t_idx_y > self.l - 1:\n","                        is_selected = False\n","                    if is_selected:\n","                        for k in cur_uav.neighbours:\n","                            # target_x = target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            # target_y = target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            target_x = self.uavs[k].target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            target_y = self.uavs[k].target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            if t_idx_x == target_x and t_idx_y == target_y:\n","                                is_selected = False\n","                    if is_selected == False:\n","                        # action_num -= 1\n","                        continue\n","                    t_idx = [t_idx_x, t_idx_y]\n","                    cur_uav.untraversed_idx = t_idx\n","                    tmp_dist = LA.norm(np.array(self.index_to_pos(target)) - np.array(self.index_to_pos(t_idx)))\n","                    if tmp_dist < dist:\n","                        dist = tmp_dist\n","                        action = a\n","                if action_num == 0:\n","                    action = 1\n","            else: # if there's no untraversed point\n","                cur_uav.done = True\n","                action = 1\n","                cur_uav.untraversed_idx = None\n","            o_action_space_ = [action]\n","        else:\n","            # print(\"not empty action list\")\n","\n","            # print(action_space_)\n","            # eliminate the same gamma point of other agent\n","            # target_gamma_idx = cur_uav.target_gamma_idx\n","            # tmp_action_space_ = np.copy(action_space_)\n","            # mapping = self.action_gamma_point_mapping\n","            # idx = []\n","            # for action in action_space_:\n","            #     t_idx = [target_gamma_idx[agent_id][0] + mapping[action][0], target_gamma_idx[agent_id][1] + mapping[action][1]]\n","            #     for j in range(self.num_agents):\n","            #         if j == agent_id:\n","            #             continue\n","            #         else:\n","            #             target_x = target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","            #             target_y = target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","            #             if t_idx[0] == target_x and t_idx[1] == target_y:\n","            #                 # del tmp_action_space_[action]\n","            #                 idx.append(action - 1)\n","            # tmp_action_space_ = np.delete(tmp_action_space_, idx)\n","            \n","            # select best action based on maximum Q-value\n","            # if obs in cur_uav.Q:\n","            #     self.num_repeated_obs += 1\n","                # print(\"obs exists in Q\")\n","            # if cur_uav.Q[obs]:\n","            #     self.num_repeated_obs += 1\n","                # print(\"obs exists in Q\")\n","            Qs = cur_uav.Q[obs]\n","            # print(\"Qs = \", Qs)\n","            # print(\"original Q \", Qs)\n","            # print(\"minimum Q = \", np.argmin(Qs))\n","\n","            # prioritize horizontal and vertical action\n","            # l_actions = [1, 2, 4, 6, 8]\n","            # s_actions = [3, 5, 7, 9]\n","            # hv_ac = []\n","            # diag_ac = []\n","            # for act in action_space_:\n","            #     if act in l_actions:\n","            #         hv_ac.append(act)\n","            #     if act in s_actions:\n","            #         diag_ac.append(act)\n","            # if len(hv_ac) != 0:\n","            #     action_space_ = hv_ac\n","            # else:\n","            #     action_space_ = diag_ac\n","\n","            global_map = self.fuse_all_maps()\n","            # res = (((np.sum(global_map[:, :]) / (self.k * self.l)) > 0.5) and (len(action_space_) == 2) and second) or (second == False)\n","            res = (((np.sum(global_map[:, :]) / (self.k * self.l)) > 0.5) and second) or (second == False)\n","\n","\n","            # if (self.prev_T > self.crt_min_T) and (self.rpt_T_count > 10):\n","            #     tmp_Qs = np.array([Qs[action - 1] for action in action_space_])\n","            #     if (len(np.where(tmp_Qs < 0)) > 0) and (len(np.where(tmp_Qs == 0)) > 1):\n","            #         del_idx = np.argmax(tmp_Qs)\n","            #         cur_uav.Q[obs][action_space_[int(del_idx)] - 1] += -0.2\n","            #         action_space_ = np.delete(action_space_, del_idx)          \n","                # for divert_p in reversed(self.divert_points):\n","                # divert_p = self.divert_points[-1]\n","                # if (divert_p[0] == agent_id) and (self.update_ac_counts[agent_id] == divert_p[1]):\n","                #     del_idx = np.where(action_space_ == divert_p[2])\n","                #     tmp_action_space_ = np.delete(action_space_, del_idx)\n","                #     tmp_Qs = np.array([Qs[action - 1] for action in tmp_action_space_])\n","                #     action_space_ = tmp_action_space_\n","                        # if np.max(tmp_Qs) < 0:\n","                        #   continue\n","                        # else:\n","                        #   action_space_ = tmp_action_space_\n","                        #   break\n","            Qs = np.array([Qs[action - 1] for action in action_space_])\n","            ## First we randomize a number\n","            exp_exp_tradeoff = np.random.rand()\n","            # Exploration probability is exponentially decaying\n","            explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n","            # and explore_probability >= 0.05\n","            # if explore_probability < 0.3:\n","            #     # prob_step = 1.0 / (self.k * self.l / self.num_agents)\n","            #     # explore_probability = ep_step * prob_step\n","            #     # if explore_probability > 1.0:\n","            #     #     explore_probability = 1.0\n","            #     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * 0.5 * decay_step)\n","                # print(\"Explore P \", explore_probability)\n","            # if (explore_probability < 0.05) and (explore_probability > 0.01):\n","            #     ep_total_steps = np.ceil(self.k * self.l / self.num_agents)\n","            #     if ep_step > (ep_total_steps - 10):\n","            #         explore_probability = 1.0\n","            #         exp_exp_tradeoff = 0\n","            #     else:\n","            #         explore_probability = 0\n","            #         exp_exp_tradeoff = 1.0\n","            # elif explore_probability <= 0.01:\n","            #     explore_probability = 0\n","            #     exp_exp_tradeoff = 1.0\n","            explore_strategy = 0\n","            # if explore_probability >= 0.7:\n","            #     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate / 8 * decay_step)\n","            if explore_probability < 0.05:\n","                explore_strategy = 0\n","                explore_probability = 0\n","                exp_exp_tradeoff = 1.0\n","            if explore_strategy == 0:\n","                if (explore_probability > exp_exp_tradeoff) and res:\n","                    # g_action_space = []\n","                    # b_action_space = []\n","                    # for a in action_space_:\n","                    #     if obs in rpt_map:\n","                    #         if len(rpt_map[obs][a]) > 0:\n","                    #             b_action_space.append(a)\n","                    #         else:\n","                    #             g_action_space.append(a)\n","                    #     else:\n","                    #         g_action_space.append(a)\n","                    # if len(b_action_space) > 0 and len(g_action_space) > 0:\n","                    #     rd_num = np.random.rand()\n","                    #     if rd_num <= 1.0:\n","                    #         action = random.choice(g_action_space)\n","                    #     else:\n","                    #         # Make a random action (exploration)\n","                    #         action = random.choice(b_action_space)\n","                    # else:\n","                    #     action = random.choice(action_space_)\n","                    action = random.choice(action_space_)\n","                    # print(\"random action chosen\")\n","                else:\n","                    # if np.max(Qs) == 0:\n","                    #     g_action_space = []\n","                    #     # b_action_space = []\n","                    #     for a in action_space_:\n","                    #         if obs in rpt_map:\n","                    #             if len(rpt_map[obs][a]) == 0:\n","                    #                 g_action_space.append(a)\n","                    #         else:\n","                    #             g_action_space.append(a)\n","                    #     if len(g_action_space) > 0:\n","                    #         action = random.choice(g_action_space)\n","                    #     else:\n","                    #         action = random.choice(action_space_)\n","                    # else:\n","                    #     choice = np.argmax(Qs)\n","                    #     action = action_space_[int(choice)]\n","                    choice = np.argmax(Qs)\n","                    action = action_space_[int(choice)]\n","                    # print(\"uav \", agent_id, \" Qs = \", Qs, \" action_space_ = \", action_space_, \" action selected = \", action)\n","                    # pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","                    # # print(\"prob = \", pi)\n","                    # choice = sample(pi)\n","                    # action = action_space_[int(choice)]\n","            elif explore_strategy == 1:\n","                pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","                # print(\"prob = \", pi)\n","                choice = sample(pi)\n","                action = action_space_[int(choice)]\n","\n","            # pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","            # explore_probability = pi\n","            # explore_probability = 0\n","            # choice = sample(pi)\n","            # print(\"restricted actions \", action_space_)\n","            # print(\"restricted Q \", Qs)\n","            # choice = np.argmax(Qs)\n","            # action = action_space_[int(choice)]\n","            # action = random.choice(action_space_)\n","\n","        return action, explore_probability, ac_size, o_action_space_\n","\n","    def fuse_all_maps(self):\n","        global_map = np.zeros((self.k, self.l))\n","        for i, uav in enumerate(self.uavs):\n","            global_map[:, :] = np.add(global_map[:, :], uav.gamma_map[:, :])\n","        global_map[:, :] = global_map[:, :] != 0\n","\n","        return global_map\n","\n","    def v_reward_fn(self, agent_id, ngb):\n","        # print(\"action is \", action)\n","        l_actions = [1, 2, 4, 6, 8]\n","        s_actions = [3, 5, 7, 9]\n","        cur_uav = self.uavs[agent_id]\n","        action = cur_uav.action_n[ngb]\n","        # global_map = self.fuse_all_maps()\n","        # print(action)\n","        # print(self.action_gamma_point_mapping[action][0])\n","        # print(cur_uav.target_gamma_idx[agent_id][0])\n","        next_gamma_x = self.action_gamma_point_mapping[action][0] + cur_uav.prev_target_gamma_idx[ngb][0]\n","        next_gamma_y = self.action_gamma_point_mapping[action][1] + cur_uav.prev_target_gamma_idx[ngb][1]\n","        # print(\"target gamma point \", cur_uav.target_gamma_idx[agent_id])\n","        # print(\"action \", action)\n","        # print(next_gamma_x)\n","        # print(next_gamma_y)\n","        # print()\n","        # print(\"global_map \", global_map[next_gamma_x, next_gamma_y])\n","        if (action in l_actions) and (cur_uav.prev_global_map[next_gamma_x, next_gamma_y] == 0):\n","            reward = 0\n","        elif (action in s_actions) and (cur_uav.prev_global_map[next_gamma_x, next_gamma_y] == 0):\n","            reward = -0.3\n","            # reward = 0\n","        else:\n","            # print(\"here\")\n","            k_r = cur_uav.prev_count_map[next_gamma_x, next_gamma_y]\n","            reward = -0.2 * np.exp(self.c_r * (k_r - 1))\n","            # reward = 0\n","\n","        return reward, next_gamma_x, next_gamma_y\n","\n","    def reward_fn(self, agent_id, action):\n","        # print(\"action is \", action)\n","        l_actions = [1, 2, 4, 6, 8]\n","        s_actions = [3, 5, 7, 9]\n","        cur_uav = self.uavs[agent_id]\n","        global_map = self.fuse_all_maps()\n","        # print(action)\n","        # print(self.action_gamma_point_mapping[action][0])\n","        # print(cur_uav.target_gamma_idx[agent_id][0])\n","        next_gamma_x = self.action_gamma_point_mapping[action][0] + cur_uav.target_gamma_idx[agent_id][0]\n","        next_gamma_y = self.action_gamma_point_mapping[action][1] + cur_uav.target_gamma_idx[agent_id][1]\n","        # print(\"target gamma point \", cur_uav.target_gamma_idx[agent_id])\n","        # print(\"action \", action)\n","        # print(next_gamma_x)\n","        # print(next_gamma_y)\n","        # print()\n","        # print(\"global_map \", global_map[next_gamma_x, next_gamma_y])\n","        if (action in l_actions) and (global_map[next_gamma_x, next_gamma_y] == 0):\n","            reward = 0\n","        elif (action in s_actions) and (global_map[next_gamma_x, next_gamma_y] == 0):\n","            reward = -0.3\n","            # reward = 0\n","        else:\n","            # print(\"here\")\n","            k_r = self.count_map[next_gamma_x, next_gamma_y]\n","            reward = -0.2 * np.exp(self.c_r * (k_r - 1))\n","            # reward = 0\n","\n","        return reward\n","\n","    def eposidic_reward(self, done):\n","        global_map = self.fuse_all_maps()\n","        R_T = -1 \n","        if np.sum(global_map[:, :]) == self.k * self.l or done:\n","            if self.T > self.c1_T * self.T_min:\n","                R_T = 0\n","            elif self.T > self.c2_T * self.T_min and self.T <= self.c1_T * self.T_min:\n","                R_T = self.r_ref / 2 * (1 + math.cos((math.pi * (self.T - self.c2_T * self.T_min)) / (self.c1_T * self.T_min - self.c2_T * self.T_min)))\n","            elif self.T <= self.c1_T * self.T_min:\n","                R_T = self.r_ref\n","\n","        return R_T\n","\n","    # def reset(self):\n","    #     env.T = 0.3\n","    #     obs_n = []\n","    #     # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]]\n","    #     for i in range(self.num_agents):\n","    #         # # initialize position and velocity of each uav\n","    #         cur_uav = self.uavs[i]\n","    #         x = 1.5 * self.m / self.l + random.random() * (self.m - 3.0 * self.m / self.l)\n","    #         y = 1.5 * self.n / self.k + random.random() * (self.n - 3.0 * self.n / self.k)\n","    #         cur_uav.pos = [x, y]\n","    #         # cur_uav.pos = pos[i]\n","    #         cur_uav.velocity = [0, 0]\n","\n","    #         # initialize state s and action a randomly for each uav\n","    #         # find the closest gamma point to the current agent\n","    #         dist = 100000000\n","    #         for j in range((int)(self.k)):\n","    #             for k in range(self.l):\n","    #                 tmp_dist = LA.norm(np.array(self.index_to_pos([j, k])) - np.array(cur_uav.pos))\n","    #                 if tmp_dist < dist:\n","    #                     dist = tmp_dist\n","    #                     cur_uav.target_gamma_idx[i] = [j, k]\n","    #         actions = np.arange(1, 10)\n","    #         cur_uav_idx = cur_uav.target_gamma_idx[i]\n","    #         # make each uav have different target gamma point\n","    #         for action in actions:\n","    #             tmp_x = cur_uav_idx[0] + self.action_gamma_point_mapping[action][0]\n","    #             tmp_y = cur_uav_idx[1] + self.action_gamma_point_mapping[action][1]\n","    #             is_same = False\n","    #             for ag_id in range(self.num_agents):\n","    #                 if ag_id == i:\n","    #                     continue\n","    #                 idx = self.uavs[ag_id].target_gamma_idx[ag_id]\n","    #                 if idx[0] == tmp_x and idx[1] == tmp_y:\n","    #                     is_same = True\n","    #                     break\n","    #             if not is_same:\n","    #                 cur_uav.target_gamma_idx[i] = [tmp_x, tmp_y]\n","    #                 break\n","    #             else:\n","    #                 continue\n","    #         # cur_uav.target_gamma_idx[i] = [pos[i][0], pos[i][1]]\n","    #         cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","    #     for i in range(self.num_agents):\n","    #         _, obs = self.fuse_maps(i)\n","    #         obs_n.append(obs)\n","    #     self.count_map = np.zeros((self.k, self.l))\n","\n","    #     return obs_n\n","\n","    def reset(self):\n","        self.T = 0.3\n","        obs_n = []\n","        # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]] # 150.8\n","\n","        # pos = [[3.125, 46.875], [46.875, 46.875], [21.875, 3.125]]\n","        # pos = [[4.125, 45.875], [45.875, 47.875], [22.875, 4.125]]\n","\n","        # pos = [[16.125, 35.375], [27.125, 33.375], [22.875, 22.875]]\n","        # pos = [[15.125, 35.375], [27.125, 33.375], [39.625, 35.375]] # [2, 2], [2, 4], [2, 6] 142.3\n","        # pos = [[16.125, 35.375], [27.125, 33.375], [16.125, 22.875]] # [2, 2], [2, 4], [4, 2] 156.3 convergence fixed\n","        pos = self.uavs_pos\n","        chosen_actions = [1 for _ in range(self.num_agents)]\n","        # chosen_actions = [8, 2, 4]\n","        self.num_repeated_obs = 0\n","        \n","        for i in range(self.num_agents):\n","            cur_uav = self.uavs[i]\n","\n","            cur_uav.tra_map = np.zeros((self.k, self.l))\n","            cur_uav.untraversed_idx = None\n","            cur_uav.done = False\n","            # np.zeros(self.k, self.l)\n","            # reset gamma_map and target gamma points\n","            cur_uav.gamma_map = np.zeros((self.k, self.l))\n","            for l in range(num_agents):\n","            # if i == agent_id:\n","            #     continue\n","                cur_uav.target_gamma_idx[l] = [-1, -1]\n","\n","            # initialize position and velocity of each uav\n","            cur_uav.pos = pos[i]\n","            cur_uav.velocity = [0, 0]\n","\n","            # initialize state s and action a randomly for each uav\n","            # find the closest gamma point to the current agent\n","            dist = 100000000\n","            for j in range(self.k):\n","                for k in range(self.l):\n","                    tmp_dist = LA.norm(np.array(self.index_to_pos([j, k])) - np.array(cur_uav.pos))\n","                    if tmp_dist < dist:\n","                        dist = tmp_dist\n","                        cur_uav.target_gamma_idx[i] = [j, k]\n","            actions = np.arange(1, 10)\n","            # possible_actions = np.copy(actions)\n","            cur_uav_idx = cur_uav.target_gamma_idx[i]\n","            # print(\"target gamma point \", cur_uav_idx)\n","\n","            # make each uav have different target gamma point\n","            action_to_delete = []\n","            for action in actions:\n","                tmp_x = cur_uav_idx[0] + self.action_gamma_point_mapping[action][0]\n","                tmp_y = cur_uav_idx[1] + self.action_gamma_point_mapping[action][1]\n","                if tmp_x < 0 or tmp_x > self.k - 1 or tmp_y < 0 or tmp_y > self.l - 1:\n","                    action_to_delete.append(action - 1)\n","                for ag_id in range(self.num_agents):\n","                    if ag_id == i:\n","                        continue\n","                    idx = np.copy(self.uavs[ag_id].target_gamma_idx[ag_id])\n","                    idx[0] += self.action_gamma_point_mapping[chosen_actions[ag_id]][0]\n","                    idx[1] += self.action_gamma_point_mapping[chosen_actions[ag_id]][1]\n","                    if idx[0] == -1:\n","                        continue\n","                    if idx[0] == tmp_x and idx[1] == tmp_y:\n","                        action_to_delete.append(action - 1)          \n","            possible_actions = np.delete(actions, action_to_delete)\n","            chosen_actions[i] = random.choice(possible_actions)\n","\n","            # if i == 0:\n","            #     chosen_actions[i] = random.choice([1, 4])\n","\n","            cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","            # print(cur_uav.tra_map)\n","            # print(np.amax(cur_uav.tra_map) + 1)\n","\n","        # choice = random.choice([0, 1])\n","        # if choice == 0:\n","        #     chosen_actions = [1, 1, 1]\n","        # else:\n","        #     chosen_actions = [6, 4, 1]\n","        # print(self.uavs[0].target_gamma_idx[0])\n","        # print(self.uavs[1].target_gamma_idx[1])\n","        # print(self.uavs[2].target_gamma_idx[2])\n","        self.count_map = np.zeros((self.k, self.l))\n","        action_n = [1 for _ in range(self.num_agents)]\n","        # global_map = self.fuse_all_maps() # only for agents initially not at the gamma points\n","        for i in range(self.num_agents):\n","            obs = self.fuse_maps(i, action_n, action_n)\n","            self.uavs[i].prev_gamma_map = np.copy(cur_uav.gamma_map)\n","            # self.uavs[i].prev_target_gamma_idx = copy.deepcopy(cur_uav.target_gamma_idx)\n","            # self.uavs[i].prev_global_map = np.copy(global_map)\n","            # self.uavs[i].prev_count_map = np.copy(self.count_map)\n","            obs_n.append(obs)\n","        # print(\"reset\")\n","        # print(self.uavs[0].target_gamma_idx[0])\n","        # print(self.uavs[1].target_gamma_idx[1])\n","        # print(self.uavs[2].target_gamma_idx[2])\n","\n","        return obs_n, chosen_actions\n","\n","    # def reset(self):\n","    #     env.T = 0.3\n","    #     obs_n = []\n","    #     # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]]\n","    #     pos = []\n","    #     num_cells = self.k * self.l\n","    #     # pos.append([i, i])\n","    #     for i in range(self.num_agents):\n","    #         # initialize state s and action a randomly for each uav\n","    #         cur_uav = self.uavs[i]\n","    #         is_same = True\n","    #         while is_same:\n","    #             is_same = False\n","    #             index = random.randint(0, num_cells - 1)\n","    #             row = (int)(index / self.l)\n","    #             col = index % self.l\n","    #             for ag_id in range(self.num_agents):\n","    #                 if ag_id == i:\n","    #                     continue\n","    #                 idx = self.uavs[ag_id].target_gamma_idx[ag_id]\n","    #                 if idx[0] == row and idx[1] == col:\n","    #                     is_same = True\n","    #         cur_uav.target_gamma_idx[i] = [row, col]\n","    #         # print([row, col])\n","\n","    #         # initialize position and velocity of each uav\n","    #         pos_c = self.index_to_pos([row, col])\n","    #         r_len = self.n / self.k / 2.0\n","    #         c_len = self.m / self.l / 2.0\n","    #         cur_uav.pos = [random.uniform(pos_c[0] - r_len, pos_c[0] + r_len), random.uniform(pos_c[1] - c_len, pos_c[1] + c_len)]\n","    #         cur_uav.velocity = [0, 0]\n","\n","    #         # cur_uav.target_gamma_idx[i] = [pos[i][0], pos[i][1]]\n","    #         cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","    #     for i in range(self.num_agents):\n","    #         obs = self.fuse_maps(i)\n","    #         obs_n.append(obs)\n","    #     self.count_map = np.zeros((self.k, self.l))\n","\n","    #     return obs_n\n","\n","    def is_episode_finished(self):\n","        global_map = self.fuse_all_maps()\n","\n","        return (np.sum(global_map[:, :]) == self.k * self.l) or (self.T > self.num_agents * 1 * self.T_min)\n","\n","    def is_fully_covered(self):\n","        global_map = self.fuse_all_maps()\n","\n","        return np.sum(global_map[:, :]) == self.k * self.l\n","\n","    def get_obs_n(self):\n","        return [repr([self.uavs[i].gamma_map, self.uavs[i].target_gamma_idx]) for i in range(self.num_agents)]\n","\n","    def get_obs(self, agent_id):\n","        return [self.uavs[agent_id].gamma_map, self.uavs[agent_id].target_gamma_idx]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpR8hn0jxieV","colab_type":"code","colab":{}},"source":["def sample(pi):\n","  # print(pi)\n","  # normalize(pi)\n","  return np.random.choice(pi.size, size=1, p=pi)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhR-037wK1Pk","colab_type":"code","colab":{}},"source":["# import tensorflow.compat.v1 as tf    \n","# tf.compat.v1.disable_eager_execution() \n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np           \n","import random                \n","import time                  \n","from collections import deque\n","import matplotlib.pyplot as plt \n","\n","### Q-LEARNING PARAMETERS\n","alpha = 1.0\n","lambda_ = 0.8\n","w = 0.5\n","r_ref = 64\n","epsilon = 0.2\n","c_r = 1.0\n","# c_r = 0.6\n","total_episodes = 1000000\n","\n","# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n","explore_start = -1.0            # Initial exploration probability \n","explore_stop = 0.0            # minimum exploration probability \n","# decay_rate = 0.000005            # exponential decay rate for exploration probability\n","decay_rate = 0.000006            # exponential decay rate for exploration probability (around 1614 for rc = 10, more for other rc)\n","# decay_rate = 0.000001              # around 8000 for rc = 60\n","# decay_rate = 0.0\n","decay_step = 0\n","\n","# HYPERPARAMETERS for soft max strategy\n","beta = 0.1\n","\n","### MAS MOTION MODEL PARAMETERS\n","r_c = 50\n","v_max = 1.0\n","d = 7\n","h = 0.1\n","c_alpha = 0.5\n","c1_gamma = 0.5\n","c2_gamma = 0.8\n","# c1_T = 1.45\n","c1_T = 170 / 131.25 # give close to op most of time (8/10 <=143.3)\n","# c1_T = 210 / 131.25 \n","# c1_T = 150 / 131.25 \n","# c1_T = 1.75\n","c2_T = 140 / 131.25 \n","# c2_T = 1\n","\n","# c1_T = 1.43\n","# c2_T = 1.28\n","# c1_T = 1.372\n","# c2_T = 1.143\n","# c1_T = 1.18\n","# c2_T = 1.12\n","\n","### OTHER HYPERPARAMETERS\n","r_s = 4.5\n","m = 50\n","n = 50\n","num_agents = 3\n","t_step = 0.5\n","action_num = 9\n","\n","actions = np.arange(9) # All possible actions\n","# possible_actions = np.identity(4, dtype=int).tolist() # One-hot encoding of possible actions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqZWXni1L-jc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1596997127902,"user_tz":-60,"elapsed":881,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"08b6c314-756d-46ef-a53e-4d586382b2a5"},"source":["env = Environment(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents)\n","ids = [[2,2], [2,4], [2,6]]\n","for i in range(3):\n","    print(env.index_to_pos(ids[i]))\n","a = [1, 0, 1, 0]\n","pos = [[15.125, 35.375], [27.125, 33.375], [39.625, 35.375]]\n","for i in range(3):\n","    print(env.pos_to_index(pos[i]))\n","# a = np.zeros(1)\n","# print(np.count_nonzero(a))\n","# print(len([]))\n","# arr = np.array([1, 2, 3])\n","# print(arr[np.where(arr == 1)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[15.625, 34.375]\n","[28.125, 34.375]\n","[40.625, 34.375]\n","[1.8399999999999999, 1.92]\n","[2.16, 3.84]\n","[1.8399999999999999, 5.84]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lGG91KR1Ziur","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597063651744,"user_tz":-60,"elapsed":468,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"6c609d7c-9d85-47b3-b4b7-1bd6c9a13eec"},"source":["d = {1: 1}\n","print(list(d.keys())[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"01YHuDapxcb6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595000660176,"user_tz":-60,"elapsed":528,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"72771373-939f-4190-958d-c827c22d7541"},"source":["def compare_idx(idx1, idx2):\n","    if idx1[0] == idx2[0] and idx1[1] == idx2[1]:\n","        return True\n","    \n","    return False\n","\n","idx = [[4, 2], [3, 3], [4, 3]]\n","action_gamma_point_mapping = {\n","    1: [0, 0],\n","    2: [0, 1],\n","    3: [-1, 1],\n","    4: [-1, 0],\n","    5: [-1, -1],\n","    6: [0, -1],\n","    7: [1, -1],\n","    8: [1, 0],\n","    9: [1, 1]\n","}\n","count = 9 * 9 * 9\n","action_ns = []\n","for i in range(9):\n","    t1_x = idx[0][0] + action_gamma_point_mapping[i + 1][0]\n","    t1_y = idx[0][1] + action_gamma_point_mapping[i + 1][1]\n","    for j in range(9):\n","        t2_x = idx[1][0] + action_gamma_point_mapping[j + 1][0]\n","        t2_y = idx[1][1] + action_gamma_point_mapping[j + 1][1]\n","        for k in range(9):\n","            t3_x = idx[2][0] + action_gamma_point_mapping[k + 1][0]\n","            t3_y = idx[2][1] + action_gamma_point_mapping[k + 1][1]\n","\n","            if compare_idx([t1_x, t1_y], [t2_x, t2_y]) or compare_idx([t1_x, t1_y], [t3_x, t3_y]) or compare_idx([t2_x, t2_y], [t3_x, t3_y]):\n","                count -= 1\n","            else:\n","                action_ns.append([i + 1, j + 1, k + 1])\n","action_ns = [[7, 2, 2]]\n","print(count)\n","print(len(action_ns))\n","# print(action_ns[2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["593\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3cYp5H-QTR16","colab_type":"code","colab":{}},"source":["from collections import deque\n","\n","# o_action_ns = np.copy(action_ns[0:3])\n","# print(o_action_ns)\n","# print(len(action_ns))\n","# epc_T = deque(np.zeros((6), dtype=np.float32), maxlen = 6)\n","# epc_T.append(1.0)\n","# print(ep_reward[5])\n","\n","def converge(epc_T):\n","    res = True\n","    for i in range(len(epc_T) - 1):\n","        if epc_T[i] != epc_T[i + 1]:\n","            res = False\n","            break\n","\n","    return res\n","\n","# converge(epc_T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXyfb0ALzACt","colab_type":"code","colab":{}},"source":["# a = [1, 2, 3]\n","# for i, num in enumerate(reversed(a)):\n","#     print(i, num)\n","a = [(1, 2), [3, 4]]\n","b = np.copy(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-WqCDDlfR9v","colab_type":"code","colab":{}},"source":["def train(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents, explore_start, explore_stop, decay_rate, decay_step):\n","    is_decay_set = False\n","    env = Environment(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents)\n","    # env.crt_min_T = 100000000\n","    # env.prev_T = 0\n","    # env.rpt_T_count = 0\n","    # env.divert_points = []\n","    obs_prev_obs = {}\n","    Ts = []\n","    mean_Ts = []\n","    final_rs = []\n","    num_T = 500\n","    s_actions = [3, 5, 7, 9]\n","    T_memory = [{} for _ in range(env.num_agents)]\n","    rpt_maps = [{} for _ in range(env.num_agents)]\n","    cac = []\n","    cac_T = []\n","\n","    # print(env.k)\n","    # print(env.l)\n","    # print(env.T_min)\n","\n","    # print(obs_n[0])\n","    # print(env.index_to_pos(env.uavs[0].target_gamma_idx[0]))\n","    # print(env.uavs[0].pos)\n","    # print(obs_n[1])\n","    # print(obs_n[2])\n","    # print(env.count_map)\n","\n","    # print(env.fuse_all_maps())\n","\n","    # print(env.index_to_pos([7, 7]))\n","    # tra_points_n = [[] for _ in range(env.num_agents)]\n","    # for i in range(env.num_agents):\n","    #     tra_points_n[i].append(env.uavs[i].pos)\n","    # print(tra_points_n)\n","    # # iteration = 1\n","\n","    # print(\"chosen actions \", action_n_)\n","    total_episodes = 4000\n","    action_ns_tra = [[] for i in range(env.num_agents)]\n","    decay_step = 0\n","    explore_prob = -1\n","    # same_gamma_point = False\n","    prev_memory = [[] for i in range(env.num_agents)]\n","    max_size = 23\n","    epc_T = deque(np.zeros((max_size), dtype=np.float32), maxlen = max_size)\n","    for episode in range(total_episodes):\n","        # if episode >= num_T - 1:\n","        #     if np.sum(np.diff(Ts[episode : episode + num_T + 1])) / num_T <= 1:\n","        #         print(\"converged\")\n","        #         break\n","        ep_step = 0\n","        diag_count = 0\n","        ac_size = [9 for i in range(env.num_agents)]\n","        memory = [[] for i in range(env.num_agents)]\n","        repeated_traversal = np.zeros(env.num_agents)\n","        pos_tra = [[] for i in range(env.num_agents)]\n","        env.update_ac_counts = np.ones(env.num_agents)\n","        divert_obtained = False\n","        last_agent_id = 0\n","        obs_n, action_n = env.reset()\n","\n","        # action_n = [1 for _ in range(env.num_agents)]\n","        # for i in range(env.num_agents):\n","        #     # print(\"uav \", i, \" action_space = \", env.action_space(i, action_n))\n","        #     action_n[i], explore_p, ac_size[i] = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], 0)\n","        # print(env.get_obs(0)[0])\n","        action_n = [1, 1, 1]\n","\n","        for i in range(env.num_agents):\n","            action_ns_tra[i].append(action_n[i])\n","            # env.uavs[i].action_n = np.copy(action_n)\n","        o_obs_n = np.copy(obs_n)\n","        # print(obs_n[0])\n","        # break\n","\n","        # print(\"chosen actions \", action_n)\n","\n","        # action_n = np.copy(action_n_)\n","        o_action_n = np.copy(action_n)\n","        prev_action_n = np.copy(action_n)\n","        done = False\n","        final_reward = 0\n","        reward = np.zeros(env.num_agents)\n","        # first_step = True\n","        # action_n = [1 for _ in range(env.num_agents)]\n","        # if episode == 100:\n","        #     env.c1_T = 1.37\n","        #     env.c2_T = 1.14\n","        # if env.rpt_T_count == 10:\n","        # cac.append(env.update_cac_map())\n","        # cac_T.append(env.T)\n","        while not env.is_episode_finished():\n","            exp_exp_tradeoff = np.random.rand()\n","            for i in range(env.num_agents):\n","                if not env.uavs[i].done:\n","                    # print(\"uav \", i)\n","                    # reward = env.reward_fn(i, action_n[i])\n","                    # print(\"reward = \", reward)\n","                    # print(action_n[i])\n","\n","                    if env.T == 0.3:\n","                        reward[i] = env.reward_fn(i, action_n[i])\n","                        # print(\"uav \", i, \" reward = \", reward[i])\n","\n","                    # if the same target gamma point is detected midway through due to r_c is too small\n","                    # update action\n","                    is_same_target_gamma_point = env.is_same_target_gamma_point(i, action_n)\n","                    if is_same_target_gamma_point:\n","                        # print(\"same target gamma point\")\n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # print(action_n[i])\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] -= env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] -= env.action_gamma_point_mapping[prev_action_n[i]][1]\n","                        prev_action_n[i], explore_p, ac_size[i], action_space = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], exp_exp_tradeoff)\n","                        if prev_action_n[i] in s_actions:\n","                            diag_count += 1                            \n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        # print(\"changed action \", prev_action_n[i])\n","                        if explore_p != -1:\n","                            explore_prob = explore_p\n","                            # if (explore_p < 0.7) and (not is_decay_set):\n","                            #     decay_rate = 0.000008\n","                            #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                            #     is_decay_set = True\n","                        reward[i] = env.reward_fn(i, prev_action_n[i])\n","                        if action_n[i] != 1:\n","                            action_n[i] = prev_action_n[i]\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] += env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] += env.action_gamma_point_mapping[prev_action_n[i]][1]\n","\n","                    # print(\"uav \", i, \" action_n[i] = \", action_n[i])\n","                    env.MAS_update(i, action_n[i])   # update target gamma point of uav i\n","                    pos_tra[i].append(env.uavs[i].pos)\n","                    # next_obs = env.fuse_maps(i, action_n, prev_action_n)\n","                    action_n[i] = 1\n","                    # tra_points_n[i].append(env.uavs[i].pos)\n","\n","                    is_target_reached = env.is_target_reached(i)\n","                    if is_target_reached:\n","                        # print(\"uav \", i, \" \", \"target \", env.uavs[i].target_gamma_idx[i] ,\"is reached\")\n","                        # print(\"env.T \", env.T)\n","                        # reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                        # print(\"target gamma point\")\n","                        # print(env.uavs[0].target_gamma_idx[0])\n","                        # print(env.uavs[1].target_gamma_idx[1])\n","                        # print(env.uavs[2].target_gamma_idx[2])\n","                        # print()\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # next_obs_n = [env.fuse_maps(k, action_n, prev_action_n) for k in range(env.num_agents)]\n","\n","                        # print()\n","                        # print(next_obs)\n","                        episodic_reward = env.eposidic_reward(False)\n","                        if episodic_reward == -1:\n","                            # print(\"prev_action_n \", prev_action_n)\n","                            # env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, num_prev_branches, True, True)\n","                            # if episode == 0 and memory[i]:\n","                            #     print(\"uav \", i, \" \", env.uavs[i].Q[obs_n[i]], \" \", np.array(memory[i][-1][6]), \" \", prev_action_n[i])\n","                            action_n[i], explore_p, ac_size[i], action_space = env.select_action(i, next_obs, action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], exp_exp_tradeoff)\n","                            # obtain the first divert point\n","                            # if update_ac_counts[i] != 0:\n","                            # if (not divert_obtained):\n","                            #     if (len(prev_memory[i]) != 0):\n","                            #         if int(env.update_ac_counts[i]) < len(prev_memory[i]):\n","                            #             if (prev_memory[i][int(env.update_ac_counts[i])][1] != action_n[i]):\n","                            #                 env.divert_points.append([i, env.update_ac_counts[i], action_n[i], action_space])  \n","                            #                 divert_obtained = True\n","                            #                 print(env.divert_points[-1]) \n","                            env.update_ac_counts[i] += 1\n","                            env.update_Qts(i, obs_n[i], next_obs, prev_action_n[i] - 1, action_space)\n","                            q_update, use_ngb = env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward[i], reward[i], 0, False, False, action_space)\n","                            # if use_ngb and (len(memory[i][-1][6]) > 1):\n","                            #     print(\"use ngb action space \", memory[i][-1][6])\n","                            # if use_ngb:\n","                                # print(\"uav \", i, \" use ngb action space \", memory[i][-1][6], \" action \", prev_action_n[i], \" step \", len(memory[i]))\n","                            for step, experience in enumerate(reversed(memory[i])):\n","                                if experience[5] <= 1:\n","                                    env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], episodic_reward, 0, False, False, experience[6])\n","                                    # print(\"uav \", i, \" use ngb action space \", memory[i][step + 1][6], \" action \", experience[1], \" step \", len(memory[i]) - step - 1, \"q_update = \", q_update)\n","                                else:\n","                                    break\n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward[i], env.uavs[i].done, ac_size[i], action_space])\n","                            # for step, experience in enumerate(reversed(memory[i])):\n","                            #     if experience[5] <= 1:\n","                            #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], episodic_reward, 0, False, False, experience[6])\n","                            #     else:\n","                            #         break\n","                            # print(\"action \", i, \" = \", action_n[i])\n","                            # if q_update:\n","                            #     print(\"uav \", i, \" q updated at step\", len(memory[i]), \"reward = \", reward[i], \"act space = \", action_space, \"qs = \", env.uavs[i].Q[obs_n[i]])\n","                            if action_n[i] in s_actions:\n","                                # print(\"uav \", i, \" diag at step\", len(memory[i]), \"chosen action = \", action_n[i], \"act space = \", action_space, \"qs = \", env.uavs[i].Q[next_obs])\n","                                diag_count += 1  \n","                            if (ac_size[i] == 0) and (not env.uavs[i].done):\n","                                # print(\"uav \", i, \" repeat at step\", len(memory[i]), \"chosen action = \", action_n[i], \"act space = \", action_space, \"qs = \", env.uavs[i].Q[next_obs])\n","                                repeated_traversal[i] += 1\n","                            if reward[i] < 0:\n","                                for ag in range(env.num_agents):\n","                                    for experience in memory[ag]:\n","                                        if experience[0] not in rpt_maps[ag]:\n","                                            tmp_map = {}\n","                                            for nc in range(action_num):\n","                                                tmp_map[nc + 1] = []\n","                                            tmp_map[experience[1]].append(next_obs)\n","                                            rpt_maps[ag][experience[0]] = tmp_map\n","                                        else:\n","                                            if next_obs not in rpt_maps[ag][experience[0]][experience[1]]:\n","                                                rpt_maps[ag][experience[0]][experience[1]].append(next_obs)\n","\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            if explore_p != -1:\n","                                explore_prob = explore_p\n","                                # if (explore_p < 0.7) and (not is_decay_set):\n","                                #     print(\"set\")\n","                                #     decay_rate = 0.000008\n","                                #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                                #     is_decay_set = True\n","                            action_ns_tra[i].append(action_n[i])\n","                            # if not env.uavs[i].done:\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # else:\n","                            #     episodic_reward = env.eposidic_reward(True)\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward, env.is_episode_finished(), True)\n","                            # if obs_n[i] == next_obs:\n","                            #     print(\"states are the same\")\n","                            # else:\n","                            #     print(\"states are not the same\")\n","                            if not env.uavs[i].done:\n","                                prev_action_n[i] = action_n[i]\n","                            # print(action_n)\n","                            # print()\n","                                obs_n[i] = next_obs\n","                                env.uavs[i].prev_gamma_map = np.copy(env.uavs[i].gamma_map)\n","\n","                                # env.uavs[i].prev_target_gamma_idx = copy.deepcopy(env.uavs[i].target_gamma_idx)\n","                                # global_map = env.fuse_all_maps()\n","                                # env.uavs[i].prev_global_map = np.copy(global_map)\n","                                # env.uavs[i].action_n = np.copy(prev_action_n)\n","                                # env.uavs[i].prev_count_map = np.copy(env.count_map)\n","                        else:\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, env.is_episode_finished())\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, episodic_reward, 0, True, False, [])\n","                            # env.update_Qts(i, obs_n[i], next_obs, prev_action_n[i] - 1, action_space)\n","                            last_agent_id = i\n","                            # if episode == 0 and memory[i]:\n","                            #     print(\"uav \", i, \" \", env.uavs[i].Q[obs_n[i]], \" \", np.array(memory[i][-1][6]), \" \", memory[i][-1][1])\n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward[i], True, ac_size[i], []])\n","                            if reward[i] < 0:\n","                                for ag in range(env.num_agents):\n","                                    for experience in memory[ag]:\n","                                        if experience[0] not in rpt_maps[ag]:\n","                                            tmp_map = {}\n","                                            for nc in range(action_num):\n","                                                tmp_map[nc + 1] = []\n","                                            tmp_map[experience[1]].append(next_obs)\n","                                            rpt_maps[ag][experience[0]] = tmp_map\n","                                        else:\n","                                            if next_obs not in rpt_maps[ag][experience[0]][experience[1]]:\n","                                                rpt_maps[ag][experience[0]][experience[1]].append(next_obs)\n","\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            # agent_ids = list(range(env.num_agents))\n","                            # del agent_ids[i]\n","                            # for id in range(len(memory[i]), -1, -1):\n","                            #     more_than_one = True\n","                            #     for ag_id in agent_ids:\n","                            #         if memory[ag_id][id][5] < 2:\n","                            #             more_than_one = False\n","                            #             print\n","                            #             break\n","                            #     if more_than_one: \n","                            #         experience = memory[i][id]:\n","                            # for experience in reversed(memory[i]):\n","                            #     if experience[5] > 1:\n","                            #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, episodic_reward, True)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            final_reward = episodic_reward\n","\n","                            # print(\"uav \", i, \" obtain \" \"episodic_reward = \", episodic_reward)\n","\n","                            env.uavs[i].prev_gamma_map = np.copy(env.uavs[i].gamma_map)\n","                            # env.uavs[i].prev_target_gamma_idx = copy.deepcopy(env.uavs[i].target_gamma_idx)\n","                            # global_map = env.fuse_all_maps()\n","                            # env.uavs[i].prev_global_map = np.copy(global_map)\n","                            # env.uavs[i].action_n = np.copy(prev_action_n)\n","                            # env.uavs[i].prev_count_map = np.copy(env.count_map)\n","                            done = True\n","                            break\n","                        reward[i] = env.reward_fn(i, action_n[i])\n","                        # print(\"uav \", i, \" reward = \", reward[i])\n","                    # else:\n","                    #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, reward)\n","                    #     reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                    # print(\"uav \", i, \" \", env.uavs[i].target_gamma_idx[i])\n","\n","                    # for ag_id in range(env.num_agents):\n","                    #     tmp_x = env.uavs[ag_id].target_gamma_idx[ag_id][0] + env.action_gamma_point_mapping[action_n[ag_id]][0]\n","                    #     tmp_y = env.uavs[ag_id].target_gamma_idx[ag_id][1] + env.action_gamma_point_mapping[action_n[ag_id]][1]\n","                    #     print(\"uav \", ag_id, \" \", [tmp_x, tmp_y])\n","                    # print()\n","\n","                    # print(action_n)\n","                    # print(\"uav \", i, \" \", env.get_obs_n()[i])\n","                    # print(\"is_target_reached\")\n","                    # print(is_target_reached)\n","            # Increase decay_step\n","            # if episode >= 4000:\n","            decay_step +=1\n","\n","            ep_step += 1\n","\n","            if done:\n","                break\n","\n","            env.T += env.t_step\n","            # if env.rpt_T_count == 10:\n","            # cac.append(env.update_cac_map())\n","            # cac_T.append(env.T)\n","\n","            a1 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[1][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[1][1]\n","            a2 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[2][1]\n","            a3 = env.uavs[0].target_gamma_idx[1][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[1][1] == env.uavs[0].target_gamma_idx[2][1]\n","            if (a1 or a2 or a3) and (env.uavs[0].target_gamma_idx[0][0] != -1 and env.uavs[0].target_gamma_idx[1][0] != -1 and env.uavs[0].target_gamma_idx[2][0] != -1):\n","                # print(\"same gamma point\")\n","                # print(env.T)\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[0].target_gamma_idx[1])\n","                # print(env.uavs[0].target_gamma_idx[2])\n","                # print()\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[1].target_gamma_idx[1])\n","                # print(env.uavs[2].target_gamma_idx[2])\n","                same_gamma_point = True\n","                # break\n","            # cur_pos_x = [env.uavs[i].pos[0] for i in range(env.num_agents)]\n","            # cur_pos_y = [env.uavs[i].pos[1] for i in range(env.num_agents)]\n","            # target_pos_x = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[0] for i in range(env.num_agents)]\n","            # target_pos_y = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[1] for i in range(env.num_agents)]\n","            # fig, ax = plt.subplots()\n","            # ax.plot(cur_pos_x, cur_pos_y, 'o')\n","            # ax.plot(target_pos_x, target_pos_y, 'x')\n","            # plt.show()\n","            ep_step += 1\n","            # print()\n","            # if ep_step == 10:\n","            #     break\n","        # episodic_reward = env.eposidic_reward()\n","        # for i in range(env.num_agents):\n","        #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, episodic_reward)\n","        #     print(\"episodic_reward = \", episodic_reward)\n","            # print(env.uavs[i].Q)\n","        # print(\"len of memory[i] = \", len(memory[0]))\n","        # print(\"Explore P \", explore_prob)\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        for i in range(env.num_agents):\n","            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, episodic_reward, 0, True, False, [])\n","        #     # if last_agent_id == i:\n","            for step, experience in enumerate(reversed(memory[i])):\n","                if step == 0:\n","                    env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, episodic_reward, 0, True, False, [])\n","                else:\n","                    if experience[5] <= 1: # or step <= 2:\n","                        env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], episodic_reward, 0, False, False, experience[6])\n","                    else:\n","                        break\n","\n","        # if episode == 1947 or episode == 1948 or episode == 1949 or episode == 1950 or episode == 1951:\n","        #     for i in range(env.num_agents):\n","        #         for step, experience in enumerate(memory[i]):\n","        #             if (step + 1) < len(memory[i]):\n","        #                 print(\"uav \", i, \" \", env.uavs[i].Q[memory[i][step + 1][0]], \" \", memory[i][step][6], \" \", memory[i][step + 1][1])\n","        #         print()\n","\n","        # Q-table update\n","        # for i in range(env.num_agents):\n","        #     # if i == 2:\n","        #     #     continue\n","        #     first_branch_set = False\n","        #     for step, experience in enumerate(reversed(memory[i])):\n","        #         num_prev_branches = len(obs_prev_obs[experience[2]])\n","        #         if step == 0:\n","        #         #     # print(\"done = \", experience[4])\n","        #             env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, num_prev_branches, True, True)\n","        #         else:\n","        #             env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward + experience[3], num_prev_branches, False, True)\n","\n","                # if experience[0] in obs_prev_obs:\n","                #     if (len(obs_prev_obs[experience[0]]) > 1) and (first_branch_set == False):\n","                #         first_branch_set = True\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 1, False, True)\n","                #     else:\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","\n","                # if step == 0:\n","                #     # print(\"done = \", experience[4])\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, step, True, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, step, False, True)\n","\n","        #     print()\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        prev_memory = np.copy(memory)\n","        if env.T < env.crt_min_T:\n","            env.crt_min_T = env.T\n","        if env.T == env.prev_T:\n","            env.rpt_T_count += 1\n","        else:\n","           env.rpt_T_count = 0\n","        # print(\"env.crt_min_T = \", env.crt_min_T)\n","        # print(\"env.rpt_T_count = \", env.rpt_T_count)\n","        env.prev_T = env.T\n","\n","        if env.is_fully_covered():\n","            str_ep = \" is\"\n","            Ts.append(env.T)\n","            final_rs.append(final_reward)\n","            if len(Ts) >= 100:\n","                mean_Ts.append(np.mean(Ts[-100:]))\n","        else:\n","            str_ep = \" not\"\n","            # print()\n","            # print(o_action_n)\n","            # print(prev_action_n)\n","            # for ag_id in range(env.num_agents):\n","            #     cur_uav = env.uavs[ag_id]\n","            #     # target_pos = env.index_to_pos(cur_uav.target_gamma_idx[ag_id])\n","            #     # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","            #     # print(\"velocity = \", LA.norm(cur_uav.velocity))\n","            #     print(cur_uav.gamma_map)\n","            #     print(cur_uav.target_gamma_idx)\n","            #     print(cur_uav.done)\n","            # break\n","        # print(\"episode \", episode, str_ep, \" fully covered\")\n","        # print(env.T)\n","        # for i in range(env.num_agents):\n","        #     print(\"uav \", i, \" repeated_traversal = \", repeated_traversal[i])\n","        #     # T_memory[i][env.T] = memory[i]\n","        # print(\"total repeated traversal = \", np.sum(repeated_traversal))\n","        # print()\n","        for i in range(env.num_agents):\n","            T_memory[i][env.T] = memory[i]\n","        # print(\"len of T_memory = \", len(T_memory[0]))\n","        # print(\"len of Ts = \")\n","        epc_T.append(env.T)\n","        #  \n","        # if converge(epc_T):\n","        #     print(\"episode \", episode)\n","        #     print(env.T)\n","        #     break\n","        # if episode > 3000 and env.is_fully_covered():\n","        #     if np.min(Ts) < 150:\n","        #         if env.T < 150:\n","        #             print(\"episode \", episode)\n","        #             print(env.T)\n","        #             break\n","        #     else:\n","        #         if (np.sum(env.count_map) - env.k * env.l) <= 5:\n","        #             print(\"episode \", episode)\n","        #             print(env.T)\n","        #             break\n","        # if converge(epc_T):\n","        #     print(\"episode \", episode)\n","        #     print(env.T)\n","        #     break\n","        if converge(epc_T) or (episode > 3000 and (((np.sum(env.count_map) - env.k * env.l) <= 5)) and env.is_fully_covered()):\n","            print(\"episode \", episode)\n","            print(env.T)\n","            break\n","        # if converge(epc_T) or (episode > 3000 and (((np.sum(env.count_map) - env.k * env.l) <= 5)) and env.is_fully_covered()) or env.T < 150:\n","        #     print(\"episode \", episode)\n","        #     print(env.T)\n","        #     break\n","        # max_q = np.max([np.max(env.uavs[0].Q[o_obs_n[0]]), np.max(env.uavs[1].Q[o_obs_n[1]]), np.max(env.uavs[2].Q[o_obs_n[2]])])\n","        # if np.max(env.uavs[0].Q[o_obs_n[0]]) > 64 or np.max(env.uavs[1].Q[o_obs_n[1]]) > 64 or np.max(env.uavs[2].Q[o_obs_n[2]]) > 64:\n","        #     break\n","        # if len(Ts) > 0:\n","        #     min_T = np.min(Ts)\n","        #     if min_T\n","        # if (max_q not in env.uavs[0].Q[o_obs_n[0]]) or (max_q not in env.uavs[1].Q[o_obs_n[1]]) or (max_q not in env.uavs[2].Q[o_obs_n[2]]):\n","        #     print(\"max not in\")\n","        #     break \n","        # print(env.uavs[0].Q)\n","        # print()\n","        # if env.T < 165:\n","        #     break\n","        # print(env.num_repeated_obs)\n","        # # print(env.T)\n","        # print(env.uavs[0].tra_map)\n","        # print()\n","        # print(env.uavs[1].tra_map)\n","        # print()\n","        # print(env.uavs[2].tra_map)\n","        # print()\n","        # if same_gamma_point:\n","        #     break\n","        # if len(Ts) == 100:\n","        #     break\n","    # print(env.uavs[0].Q[o_action_n[0]])\n","    print()\n","    # print(env.uavs[1].Q[o_action_n[1]])\n","    # print()\n","    # print(env.uavs[2].Q[o_action_n[2]])\n","    # print()\n","    print(\"chosen actions \", o_action_n)\n","    print(env.count_map)\n","    print()\n","    if Ts:\n","        print(\"mean T = \", np.mean(Ts))\n","        print(\"min T = \", np.min(Ts))\n","    print(\"diag_count = \", diag_count)\n","    print()\n","    \n","    return Ts, final_rs, env, o_obs_n, obs_prev_obs, pos_tra, T_memory, cac, cac_T\n","    # Ts_s.append(Ts)\n","    # final_rs_s.append(final_rs)\n","    # ep_T.append(env.T)\n","    # print(\"chosen actions \", o_action_n, \" env.T \", env.T)\n","# for i in range(env.num_agents):\n","#     print(\"uav \", i)\n","#     for experience in memory[i]:\n","#         print(env.uavs[i].Q[experience[0]])\n","#     print()\n","    # c_Ts[time] = np.min(Ts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMqG5M-Kzlrz","colab_type":"code","colab":{}},"source":["def train_initial_action_n(explore_start, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs):\n","    Ts = []\n","    mean_Ts = []\n","    final_rs = []\n","    # diag_count = 0\n","    s_actions = [3, 5, 7, 9]\n","\n","    total_episodes = 10\n","    action_ns_tra = [[] for i in range(env.num_agents)]\n","    # explore_start = 1.0\n","    decay_step = 0\n","    explore_prob = -1\n","    # same_gamma_point = False\n","    max_size = 12\n","    epc_T = deque(np.zeros((max_size), dtype=np.float32), maxlen = max_size)\n","    exp_exp_tradeoff = 0\n","    for episode in range(total_episodes):\n","        # if episode >= num_T - 1:\n","        #     if np.sum(np.diff(Ts[episode : episode + num_T + 1])) / num_T <= 1:\n","        #         print(\"converged\")\n","        #         break\n","        ep_step = 0\n","        ac_size = [9 for i in range(env.num_agents)]\n","        memory = [[] for i in range(env.num_agents)]\n","        pos_tra = [[] for i in range(env.num_agents)]\n","        last_agent_id = 0\n","        obs_n, action_n = env.reset()\n","        action_n = np.copy(action_n_)\n","\n","        action_n = [1 for _ in range(env.num_agents)]\n","        for i in range(env.num_agents):\n","            # print(\"uav \", i, \" action_space = \", env.action_space(i, action_n))\n","            action_n[i], explore_p, ac_size[i], _ = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, ep_step, 0, exp_exp_tradeoff)\n","        # print(env.get_obs(0)[0])\n","\n","        for i in range(env.num_agents):\n","            action_ns_tra[i].append(action_n[i])\n","        o_obs_n = np.copy(obs_n)\n","        # print(obs_n[0])\n","        # break\n","        print(\"chosen actions \", action_n)\n","        # action_n = np.copy(action_n_)\n","        o_action_n = np.copy(action_n)\n","        prev_action_n = np.copy(action_n)\n","        done = False\n","        final_reward = 0\n","        diag_count = 0\n","        # first_step = True\n","        # action_n = [1 for _ in range(env.num_agents)]\n","        # if episode == 100:\n","        #     env.c1_T = 1.37\n","        #     env.c2_T = 1.14\n","        while not env.is_episode_finished():\n","            for i in range(env.num_agents):\n","                if not env.uavs[i].done:\n","                    # print(\"uav \", i)\n","                    # reward = env.reward_fn(i, action_n[i])\n","                    # print(\"reward = \", reward)\n","                    # print(action_n[i])\n","\n","                    if env.T == 0.3:\n","                        reward = env.reward_fn(i, action_n[i])\n","                    #     print(\"reward = \", reward)\n","\n","                    # if the same target gamma point is detected midway through due to r_c is too small\n","                    # update action\n","                    is_same_target_gamma_point = env.is_same_target_gamma_point(i, action_n)\n","                    if is_same_target_gamma_point:\n","                        # print(\"same target gamma point\")\n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # print(action_n[i])\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] -= env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] -= env.action_gamma_point_mapping[prev_action_n[i]][1]\n","                        prev_action_n[i], explore_p, ac_size[i], _ = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, 0, exp_exp_tradeoff)\n","                        if prev_action_n[i] in s_actions:\n","                            diag_count += 1  \n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        # print(\"changed action \", prev_action_n[i])\n","                        if explore_p != -1:\n","                            explore_prob = explore_p\n","                            # if (explore_p < 0.7) and (not is_decay_set):\n","                            #     decay_rate = 0.000008\n","                            #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                            #     is_decay_set = True\n","                        reward = env.reward_fn(i, prev_action_n[i])\n","                        if action_n[i] != 1:\n","                            action_n[i] = prev_action_n[i]\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] += env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] += env.action_gamma_point_mapping[prev_action_n[i]][1]\n","\n","                    # print(\"uav \", i, \" action_n[i] = \", action_n[i])\n","                    env.MAS_update(i, action_n[i])   # update target gamma point of uav i\n","                    pos_tra[i].append(env.uavs[i].pos)\n","                    # next_obs = env.fuse_maps(i, action_n, prev_action_n)\n","                    action_n[i] = 1\n","                    # tra_points_n[i].append(env.uavs[i].pos)\n","\n","                    is_target_reached = env.is_target_reached(i)\n","                    if is_target_reached:\n","                        # print(\"uav \", i, \" \", \"target \", env.uavs[i].target_gamma_idx[i] ,\"is reached\")\n","                        # print(\"env.T \", env.T)\n","                        # reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                        # print(\"target gamma point\")\n","                        # print(env.uavs[0].target_gamma_idx[0])\n","                        # print(env.uavs[1].target_gamma_idx[1])\n","                        # print(env.uavs[2].target_gamma_idx[2])\n","                        # print()\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # next_obs_n = [env.fuse_maps(k, action_n, prev_action_n) for k in range(env.num_agents)]\n","\n","                        # print()\n","                        # print(next_obs)\n","                        episodic_reward = env.eposidic_reward(False)\n","                        if episodic_reward == -1:\n","                            # print(\"prev_action_n \", prev_action_n)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            action_n[i], explore_p, ac_size[i], _ = env.select_action(i, next_obs, action_n, explore_start, explore_stop, decay_rate, decay_step, False, 0, exp_exp_tradeoff)\n","                            if action_n[i] in s_actions:\n","                                diag_count += 1  \n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward, env.uavs[i].done, ac_size[i]])\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            if explore_p != -1:\n","                                explore_prob = explore_p\n","                                # if (explore_p < 0.7) and (not is_decay_set):\n","                                #     print(\"set\")\n","                                #     decay_rate = 0.000008\n","                                #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                                #     is_decay_set = True\n","                            action_ns_tra[i].append(action_n[i])\n","                            # if not env.uavs[i].done:\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # else:\n","                            #     episodic_reward = env.eposidic_reward(True)\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward, env.is_episode_finished(), True)\n","                            # if obs_n[i] == next_obs:\n","                            #     print(\"states are the same\")\n","                            # else:\n","                            #     print(\"states are not the same\")\n","                            # if not env.uavs[i].done:\n","                            prev_action_n[i] = action_n[i]\n","                            # print(action_n)\n","                            # print()\n","                            obs_n[i] = next_obs\n","                        else:\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, env.is_episode_finished())\n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward, True, ac_size[i]])\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            # agent_ids = list(range(env.num_agents))\n","                            # del agent_ids[i]\n","                            # for id in range(len(memory[i]), -1, -1):\n","                            #     more_than_one = True\n","                            #     for ag_id in agent_ids:\n","                            #         if memory[ag_id][id][5] < 2:\n","                            #             more_than_one = False\n","                            #             print\n","                            #             break\n","                            #     if more_than_one: \n","                            #         experience = memory[i][id]:\n","                            # for experience in reversed(memory[i]):\n","                            #     if experience[5] > 1:\n","                            #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, episodic_reward, True)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            final_reward = episodic_reward\n","                            # print(\"uav \", i, \" obtain \" \"episodic_reward = \", episodic_reward)\n","                            done = True\n","                            break\n","                        reward = env.reward_fn(i, action_n[i])\n","                    # else:\n","                    #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, reward)\n","                    #     reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                    # print(\"uav \", i, \" \", env.uavs[i].target_gamma_idx[i])\n","\n","                    # for ag_id in range(env.num_agents):\n","                    #     tmp_x = env.uavs[ag_id].target_gamma_idx[ag_id][0] + env.action_gamma_point_mapping[action_n[ag_id]][0]\n","                    #     tmp_y = env.uavs[ag_id].target_gamma_idx[ag_id][1] + env.action_gamma_point_mapping[action_n[ag_id]][1]\n","                    #     print(\"uav \", ag_id, \" \", [tmp_x, tmp_y])\n","                    # print()\n","\n","                    # print(action_n)\n","                    # print(\"uav \", i, \" \", env.get_obs_n()[i])\n","                    # print(\"is_target_reached\")\n","                    # print(is_target_reached)\n","            # Increase decay_step\n","            decay_step +=1\n","\n","            # ep_step += 1\n","\n","            if done:\n","                break\n","\n","            env.T += env.t_step\n","            a1 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[1][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[1][1]\n","            a2 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[2][1]\n","            a3 = env.uavs[0].target_gamma_idx[1][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[1][1] == env.uavs[0].target_gamma_idx[2][1]\n","            if (a1 or a2 or a3) and (env.uavs[0].target_gamma_idx[0][0] != -1 and env.uavs[0].target_gamma_idx[1][0] != -1 and env.uavs[0].target_gamma_idx[2][0] != -1):\n","                # print(\"same gamma point\")\n","                # print(env.T)\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[0].target_gamma_idx[1])\n","                # print(env.uavs[0].target_gamma_idx[2])\n","                # print()\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[1].target_gamma_idx[1])\n","                # print(env.uavs[2].target_gamma_idx[2])\n","                same_gamma_point = True\n","                # break\n","            # cur_pos_x = [env.uavs[i].pos[0] for i in range(env.num_agents)]\n","            # cur_pos_y = [env.uavs[i].pos[1] for i in range(env.num_agents)]\n","            # target_pos_x = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[0] for i in range(env.num_agents)]\n","            # target_pos_y = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[1] for i in range(env.num_agents)]\n","            # fig, ax = plt.subplots()\n","            # ax.plot(cur_pos_x, cur_pos_y, 'o')\n","            # ax.plot(target_pos_x, target_pos_y, 'x')\n","            # plt.show()\n","            ep_step += 1\n","            # print()\n","            # if ep_step == 10:\n","            #     break\n","        # episodic_reward = env.eposidic_reward()\n","        # for i in range(env.num_agents):\n","        #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, episodic_reward)\n","        #     print(\"episodic_reward = \", episodic_reward)\n","            # print(env.uavs[i].Q)\n","        # print(\"len of memory[i] = \", len(memory[0]))\n","        print(\"Explore P \", explore_prob)\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        # Q-table update\n","        # for i in range(env.num_agents):\n","        #     # if i == 2:\n","        #     #     continue\n","        #     first_branch_set = False\n","        #     for step, experience in enumerate(reversed(memory[i])):\n","        #         num_prev_branches = len(obs_prev_obs[experience[2]])\n","        #         if step == 0:\n","        #         #     # print(\"done = \", experience[4])\n","        #             env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, num_prev_branches, True, True)\n","        #         else:\n","        #             env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, num_prev_branches, False, True)\n","\n","                # if experience[0] in obs_prev_obs:\n","                #     if (len(obs_prev_obs[experience[0]]) > 1) and (first_branch_set == False):\n","                #         first_branch_set = True\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 1, False, True)\n","                #     else:\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","\n","                # if step == 0:\n","                #     # print(\"done = \", experience[4])\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, step, True, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, step, False, True)\n","\n","        #     print()\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        if env.is_fully_covered():\n","            str_ep = \" is\"\n","            Ts.append(env.T)\n","            final_rs.append(final_reward)\n","            if len(Ts) >= 100:\n","                mean_Ts.append(np.mean(Ts[-100:]))\n","        else:\n","            str_ep = \" not\"\n","            # print()\n","            # print(o_action_n)\n","            # print(prev_action_n)\n","            # for ag_id in range(env.num_agents):\n","            #     cur_uav = env.uavs[ag_id]\n","            #     # target_pos = env.index_to_pos(cur_uav.target_gamma_idx[ag_id])\n","            #     # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","            #     # print(\"velocity = \", LA.norm(cur_uav.velocity))\n","            #     print(cur_uav.gamma_map)\n","            #     print(cur_uav.target_gamma_idx)\n","            #     print(cur_uav.done)\n","            # break\n","        print(\"episode \", episode, str_ep, \" fully covered\")\n","        print(env.T)\n","        epc_T.append(env.T)\n","        if converge(epc_T):\n","            break\n","        # max_q = np.max([np.max(env.uavs[0].Q[o_obs_n[0]]), np.max(env.uavs[1].Q[o_obs_n[1]]), np.max(env.uavs[2].Q[o_obs_n[2]])])\n","        # if np.max(env.uavs[0].Q[o_obs_n[0]]) > 64 or np.max(env.uavs[1].Q[o_obs_n[1]]) > 64 or np.max(env.uavs[2].Q[o_obs_n[2]]) > 64:\n","        #     break\n","        # if len(Ts) > 0:\n","        #     min_T = np.min(Ts)\n","        #     if min_T\n","        # if (max_q not in env.uavs[0].Q[o_obs_n[0]]) or (max_q not in env.uavs[1].Q[o_obs_n[1]]) or (max_q not in env.uavs[2].Q[o_obs_n[2]]):\n","        #     print(\"max not in\")\n","        #     break \n","        # print(env.uavs[0].Q)\n","        # print()\n","        # if env.T < 165:\n","        #     break\n","        # print(env.num_repeated_obs)\n","        # # print(env.T)\n","        # print(env.uavs[0].tra_map)\n","        # print()\n","        # print(env.uavs[1].tra_map)\n","        # print()\n","        # print(env.uavs[2].tra_map)\n","        # print()\n","        # if same_gamma_point:\n","        #     break\n","        # if len(Ts) == 100:\n","        #     break\n","    # print(env.uavs[0].Q[o_action_n[0]])\n","    # print()\n","    # print(env.uavs[1].Q[o_action_n[1]])\n","    # print()\n","    # print(env.uavs[2].Q[o_action_n[2]])\n","    # print()\n","    print(env.count_map)\n","    print()\n","    print(\"mean T = \", np.mean(Ts))\n","    print(\"min T = \", np.min(Ts))\n","    print(\"diag_count = \", diag_count)\n","\n","    return Ts, final_rs\n","    # Ts_s.append(Ts)\n","    # final_rs_s.append(final_rs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ao400vtChAHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1600189977646,"user_tz":-60,"elapsed":31306,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"23109d44-746b-4c2d-93b6-366bc13a28d1"},"source":["import csv\n","\n","Ts_s = []\n","final_rs_s = []\n","# r_c_set = list(range(10, 70, 10))\n","r_c_set = [60]\n","num_times = 1\n","# c_Ts = np.zeros(num_times)\n","c_Ts = []\n","for time in range(num_times):\n","    for r_c in r_c_set:\n","        Ts, final_rs, env, o_obs_n, obs_prev_obs, pos_tra, T_memory, cac, cac_T = train(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents, explore_start, explore_stop, decay_rate, decay_step)\n","    # c_Ts[time] = np.min(Ts)\n","    if Ts:\n","        c_Ts.append(Ts[-1])\n","\n","# print(np.mean(c_Ts)) \n","# print(np.var(c_Ts))\n","# plt.plot(cac_T, cac)\n","# coordinates = []\n","# for i in range(len(cac)):\n","#     coordinates.append([cac_T[i], cac[i]])\n","# # print(np.array(coordinates).shape)\n","# # with open('out.txt', 'wb') as f:\n","# #     csv.writer(f, delimiter=' ').writerows(np.array(coordinates))\n","with open('rc_60_first_convergence_', 'w') as fh:\n","    spamwriter = csv.writer(fh)\n","    for t in Ts:\n","        spamwriter.writerow([t])\n","\n","    # x = {}\n","    # for i in range(9):\n","    #     if env.uavs[0].Q[o_obs_n[0]][i] == 0:\n","    #         continue\n","    #     for j in range(9):\n","    #         if env.uavs[1].Q[o_obs_n[1]][j] == 0:\n","    #             continue\n","    #         for k in range(9):\n","    #             if env.uavs[2].Q[o_obs_n[2]][k] == 0:\n","    #                 continue\n","    #             # x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = str(i + 1) + str(j + 1) + str(k + 1)\n","    #             if i == 3 and j == 0 and k ==0:\n","    #                 print(env.uavs[0].Q[o_obs_n[0]][3])\n","    #                 print(env.uavs[1].Q[o_obs_n[1]][0])\n","    #                 print(env.uavs[2].Q[o_obs_n[2]][0])\n","    #             x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = [i + 1, j + 1, k + 1]\n","\n","    # # x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n","    # # x = {k:v for k, v in sorted(x.items(), key=lambda item: item[1])}\n","    # x = x.items()\n","    # x = sorted(x, reverse=True)\n","    # num = 0\n","    # for\n","    # action_n_ = [] \n","    # for i in range(env.num_agents):\n","    #     action_n_.append(sorted(T_memory[i].items())[0][1][0][1])\n","    # Ts, final_rs = train_initial_action_n(1.0, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs)\n","\n","    # for action_n_ in x:\n","    #     Ts, final_rs = train_initial_action_n(0.2, explore_stop, decay_rate, decay_step, action_n_[1], env, obs_prev_obs)\n","    #     Ts_s.append(np.min(Ts))\n","    #     num += 1\n","    #     if num == 4:\n","    #         break\n","    # final_rs_s.append(final_rs)\n","    # c_Ts[time] = np.min(Ts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[6, 3]\n","[5, 1]\n","[1, 6]\n","episode  112\n","150.3\n","\n","chosen actions  [1 1 1]\n","[[1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 2. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 2. 2. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]]\n","\n","mean T =  175.57570093457943\n","min T =  150.3\n","diag_count =  3\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J4Qbz11ILBlr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600113116297,"user_tz":-60,"elapsed":745,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"c46f71b1-e4b5-47aa-e9aa-ad067f00489c"},"source":["print(Ts)\n","with open('rc_20_first', 'w') as fh:\n","    spamwriter = csv.writer(fh)\n","    for t in Ts:\n","        spamwriter.writerow([t])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[227.3, 202.3, 204.8, 223.8, 198.3, 222.3, 218.3, 234.3, 184.3, 207.3, 201.8, 209.8, 175.3, 177.3, 179.3, 173.3, 170.8, 170.8, 174.3, 170.8, 173.8, 171.3, 174.3, 173.3, 164.8, 161.3, 164.8, 173.8, 180.3, 154.8, 154.8, 168.3, 154.8, 174.3, 181.8, 178.8, 173.3, 168.8, 154.8, 174.8, 201.3, 168.3, 168.3, 170.3, 154.8, 169.8, 164.3, 165.3, 165.3, 192.8, 184.8, 172.3, 172.3, 189.3, 188.3, 188.3, 168.8, 168.8, 185.8, 202.8, 202.8, 178.8, 168.8, 154.8, 188.3, 213.8, 169.8, 188.3, 181.8, 179.3, 198.8, 191.3, 202.8, 202.8, 202.8, 182.3, 205.8, 203.3, 203.3, 201.3, 170.8, 185.3, 206.3, 201.8, 201.8, 201.8, 196.8, 193.3, 167.8, 198.3, 185.3, 193.8, 179.8, 190.3, 189.8, 191.8, 185.8, 192.3, 189.3, 189.3, 199.3, 195.3, 199.3, 191.8, 182.8, 161.3, 185.3, 185.3, 201.3, 198.8, 198.8, 200.3, 163.8, 157.8, 168.8, 159.8, 191.3, 156.8, 166.3, 154.3, 160.8, 178.8, 150.8, 163.8, 160.3, 164.3, 164.3, 164.3, 161.8, 167.3, 173.8, 179.3, 179.3, 175.8, 160.3, 150.3, 201.3, 194.8, 194.8, 194.8, 161.3, 160.3, 170.8, 170.8, 170.8, 189.3, 171.3, 170.8, 185.3, 171.3, 185.3, 181.3, 181.3, 169.8, 169.8, 175.3, 171.3, 171.3, 179.3, 179.3, 175.3, 175.3, 175.3, 193.3, 175.3, 170.8, 170.8, 176.3, 170.8, 176.8, 163.8, 191.8, 163.8, 157.3, 153.8, 160.8, 216.3, 151.8, 151.8, 151.8, 153.8, 191.8, 163.3, 183.3, 183.3, 204.3, 179.3, 166.8, 166.8, 174.8, 154.3, 154.3, 154.3, 154.3, 154.3, 154.3, 178.8, 183.8, 147.8, 147.8, 147.8, 154.3, 153.8, 175.3, 187.8, 178.3, 169.8, 185.3, 193.3, 162.8, 162.8, 162.8, 162.8, 162.8, 162.8, 162.8, 160.3, 163.8, 163.8, 163.8, 163.8, 159.8, 159.8, 165.3, 157.8, 182.3, 159.8, 163.3, 163.3, 167.8, 154.8, 165.8, 160.8, 194.3, 189.8, 198.8, 162.8, 175.8, 175.8, 182.3, 181.8, 160.8, 204.8, 185.8, 176.3, 203.3, 198.8, 168.8, 185.3, 179.3, 192.8, 182.8, 198.3, 182.8, 162.8, 162.8, 162.8, 189.3, 223.3, 232.3, 163.3, 179.8, 179.8, 179.8, 167.8, 189.8, 189.8, 158.8, 163.8, 160.3, 162.8, 204.8, 185.8, 179.3, 179.3, 185.8, 169.8, 195.8, 185.8, 180.3, 179.3, 169.8, 175.8, 219.3, 198.8, 207.3, 189.8, 193.3, 187.3, 202.3, 175.8, 181.8, 185.3, 175.8, 175.8, 179.3, 203.8, 185.3, 202.3, 189.8, 184.3, 167.8, 183.3, 154.8, 185.8, 163.8, 194.3, 188.8, 175.8, 185.3, 184.8, 185.8, 179.8, 162.8, 162.8, 177.3, 230.8, 162.8, 162.8, 162.8, 162.8, 162.8, 156.3, 163.3, 167.3, 163.3, 166.3, 182.3, 182.3, 156.3, 162.8, 162.8, 188.8, 159.8, 167.3, 162.8, 175.8, 194.8, 178.3, 175.8, 181.3, 181.3, 190.3, 180.8, 181.3, 162.8, 180.3, 184.8, 162.8, 162.8, 162.8, 162.8, 162.8, 162.8, 184.8, 195.8, 200.3, 231.8, 161.3, 188.8, 188.8, 229.3, 166.3, 180.8, 175.8, 185.3, 176.3, 179.3, 187.3, 175.8, 179.3, 187.3, 169.8, 169.8, 168.8, 159.8, 179.8, 172.8, 185.8, 208.8, 189.3, 184.3, 163.3, 161.8, 187.8, 162.8, 171.3, 157.8, 154.8, 165.3, 160.3, 159.8, 153.8, 153.8, 153.8, 153.8, 156.3, 153.8, 157.3, 157.3, 157.3, 153.8, 157.3, 156.8, 159.8, 182.3, 172.8, 186.8, 186.8, 157.8, 175.8, 179.8, 160.8, 164.3, 160.8, 160.8, 160.8, 164.3, 164.3, 150.8, 159.8, 153.3, 163.3, 160.3, 180.3, 166.8, 166.3, 169.3, 166.8, 196.3, 172.3, 169.8, 153.8, 153.8, 179.8, 153.8, 153.8, 170.3, 170.3, 159.8, 170.3, 175.8, 189.8, 189.8, 170.3, 185.3, 190.3, 190.8, 166.8, 173.8, 176.3, 163.8, 163.8, 176.3, 173.3, 169.8, 160.8, 184.8, 160.8, 153.8, 157.3, 174.3, 163.3, 158.3, 157.8, 157.8, 157.8, 192.3, 157.8, 205.3, 239.3, 204.3, 179.8, 216.3, 206.8, 248.3, 196.8, 194.3, 181.3, 188.8, 169.8, 183.3, 168.3, 170.3, 182.8, 163.8, 161.3, 168.3, 158.8, 189.3, 168.8, 171.8, 171.8, 211.3, 174.3, 171.3, 171.3, 168.3, 164.8, 169.8, 165.8, 172.3, 175.3, 165.3, 164.8, 168.3, 168.3, 177.8, 182.3, 170.3, 176.3, 176.3, 165.8, 215.3, 165.8, 222.3, 222.3, 168.3, 194.8, 195.8, 171.8, 190.8, 152.3, 188.3, 186.8, 195.8, 192.3, 190.3, 190.3, 199.3, 181.8, 192.3, 175.8, 202.3, 223.3, 185.3, 189.3, 170.8, 167.3, 178.3, 171.8, 164.8, 167.3, 188.3, 171.3, 160.8, 154.3, 191.3, 191.3, 154.3, 187.8, 169.8, 164.8, 165.3, 180.8, 170.3, 160.3, 157.3, 154.3, 166.8, 215.3, 163.8, 189.8, 178.3, 182.3, 161.3, 194.8, 202.3, 182.3, 167.3, 157.8, 172.3, 173.3, 189.3, 167.3, 167.3, 172.3, 217.3, 219.3, 219.3, 199.3, 172.8, 170.8, 154.3, 169.8, 160.8, 187.8, 185.3, 167.8, 179.8, 164.3, 178.8, 217.8, 163.3, 170.8, 159.8, 185.3, 170.8, 180.3, 196.3, 178.3, 159.8, 187.3, 156.3, 187.3, 175.3, 177.8, 170.8, 177.3, 177.8, 170.8, 162.3, 156.3, 156.3, 179.8, 196.8, 235.3, 242.3, 208.3, 232.3, 252.3, 240.3, 189.8, 177.8, 188.8, 181.8, 183.8, 175.8, 191.3, 167.8, 167.8, 155.3, 165.3, 149.8, 148.8, 157.3, 157.3, 154.8, 152.3, 149.8, 157.8, 148.3, 145.8, 145.8, 154.8, 154.8, 154.8, 150.3, 150.3, 163.3, 160.8, 162.8, 173.8, 149.3, 149.3, 149.3, 168.8, 188.8, 167.8, 167.8, 192.8, 165.3, 160.3, 171.3, 156.8, 171.3, 163.8, 173.8, 152.8, 151.8, 151.8, 149.3, 151.8, 159.8, 167.8, 167.8, 158.8, 173.8, 162.3, 155.3, 155.3, 155.3, 163.8, 150.3, 161.3, 151.3, 161.3, 158.3, 157.8, 166.3, 166.8, 171.8, 171.8, 155.8, 155.8, 158.3, 155.8, 170.8, 155.3, 148.8, 160.3, 154.8, 148.8, 164.8, 154.3, 167.8, 165.8, 165.8, 154.8, 165.8, 157.3, 165.8, 165.3, 171.8, 149.8, 162.8, 176.8, 179.8, 154.8, 151.3, 151.3, 150.8, 161.3, 172.8, 177.8, 185.8, 167.8, 164.3, 173.3, 183.8, 158.8, 150.8, 163.3, 163.3, 171.8, 147.8, 168.8, 213.8, 207.3, 214.3, 204.3, 147.8, 147.8, 158.3, 214.8, 176.8, 173.3, 169.3, 198.8, 151.8, 151.8, 151.8, 151.8, 169.3, 164.8, 156.3, 156.3, 156.3, 156.3, 156.3, 159.3, 170.8, 165.3, 165.3, 165.3, 172.3, 155.8, 162.8, 162.8, 162.3, 162.8, 155.8, 162.8, 155.8, 159.3, 156.3, 180.8, 156.3, 167.8, 167.8, 206.8, 221.3, 175.3, 181.3, 182.3, 182.3, 164.3, 154.8, 152.8, 157.3, 153.3, 164.8, 151.3, 154.8, 154.8, 154.8, 163.8, 142.8, 170.3, 142.8, 178.8, 154.8, 178.8, 160.3, 160.3, 193.3, 187.3, 190.8, 190.8, 173.8, 158.8, 167.8, 155.3, 175.3, 177.8, 178.8, 156.8, 204.8, 204.8, 204.8, 204.8, 208.3, 204.8, 179.8, 174.3, 187.3, 193.3, 193.8, 190.3, 194.8, 194.8, 190.8, 169.8, 179.8, 179.8, 160.3, 160.3, 200.3, 236.8, 207.3, 212.8, 196.8, 212.8, 196.8, 225.8, 229.3, 210.3, 204.3, 162.8, 236.8, 198.3, 198.3, 198.3, 216.8, 207.8, 211.8, 170.8, 198.3, 167.8, 193.8, 185.3, 178.3, 163.3, 164.3, 168.8, 192.8, 180.8, 230.8, 175.3, 195.8, 180.8, 167.8, 179.3, 179.3, 179.3, 192.3, 182.8, 176.8, 183.3, 187.3, 194.3, 164.3, 154.8, 198.8, 186.8, 191.8, 184.8, 151.3, 184.8, 184.8, 151.8, 199.3, 151.8, 154.8, 164.8, 185.8, 148.8, 166.8, 197.3, 187.8, 198.3, 196.8, 188.8, 170.3, 151.8, 166.3, 166.3, 205.8, 196.3, 195.8, 160.3, 189.8, 188.8, 174.3, 210.3, 170.3, 225.3, 197.3, 191.8, 191.8, 209.8, 191.8, 181.3, 227.8, 232.3, 237.8, 232.3, 181.3, 232.3, 206.8, 202.8, 196.8, 174.3, 187.3, 187.3, 225.3, 219.8, 219.8, 241.8, 252.3, 252.3, 231.3, 252.3, 207.3, 194.8, 234.3, 187.3, 174.3, 174.3, 181.8, 169.3, 154.8, 185.3, 169.8, 169.8, 160.3, 154.8, 167.8, 164.3, 169.3, 169.3, 179.3, 201.3, 187.3, 187.3, 189.3, 187.3, 207.8, 216.8, 226.8, 203.8, 204.3, 204.3, 161.3, 199.8, 216.8, 216.8, 216.8, 195.8, 194.3, 194.3, 156.8, 216.8, 189.8, 175.3, 187.3, 154.8, 151.3, 154.8, 154.8, 156.3, 158.8, 171.8, 201.3, 160.8, 160.8, 151.3, 152.3, 158.3, 169.8, 157.8, 202.8, 157.8, 173.8, 154.8, 170.3, 149.3, 170.3, 166.3, 160.8, 166.8, 177.3, 184.8, 180.3, 172.3, 187.8, 210.8, 211.3, 191.8, 160.3, 156.8, 156.8, 189.8, 157.8, 189.8, 230.8, 193.3, 202.8, 201.8, 176.8, 164.3, 164.3, 159.3, 154.8, 154.8, 154.8, 171.8, 154.8, 193.8, 162.8, 164.8, 155.3, 155.8, 155.8, 155.8, 155.8, 154.8, 155.8, 172.8, 171.3, 168.8, 169.8, 182.8, 166.3, 183.8, 183.8, 183.8, 164.8, 148.8, 151.8, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3, 149.3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2P4LtX9ELn4j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1599609939818,"user_tz":-60,"elapsed":752,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"2936a39c-b623-446d-8809-80b27b0671a3"},"source":["plt.plot(Ts) "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f79f96b7470>]"]},"metadata":{"tags":[]},"execution_count":80},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"wBlxfwsy3E6W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599840119825,"user_tz":-60,"elapsed":526,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"29e8caff-ec26-4ac5-ee85-b194cf52de35"},"source":["res_ = [156.8, 155.3, 157.8, 150.8, 141.8, 151.8, 162.3, 144.3] # r_c = 60\n","# res_ = [161.3, 142.3, 165.3, 155.8, 155.3, 154.8, 150.8, 156.3, 160.8, 157.3, 140.8] # r_c = 50\n","print(np.mean(res_))\n","print(np.var(res_))\n","# print(np.mean(c_Ts))\n","# print(np.var(c_Ts))\n","# print(np.array(coordinates))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["152.6125\n","41.93359375\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3HTd21o5Suu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":901},"executionInfo":{"status":"ok","timestamp":1596920991523,"user_tz":-60,"elapsed":3610,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"023c3514-fbef-4237-aa13-6110a36cb915"},"source":["action_n_ = [] \n","for i in range(env.num_agents):\n","    action_n_.append(sorted(T_memory[i].items())[0][1][0][1])\n","Ts, final_rs = train_initial_action_n(-1.0, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  0  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  2  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  3  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  4  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  5  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  6  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  7  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  8  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  9  is  fully covered\n","174.3\n","[[1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 2. 1. 1. 1. 1. 1. 1.]\n"," [2. 1. 1. 1. 1. 1. 1. 1.]\n"," [2. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 2. 1. 1. 1.]\n"," [1. 2. 1. 2. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 2. 1.]]\n","\n","mean T =  174.3\n","min T =  174.3\n","diag_count =  10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AF6Kn3utA4Sn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"status":"ok","timestamp":1596720760146,"user_tz":-60,"elapsed":764,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"72358d02-7614-4c9e-fc38-58ea31a58a0e"},"source":["# print(np.min(Ts))\n","# print(env.index_to_pos([2, 1]))\n","x = T_memory[2]\n","print(len(T_memory[1]))\n","# print(T_memory[0])\n","print(len(Ts))\n","x = sorted(T_memory[0].items())\n","# memory = [v for k, v in sorted(T_memory[0].items(), key=lambda item: item[1])]\n","# y = [k for k, v in sorted(T_memory[0].items(), key=lambda item: item[1])]\n","first = 0\n","second = 3\n","print(x[first][0])\n","print(x[second][0])\n","print(len(x[first][1]))\n","print(len(x[second][1]))\n","for i in range(len(x[first][1])):\n","    if i == len(x[second][1]):\n","        break\n","    else:\n","        if x[first][1][i][0] == x[second][1][i][0]:\n","            print(\"state \", i, \" is the same\")\n","        else:\n","            print(\"state \", i, \" is different\")\n","# print(x[0][1][12][0], x[0][1][12][1])\n","# print(x[1][1][12][0], x[1][1][12][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["185\n","999\n","142.8\n","147.3\n","21\n","21\n","state  0  is the same\n","state  1  is the same\n","state  2  is the same\n","state  3  is different\n","state  4  is different\n","state  5  is different\n","state  6  is different\n","state  7  is different\n","state  8  is different\n","state  9  is different\n","state  10  is different\n","state  11  is different\n","state  12  is different\n","state  13  is different\n","state  14  is different\n","state  15  is different\n","state  16  is different\n","state  17  is different\n","state  18  is different\n","state  19  is different\n","state  20  is different\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KisyvGkdrQ4Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597003736578,"user_tz":-60,"elapsed":918,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"3c9ed282-ea6e-4ffa-cece-6cd28554c95a"},"source":["T = 142.3\n","for i in range(env.num_agents):\n","    memory = T_memory[i][T]\n","    print(\"uav \", i)\n","    for epr in memory:\n","        print(env.uavs[i].Q[epr[0]])\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["uav  0\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.    0.   -0.45  0.    0.    0.    0.    0.    0.  ]\n","[ 0.000000e+00 -4.915200e-04 -4.500000e-01 -1.572864e-04 -4.500000e-01\n","  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00]\n","[ 0.000000e+00  0.000000e+00 -4.500000e-01 -6.291456e-05 -4.500000e-01\n","  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00]\n","[ 0.00000000e+00  0.00000000e+00 -4.50000000e-01 -5.03316480e-05\n"," -4.50000000e-01 -3.22122547e-05 -4.50000000e-01  0.00000000e+00\n","  0.00000000e+00]\n","[ 0.    0.    0.    0.   -0.45  0.    0.    0.    0.  ]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.    0.   -0.45  0.    0.    0.    0.    0.    0.  ]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0.         0.02645612 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         0.03307015 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         0.08267536 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[ 0.         -0.003072    0.          0.          0.          0.\n","  0.          0.20668841  0.        ]\n","[0.         0.51672102 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[ 0.         -0.04453633 -0.45        1.29180256  0.          0.\n","  0.          0.          0.        ]\n","[0.        3.2295064 0.        0.        0.        0.        0.\n"," 0.        0.       ]\n","[0.         8.07376601 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[ 0.          0.          0.          0.          0.          0.\n"," -0.45       20.18441503  0.        ]\n","[ 0.          0.          0.          0.          0.         50.46103757\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.          0.\n","  0.          0.         63.07629696]\n","\n","uav  1\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.       0.      -0.45    -0.00768 -0.45     0.      -0.45     0.\n"," -0.45   ]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.    0.    0.    0.    0.    0.   -0.45  0.    0.  ]\n","[0.         0.         0.         0.         0.         0.10582447\n"," 0.         0.         0.        ]\n","[0.         0.         0.         0.         0.         0.\n"," 0.         0.26456116 0.        ]\n","[0.         0.66140291 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         1.65350728 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.        4.1337682 0.        0.        0.        0.        0.\n"," 0.        0.       ]\n","[ 0.         10.33442049  0.          0.          0.          0.\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.          0.\n"," -0.3        25.83605123  0.        ]\n","[ 0.          0.          0.          0.          0.         32.29506404\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.         40.36883005\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.         50.46103757\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.         63.07629696\n","  0.          0.          0.        ]\n","\n","uav  2\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.00000000e+00 -1.61061274e-06 -4.50000000e-01 -1.53600000e-02\n"," -4.50000000e-01  0.00000000e+00 -4.50000000e-01  0.00000000e+00\n","  0.00000000e+00]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.     0.     0.    -0.048  0.     0.    -0.45   0.     0.   ]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[ 0.    0.    0.    0.    0.    0.   -0.45  0.    0.  ]\n","[0.         0.05291223 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         0.13228058 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         0.33070146 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[0.         0.82675364 0.         0.         0.         0.\n"," 0.         0.         0.        ]\n","[ 0.        -0.0192     0.         0.         0.         0.\n"," -0.45       2.0668841  0.       ]\n","[ 0.         -0.13917603 -0.3         0.          0.          5.16721025\n","  0.          0.          0.        ]\n","[ 0.          0.          0.          0.          0.          0.\n"," -0.3        12.91802562  0.        ]\n","[ 0.         16.14753202  0.          0.          0.          0.\n","  0.          0.          0.        ]\n","[ 0.         40.36883005  0.          0.          0.          0.\n","  0.          0.          0.        ]\n","[ 0.          0.          0.         50.46103757  0.          0.\n","  0.          0.          0.        ]\n","[ 0.          0.          0.         63.07629696  0.          0.\n","  0.          0.          0.        ]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V4fWBbZ3YXkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597147029487,"user_tz":-60,"elapsed":929,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"2048073e-b2fc-4864-e770-e8a7bcc19260"},"source":["print(Ts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[212.3, 190.3, 206.3, 206.3, 201.8, 212.3, 231.8, 207.8, 215.3, 195.3, 195.8, 214.8, 178.8, 192.3, 175.8, 175.8, 205.3, 187.3, 215.3, 236.8, 245.8, 230.8, 219.8, 209.8, 203.8, 186.8, 212.8, 257.3, 232.3, 194.8, 191.8, 188.3, 236.3, 190.8, 217.8, 198.3, 186.8, 194.8, 193.3, 252.3, 225.8, 199.8, 187.8, 189.3, 186.3, 207.3, 219.3, 195.8, 221.3, 183.8, 224.8, 228.3, 245.3, 232.8, 226.3, 233.3, 210.8, 212.3, 211.8, 193.3, 195.8, 209.3, 192.8, 210.8, 196.3, 250.3, 188.8, 240.8, 269.8, 191.3, 200.3, 179.3, 198.3, 198.3, 211.8, 245.8, 211.8, 172.3, 173.3, 187.3, 187.3, 210.3, 197.8, 177.3, 279.3, 254.8, 249.8, 250.8, 275.8, 262.3, 246.3, 218.3, 202.3, 229.3, 222.3, 226.3, 182.8, 227.8, 180.3, 179.3, 197.3, 216.3, 188.8, 196.8, 176.3, 164.8, 186.8, 194.8, 187.8, 188.3, 191.3, 225.8, 225.8, 181.8, 186.3, 183.8, 227.3, 188.3, 189.8, 243.8, 229.8, 190.8, 219.3, 246.3, 240.3, 200.3, 194.8, 195.3, 182.8, 179.3, 234.3, 179.3, 188.8, 198.8, 172.3, 176.3, 186.3, 172.8, 209.3, 171.8, 178.8, 183.3, 182.8, 194.8, 178.3, 181.3, 205.3, 195.3, 176.3, 175.8, 167.8, 182.3, 165.3, 181.3, 189.3, 198.3, 172.3, 198.3, 181.3, 176.3, 186.8, 165.3, 181.3, 173.8, 182.3, 191.3, 172.8, 197.8, 228.3, 206.8, 168.3, 205.3, 228.3, 216.8, 198.3, 176.3, 195.8, 201.8, 178.8, 237.8, 213.3, 235.8, 263.3, 189.3, 202.8, 215.8, 199.8, 186.3, 181.8, 175.8, 177.8, 202.3, 215.8, 210.8, 237.8, 206.8, 215.3, 205.3, 184.3, 214.8, 176.8, 173.3, 174.3, 190.8, 190.8, 183.8, 216.8, 207.8, 205.8, 197.8, 173.3, 182.8, 198.8, 211.8, 198.3, 194.3, 170.8, 178.3, 200.8, 213.3, 243.3, 180.8, 196.3, 190.8, 170.3, 191.3, 159.8, 169.3, 184.3, 164.3, 188.3, 165.3, 191.8, 176.8, 175.3, 174.8, 251.3, 180.8, 181.8, 226.3, 201.8, 214.8, 227.3, 207.3, 202.8, 227.3, 207.3, 184.3, 185.3, 186.3, 204.3, 207.8, 196.3, 187.3, 214.3, 205.3, 196.3, 272.3, 243.3, 193.8, 214.3, 169.3, 195.8, 177.3, 226.3, 196.8, 197.8, 211.3, 190.8, 189.3, 198.8, 193.8, 162.3, 186.8, 174.8, 185.3, 182.8, 197.8, 191.3, 176.3, 188.3, 189.8, 199.3, 183.8, 177.8, 187.8, 191.8, 184.8, 164.3, 191.3, 195.8, 185.8, 198.8, 200.3, 206.8, 183.3, 184.8, 242.8, 189.3, 194.3, 193.3, 212.8, 186.3, 193.3, 186.3, 186.3, 172.8, 172.8, 172.8, 183.8, 172.8, 191.3, 184.3, 178.3, 164.3, 195.8, 184.3, 195.3, 179.3, 175.8, 164.3, 189.3, 233.3, 181.3, 190.3, 175.3, 192.8, 198.8, 178.8, 198.3, 165.3, 178.3, 195.3, 171.3, 172.3, 164.8, 187.8, 181.8, 189.8, 164.8, 170.8, 164.3, 166.3, 180.3, 220.3, 191.8, 206.3, 187.8, 234.3, 219.3, 193.3, 238.3, 171.8, 188.3, 182.8, 189.3, 183.8, 184.8, 192.3, 180.3, 184.3, 194.3, 177.8, 193.3, 187.8, 178.3, 171.3, 165.3, 201.3, 179.3, 178.8, 202.8, 203.8, 176.3, 189.3, 189.3, 181.8, 201.8, 164.3, 190.8, 288.8, 257.8, 252.8, 270.8, 216.8, 216.3, 262.3, 234.3, 225.3, 225.3, 185.3, 202.8, 177.3, 201.8, 181.8, 179.3, 197.3, 187.8, 194.8, 243.8, 192.8, 190.8, 173.8, 242.3, 181.8, 175.8, 186.8, 167.8, 185.8, 189.8, 189.8, 194.3, 200.8, 191.8, 196.3, 180.3, 181.8, 184.8, 186.3, 186.3, 177.8, 191.8, 182.8, 188.8, 207.3, 199.8, 196.3, 205.8, 183.3, 185.8, 186.8, 209.8, 166.8, 164.8, 166.3, 189.8, 191.8, 192.3, 203.3, 189.3, 182.3, 190.8, 196.8, 179.8, 191.3, 196.8, 187.3, 165.3, 178.3, 201.8, 210.8, 208.8, 190.3, 193.8, 201.3, 193.8, 197.3, 173.8, 188.8, 188.8, 188.8, 173.8, 200.8, 205.8, 205.3, 211.3, 194.8, 192.3, 175.3, 174.8, 174.8, 186.8, 194.3, 189.8, 225.8, 188.8, 167.8, 207.8, 221.8, 207.8, 223.8, 207.8, 244.3, 167.8, 213.8, 167.8, 186.3, 182.3, 181.3, 181.3, 173.8, 219.3, 181.8, 196.8, 211.3, 193.8, 193.8, 206.8, 182.3, 206.8, 262.3, 216.8, 192.3, 195.3, 191.3, 167.8, 196.8, 196.8, 196.8, 195.8, 192.3, 181.8, 196.3, 193.3, 219.3, 186.3, 180.3, 214.3, 168.8, 181.3, 211.3, 181.8, 185.3, 183.8, 203.3, 234.3, 178.8, 231.3, 184.8, 201.8, 212.8, 229.8, 232.3, 227.3, 208.8, 177.3, 183.3, 184.8, 185.8, 190.3, 180.3, 183.8, 206.3, 243.3, 179.3, 202.8, 208.3, 198.8, 177.8, 231.8, 172.3, 198.3, 181.8, 176.8, 174.3, 188.3, 185.3, 167.3, 168.8, 171.3, 211.8, 177.8, 176.8, 176.8, 183.8, 206.8, 175.3, 175.3, 176.3, 181.3, 180.8, 181.8, 194.3, 187.8, 194.3, 192.8, 213.3, 239.3, 187.8, 191.3, 171.8, 280.8, 215.3, 187.3, 199.8, 199.3, 218.8, 203.3, 187.3, 240.3, 174.3, 213.8, 185.8, 214.8, 254.3, 247.3, 214.8, 167.8, 208.3, 172.3, 169.3, 174.8, 203.8, 180.8, 188.8, 192.8, 181.3, 167.8, 230.3, 192.8, 200.8, 184.8, 187.3, 172.8, 176.8, 200.8, 173.3, 182.3, 177.3, 186.3, 198.3, 198.3, 189.3, 201.3, 176.8, 167.8, 199.8, 199.8, 199.8, 181.3, 201.8, 175.3, 187.8, 178.3, 199.8, 175.3, 175.3, 209.3, 194.8, 225.8, 179.8, 200.8, 197.8, 166.8, 195.8, 191.3, 200.3, 208.3, 178.8, 193.8, 209.3, 171.3, 165.3, 202.8, 208.8, 203.3, 196.8, 220.8, 215.8, 187.8, 215.8, 177.8, 196.8, 210.8, 183.3, 214.8, 199.8, 275.3, 206.3, 262.3, 194.8, 273.8, 208.3, 203.3, 214.3, 208.8, 187.3, 193.3, 173.8, 187.3, 214.3, 200.8, 214.3, 187.3, 205.8, 180.8, 200.3, 213.3, 187.8, 184.8, 195.3, 204.8, 219.3, 187.3, 183.8, 183.8, 195.8, 199.3, 187.3, 187.3, 186.8, 190.8, 166.3, 205.3, 196.8, 198.3, 208.3, 203.8, 182.3, 182.3, 204.8, 179.8, 179.8, 182.3, 187.3, 165.3, 178.3, 187.3, 205.3, 218.8, 228.3, 205.3, 228.3, 202.8, 209.8, 196.3, 198.3, 197.8, 214.8, 209.8, 210.8, 218.3, 182.3, 173.3, 208.3, 205.8, 175.3, 175.3, 175.3, 204.8, 193.3, 223.8, 202.8, 188.8, 208.3, 186.8, 202.8, 198.8, 203.8, 211.8, 171.3, 253.8, 174.3, 167.8, 269.8, 206.8, 204.3, 206.3, 204.3, 278.3, 185.8, 194.8, 223.3, 211.8, 211.8, 211.8, 171.3, 203.8, 288.3, 194.3, 199.8, 203.3, 188.3, 167.8, 222.3, 196.3, 213.3, 206.8, 200.8, 187.8, 207.3, 182.8, 219.8, 199.8, 187.3, 273.3, 168.8, 175.3, 161.8, 206.8, 186.3, 170.8, 183.3, 196.3, 177.3, 182.3, 170.8, 168.8, 182.8, 174.8, 195.3, 202.3, 185.8, 190.8, 167.8, 187.3, 200.8, 191.8, 203.8, 194.8, 187.3, 201.3, 206.8, 192.8, 204.3, 184.3, 182.3, 167.3, 167.8, 185.8, 180.3, 197.8, 166.8, 200.3, 183.3, 183.3, 181.8, 177.8, 203.3, 211.3, 167.8, 172.3, 206.8, 198.3, 198.8, 206.3, 193.8, 193.8, 206.3, 208.3, 205.8, 197.3, 186.3, 191.8, 188.3, 192.3, 162.3, 178.3, 181.8, 187.8, 183.3, 198.3, 209.8, 183.3, 183.3, 207.8, 196.8, 195.3, 181.3, 180.8, 173.3, 198.8, 204.3, 171.8, 170.3, 190.3, 202.3, 201.3, 217.3, 197.3, 200.8, 197.3, 203.8, 186.3, 178.3, 163.8, 182.8, 180.3, 175.3, 174.8, 179.3, 226.3, 208.8, 207.8, 228.3, 188.8, 167.8, 170.3, 222.3, 214.8, 189.3, 179.8, 176.8, 167.8, 210.8, 204.8, 180.8, 188.3, 174.8, 176.8, 172.8, 190.8, 173.8, 194.8, 177.8, 182.3, 207.3, 217.8, 221.3, 221.3, 176.8, 218.3, 208.3, 181.8, 189.8, 193.8, 193.8, 191.8, 167.8, 191.8, 182.3, 182.3, 182.3, 200.8, 191.3, 189.8, 188.8, 183.8, 190.8, 213.8, 165.3, 181.8, 177.3, 168.8, 171.8, 172.3, 175.8, 186.8, 194.3, 224.3, 217.8, 176.8, 167.8, 214.8, 218.3, 219.3, 214.3, 189.3, 171.3, 167.8, 215.8, 167.8, 179.3, 184.3, 161.3, 184.8, 186.3, 177.8, 176.3, 185.8, 207.3, 193.8, 178.3, 188.3, 178.3, 182.3, 198.8, 175.3, 193.8, 162.3, 200.3, 169.8, 169.3, 192.3, 195.3, 166.3, 163.3, 184.3, 194.8, 211.3, 182.8, 177.3, 177.3, 185.8, 176.3, 201.8, 204.3, 192.8, 157.8, 158.3, 178.3, 173.8, 178.8, 185.3, 185.8, 169.3, 173.3, 178.8, 202.3, 170.3, 183.3, 211.3, 182.3, 166.8, 177.8, 187.8, 178.3, 181.8, 181.8, 171.3, 171.3, 173.3, 188.3, 168.8, 181.8, 178.3, 173.3, 183.3, 171.3, 239.3, 166.3, 168.3, 204.8, 172.8, 190.8, 218.8, 166.8, 166.8, 209.3, 220.8, 182.3, 218.3, 184.8, 178.8, 217.3, 203.8, 185.3, 166.8, 200.3, 196.3, 205.8, 196.8, 172.3, 187.3, 204.8, 165.3, 172.3, 200.3, 199.3, 184.8, 205.8, 196.3, 204.3, 199.3, 203.3, 190.3, 222.3, 183.3, 187.3, 195.3, 196.8, 178.3, 196.8, 167.3, 172.3, 194.3, 173.3, 176.8, 181.8, 204.8, 179.8, 185.8, 185.8, 211.8, 173.8, 197.8, 242.3, 180.3, 185.8, 209.3, 201.8, 188.3, 196.3, 208.8, 189.3, 217.3, 210.8, 176.3, 173.3, 224.3, 169.3, 176.3, 173.8, 173.8, 176.3, 185.3, 200.3, 179.3, 184.3, 212.3, 184.8, 176.8, 215.3, 189.8, 169.3, 169.3, 180.3, 176.3, 172.8, 177.3, 182.8, 189.3, 226.8, 233.8, 169.8, 167.8, 170.8, 186.3, 170.3, 170.3, 170.8, 173.3, 182.3, 182.3, 173.8, 173.8, 220.3, 193.3, 173.8, 158.8, 168.8, 167.8, 168.3, 180.3, 174.8, 165.3, 171.3, 163.3, 167.3, 167.3, 209.8, 177.8, 177.3, 239.3, 198.3, 175.8, 195.3, 212.8, 177.8, 174.8, 182.3, 219.8, 234.8, 194.8, 166.3, 182.8, 186.8, 184.8, 180.8, 167.8, 183.3, 180.3, 178.3, 163.8, 206.8, 246.3, 181.8, 214.8, 221.3, 178.3, 195.8, 198.3, 206.3, 205.3, 171.3, 187.8, 198.8, 200.8, 177.8, 189.3, 197.8, 191.8, 220.3, 170.8, 220.3, 176.8, 185.3, 181.8, 202.3, 192.8, 174.3, 199.3, 185.8, 186.8, 183.8, 173.8, 193.3, 187.3, 183.3, 173.3, 168.8, 181.3, 183.3, 174.8, 195.3, 181.3, 190.3, 193.8, 177.8, 160.8, 180.8, 237.8, 191.3, 177.3, 179.3, 158.8, 210.3, 173.8, 171.8, 168.8, 172.3, 203.3, 186.3, 165.8, 233.8, 182.8, 194.8, 180.3, 173.3, 168.3, 180.3, 185.8, 180.3, 184.8, 177.8, 177.8, 174.8, 186.3, 176.3, 173.8, 173.3, 175.3, 181.3, 180.3, 174.8, 172.8, 177.8, 174.8, 172.8, 177.3, 185.8, 164.8, 176.3, 170.3, 218.3, 191.3, 227.8, 231.3, 211.8, 209.8, 228.8, 223.8, 186.8, 218.8, 210.3, 225.8, 267.8, 190.3, 202.3, 190.3, 226.8, 216.3, 186.3, 170.3, 174.8, 173.3, 192.8, 184.3, 208.3, 186.3, 187.8, 186.8, 182.8, 190.3, 168.3, 211.8, 208.3, 180.3, 182.3, 173.8, 184.8, 165.8, 177.8, 174.8, 163.3, 174.8, 167.8, 195.3, 191.3, 164.8, 192.8, 184.8, 233.3, 191.8, 173.3, 161.8, 174.3, 179.3, 168.8, 168.8, 178.8, 179.3, 209.3, 196.8, 228.3, 170.8, 226.8, 188.8, 204.3, 186.8, 189.8, 190.3, 185.3, 209.8, 172.3, 169.8, 169.8, 183.8, 219.8, 226.8, 164.3, 189.8, 166.3, 174.8, 206.8, 203.3, 157.8, 157.3, 207.3, 169.8, 178.8, 178.3, 190.3, 194.8, 198.3, 178.3, 184.8, 181.8, 165.8, 169.8, 185.3, 169.8, 194.3, 173.3, 204.3, 186.8, 218.3, 206.3, 222.8, 213.8, 213.8, 177.3, 200.8, 213.8, 205.8, 190.3, 198.8, 177.8, 220.8, 190.3, 204.8, 195.8, 169.3, 169.8, 182.3, 197.3, 205.3, 189.8, 200.8, 182.3, 200.8, 182.3, 205.8, 196.8, 168.8, 179.8, 215.8, 215.8, 211.3, 199.3, 207.3, 203.8, 164.8, 228.8, 208.3, 221.8, 242.8, 208.3, 197.8, 194.8, 217.3, 223.3, 192.3, 212.8, 216.3, 192.3, 212.8, 261.8, 234.3, 203.8, 208.8, 203.8, 193.3, 201.3, 259.3, 253.8, 195.8, 199.8, 204.8, 247.8, 195.8, 223.8, 213.8, 228.3, 245.3, 199.8, 221.3, 199.8, 219.3, 200.3, 234.3, 218.8, 188.3, 176.3, 200.3, 171.8, 195.8, 193.8, 167.3, 209.3, 201.3, 204.8, 170.8, 176.8, 194.8, 194.8, 186.8, 194.3, 206.8, 210.8, 188.8, 188.8, 191.8, 182.8, 205.3, 196.8, 190.3, 179.3, 196.3, 205.3, 194.3, 179.8, 175.3, 192.3, 179.3, 203.3, 179.3, 194.3, 278.3, 239.3, 214.3, 182.3, 184.3, 192.8, 214.3, 225.3, 223.8, 161.8, 200.8, 210.3, 178.3, 237.8, 198.8, 183.3, 194.8, 187.3, 168.8, 179.8, 171.8, 199.8, 176.8, 183.8, 183.8, 232.8, 214.8, 221.8, 190.8, 177.8, 189.8, 177.8, 187.3, 200.8, 191.3, 186.8, 168.8, 176.3, 173.8, 163.8, 208.3, 194.3, 219.3, 190.3, 205.3, 195.3, 176.8, 210.3, 194.3, 181.8, 179.8, 172.3, 280.3, 198.3, 176.8, 280.3, 224.8, 179.8, 189.3, 208.8, 208.8, 196.3, 227.3, 211.8, 193.8, 190.8, 226.8, 172.8, 181.8, 193.3, 171.3, 181.3, 180.8, 189.8, 224.8, 191.8, 195.3, 176.8, 186.3, 173.3, 214.3, 167.3, 190.8, 177.8, 180.8, 254.3, 210.8, 164.8, 237.8, 180.8, 208.3, 190.8, 189.8, 181.3, 170.8, 177.3, 225.8, 186.8, 174.3, 164.8, 219.8, 191.3, 206.8, 178.3, 183.8, 190.8, 164.8, 181.3, 204.3, 211.3, 174.3, 208.3, 207.3, 164.8, 182.8, 179.8, 179.3, 175.3, 175.3, 204.3, 236.8, 164.8, 164.8, 181.3, 179.3, 232.3, 180.8, 217.8, 201.3, 164.8, 171.3, 164.8, 218.8, 200.8, 229.3, 214.8, 227.3, 246.3, 254.8, 179.8, 176.3, 210.3, 170.8, 196.3, 190.3, 197.8, 179.8, 179.8, 175.3, 176.3, 213.3, 205.3, 164.8, 221.8, 206.8, 188.8, 210.3, 191.8, 220.3, 179.8, 195.8, 176.3, 164.8, 164.8, 225.8, 229.8, 252.3, 253.8, 213.8, 204.8, 236.8, 236.8, 164.8, 182.3, 223.3, 210.3, 174.8, 209.3, 174.8, 216.8, 204.3, 195.8, 185.3, 174.3, 211.3, 188.3, 193.8, 198.3, 200.8, 187.8, 176.8, 190.8, 210.3, 169.3, 200.8, 171.8, 168.8, 168.3, 209.3, 176.3, 176.3, 215.3, 215.3, 171.3, 206.8, 204.8, 181.8, 196.3, 196.8, 204.8, 181.8, 164.8, 235.8, 181.8, 185.8, 214.8, 194.8, 223.8, 218.3, 170.8, 213.3, 213.3, 204.3, 186.8, 187.3, 207.3, 212.8, 212.8, 196.8, 222.3, 185.8, 190.3, 204.3, 182.3, 197.8, 199.8, 197.8, 174.8, 211.3, 236.3, 222.8, 201.3, 195.8, 199.8, 199.8, 211.3, 213.8, 204.8, 196.3, 197.3, 195.3, 176.8, 187.3, 180.3, 181.3, 190.3, 207.3, 186.3, 199.3, 205.8, 206.3, 168.3, 195.3, 211.8, 194.8, 205.8, 199.3, 217.8, 196.3, 232.3, 199.3, 164.8, 171.3, 242.8, 218.3, 201.3, 226.8, 208.3, 198.8, 226.8, 226.3, 172.8, 183.8, 225.3, 218.8, 177.3, 212.3, 206.3, 217.3, 198.3, 208.3, 184.8, 194.3, 208.8, 203.8, 224.8, 241.3, 209.3, 182.8, 180.3, 205.3, 190.3, 184.8, 186.3, 199.3, 211.3, 211.3, 207.3, 186.3, 200.3, 190.3, 178.8, 171.3, 208.8, 246.3, 218.3, 181.3, 259.8, 213.8, 213.8, 241.8, 237.8, 204.3, 224.8, 224.8, 220.8, 215.8, 222.3, 270.8, 186.8, 219.3, 181.3, 202.3, 205.3, 206.3, 199.3, 183.3, 215.3, 188.3, 164.3, 182.8, 173.8, 189.3, 190.3, 189.8, 178.8, 165.8, 210.3, 204.8, 195.3, 191.8, 194.8, 189.8, 184.8, 191.8, 165.3, 209.3, 168.8, 195.8, 185.3, 202.8, 196.3, 188.3, 201.3, 215.8, 181.8, 199.3, 190.3, 204.8, 204.3, 189.8, 191.3, 189.3, 179.8, 190.8, 192.3, 188.3, 185.3, 178.8, 178.3, 196.3, 188.3, 198.8, 174.8, 174.3, 167.8, 179.8, 188.3, 198.8, 183.8, 210.8, 218.8, 176.8, 211.8, 185.3, 180.3, 236.3, 207.3, 214.3, 202.3, 192.3, 206.8, 194.8, 178.3, 189.3, 175.8, 223.3, 208.8, 241.3, 233.8, 227.3, 242.3, 212.3, 185.3, 193.8, 237.8, 219.8, 213.3, 205.3, 221.8, 196.8, 221.8, 204.8, 216.3, 208.8, 194.8, 189.3, 223.3, 225.8, 189.3, 186.8, 173.8, 176.8, 182.3, 176.3, 172.8, 167.3, 185.8, 179.3, 176.3, 191.3, 184.3, 210.3, 217.8, 186.3, 185.3, 187.8, 179.8, 166.3, 180.8, 191.3, 168.8, 184.8, 192.8, 165.3, 177.3, 201.3, 176.8, 180.8, 189.3, 206.8, 180.3, 183.3, 175.3, 164.8, 164.8, 176.3, 163.3, 171.3, 176.8, 154.8, 152.8, 181.3, 172.8, 154.8, 194.3, 207.8, 195.3, 158.8, 165.3, 152.8, 179.3, 172.8, 166.8, 198.3, 212.3, 195.3, 175.3, 165.3, 160.8, 179.3, 170.8, 165.3, 167.3, 167.3, 236.8, 218.3, 212.3, 183.8, 224.8, 222.8, 217.8, 161.3, 161.3, 198.3, 198.3, 203.3, 203.3, 176.8, 204.8, 201.8, 204.3, 200.3, 200.3, 191.3, 191.3, 200.3, 191.3, 179.3, 172.8, 173.3, 158.8, 167.8, 160.3, 160.3, 159.3, 164.8, 191.8, 204.3, 192.8, 187.3, 192.8, 213.3, 178.3, 161.8, 154.8, 201.8, 209.8, 233.3, 204.8, 208.3, 165.8, 191.3, 219.3, 200.3, 195.8, 181.8, 200.3, 191.3, 200.3, 191.3, 207.3, 225.3, 196.3, 196.3, 196.3, 187.3, 196.3, 196.3, 187.3, 187.3, 187.3, 217.8, 208.3, 219.3, 201.8, 200.3, 191.3, 230.8, 191.3, 187.3, 187.3, 188.3, 188.3, 188.3, 196.8, 196.8, 188.8, 196.3, 196.3, 173.3, 173.3, 215.8, 213.8, 241.3, 233.8, 211.3, 173.8, 186.3, 185.3, 174.3, 186.8, 176.3, 237.3, 216.8, 156.3, 195.3, 191.8, 195.3, 173.8, 177.8, 174.3, 186.8, 201.8, 185.8, 219.3, 173.3, 185.3, 189.8, 189.8, 192.8, 182.3, 170.3, 173.8, 173.8, 173.8, 202.3, 172.8, 204.8, 237.3, 237.8, 250.8, 222.8, 192.3, 188.3, 178.3, 187.3, 178.3, 216.3, 164.8, 164.8, 164.8, 215.8, 183.3, 188.3, 178.8, 233.3, 177.8, 177.8, 178.3, 174.3, 173.8, 191.8, 178.8, 173.8, 174.3, 174.3, 174.8, 174.3, 177.8, 174.8, 173.8, 183.3, 182.3, 235.8, 224.3, 208.3, 209.3, 178.8, 226.8, 177.8, 177.3, 158.8, 168.3, 168.8, 168.8, 168.3, 168.3, 200.8, 205.8, 200.3, 196.3, 196.3, 201.8, 168.3, 169.3, 154.8, 191.3, 205.3, 217.8, 221.3, 196.8, 172.3, 195.8, 195.8, 200.8, 196.3, 197.8, 200.3, 191.3, 199.8, 209.3, 191.3, 200.3, 196.3, 196.3, 207.8, 215.3, 191.3, 197.8, 191.3, 200.3, 212.8, 212.8, 218.3, 191.3, 196.3, 205.8, 187.8, 195.8, 157.3, 200.3, 195.8, 195.8, 204.8, 181.3, 191.3, 191.3, 196.8, 191.3, 191.3, 206.3, 183.8, 186.8, 186.8, 186.8, 186.8, 182.8, 186.8, 165.3, 165.3, 165.3, 165.3, 165.3, 165.3, 173.8, 165.3, 165.3, 165.3, 162.3, 165.3, 193.3, 226.3, 165.3, 220.8, 208.3, 186.3, 186.3, 186.3, 204.8, 172.3, 197.8, 193.8, 193.8, 195.8, 182.8, 188.3, 209.3, 209.3, 209.3, 209.3, 188.3, 196.3, 187.3, 187.3, 187.3, 187.3, 191.8, 196.3, 168.8, 199.8, 199.8, 168.8, 187.8, 165.8, 168.8, 165.8, 165.8, 169.8, 216.8, 209.3, 205.3, 214.8, 191.3, 169.8, 218.3, 165.3, 180.8, 165.3, 162.3, 160.8, 193.8, 222.8, 226.8, 226.3, 233.3, 212.3, 243.8, 231.8, 213.3, 233.8, 240.3, 241.3, 236.3, 230.3, 198.3, 190.3, 181.3, 196.3, 191.8, 186.8, 188.3, 216.8, 197.3, 212.8, 193.8, 186.8, 202.3, 211.8, 226.8, 171.8, 242.3, 232.3, 231.8, 234.3, 171.8, 164.8, 211.3, 195.8, 200.3, 183.8, 190.3, 228.3, 171.8, 226.3, 203.8, 205.3, 200.8, 209.3, 206.3, 221.3, 191.3, 212.3, 210.8, 178.3, 234.8, 192.3, 212.3, 199.3, 197.3, 202.8, 195.3, 187.3, 210.8, 192.3, 192.8, 216.3, 187.3, 187.3, 188.8, 210.8, 192.3, 178.8, 174.3, 182.8, 182.8, 182.8, 191.8, 182.8, 178.8, 183.3, 183.3, 233.3, 215.8, 212.8, 192.3, 183.3, 187.8, 180.8, 187.3, 186.3, 192.3, 184.8, 216.8, 244.8, 190.8, 184.8, 187.8, 173.8, 231.3, 174.3, 196.8, 200.8, 207.8, 205.3, 201.3, 196.8, 187.3, 187.3, 187.3, 183.3, 202.8, 201.8, 194.3, 178.3, 184.3, 184.3, 184.3, 184.8, 182.8, 191.8, 178.8, 194.3, 199.3, 198.3, 198.3, 199.3, 206.3, 184.8, 187.3, 181.8, 181.8, 200.8, 178.8, 183.3, 189.8, 179.3, 183.3, 192.3, 179.8, 174.8, 174.8, 175.3, 178.8, 179.8, 179.8, 179.3, 175.3, 186.8, 183.3, 186.3, 190.8, 204.3, 194.8, 222.8, 208.3, 208.3, 191.8, 178.3, 191.3, 191.3, 191.3, 203.3, 181.8, 199.8, 187.3, 219.8, 203.3, 191.8, 224.8, 188.8, 188.8, 188.8, 188.8, 199.3, 201.8, 201.8, 184.8, 191.8, 191.8, 184.8, 206.3, 191.8, 206.3, 232.8, 206.3, 189.3, 199.8, 199.8, 188.8, 200.3, 206.3, 233.8, 228.8, 184.8, 209.8, 225.3, 194.3, 194.8, 218.3, 229.3, 191.3, 187.8, 187.8, 181.8, 195.3, 195.3, 194.8, 202.3, 181.8, 203.3, 220.8, 189.8, 216.8, 208.8, 208.8, 187.3, 191.3, 216.3, 187.3, 214.8, 209.3, 200.3, 188.3, 212.8, 188.8, 188.8, 188.8, 188.8, 188.8, 199.8, 196.3, 178.8, 178.8, 178.8, 178.8, 175.3, 196.3, 175.3, 202.3, 174.3, 202.3, 216.8, 196.8, 196.8, 196.3, 202.8, 202.8, 209.8, 202.3, 199.3, 219.8, 186.8, 201.3, 202.8, 187.8, 201.3, 188.8, 195.3, 183.3, 252.3, 226.8, 240.8, 226.3, 182.3, 165.8, 155.3, 157.8, 181.3, 172.8, 166.8, 199.8, 212.3, 195.3, 182.3, 165.3, 158.8, 186.8, 170.8, 161.3, 177.8, 198.3, 208.3, 199.3, 216.3, 202.3, 200.3, 218.3, 165.8, 159.3, 159.3, 214.3, 192.3, 204.3, 199.8, 194.8, 173.3, 182.3, 180.8, 180.8, 180.8, 180.8, 180.8, 200.8, 204.3, 196.3, 196.3, 196.3, 216.3, 180.8, 180.8, 166.8, 227.3, 220.8, 210.3, 193.3, 204.8, 184.3, 191.3, 191.3, 209.8, 195.8, 194.8, 222.3, 191.3, 191.3, 208.8, 188.3, 200.3, 196.3, 187.3, 187.3, 207.8, 187.3, 190.8, 196.3, 176.8, 174.3, 174.3, 176.8, 176.8, 196.3, 187.3, 196.3, 187.3, 196.3, 187.3, 187.3, 187.3, 187.3, 199.8, 187.3, 216.8, 191.3, 207.8, 215.8, 206.3, 187.3, 200.3, 194.8, 220.3, 191.3, 200.3, 237.3, 204.8, 204.8, 174.3, 200.3, 191.3, 191.3, 195.8, 200.3, 217.8, 200.3, 200.3, 200.3, 230.8, 196.3, 187.3, 187.3, 188.8, 187.3, 199.8, 196.3, 216.8, 190.3, 207.8, 216.8, 197.3, 197.3, 187.3, 188.3, 188.3, 188.3, 188.3, 188.3, 199.8, 203.8, 184.8, 196.3, 187.3, 187.3, 187.3, 205.8, 175.3, 207.8, 191.3, 175.3, 206.8, 208.8, 207.8, 209.3, 194.3, 217.8, 183.8, 202.3, 199.8, 199.8, 195.8, 181.3, 200.3, 212.3, 212.3, 191.3, 191.3, 226.8, 196.3, 187.3, 187.3, 196.3, 187.3, 199.8, 201.8, 178.3, 200.3, 200.3, 169.8, 169.8, 162.3, 184.8, 178.3, 184.8, 162.3, 165.3, 184.8, 164.3, 187.3, 209.8, 209.8, 187.3, 209.3, 184.8, 195.3, 160.3, 162.8, 186.3, 216.8, 202.8, 200.3, 185.8, 221.3, 185.8, 173.3, 173.3, 158.8, 172.8, 172.8, 172.8, 172.8, 172.8, 201.8, 213.3, 191.3, 202.3, 205.8, 204.3, 172.8, 172.8, 154.8, 192.3, 196.3, 177.8, 200.8, 204.8, 179.3, 191.3, 211.8, 205.3, 195.8, 187.3, 205.8, 206.3, 201.8, 215.3, 200.3, 226.8, 196.3, 187.3, 187.3, 193.8, 187.3, 190.8, 201.8, 173.3, 184.3, 184.3, 180.3, 173.3, 166.3, 175.3, 177.8, 177.8, 166.3, 175.8, 189.3, 184.3, 188.3, 219.3, 190.3, 191.3, 179.8, 154.8, 162.3, 160.8, 152.8, 190.3, 160.3, 172.8, 198.3, 194.3, 205.3, 205.3, 193.8, 194.3, 200.3, 207.8, 200.8, 191.8, 213.8, 207.8, 184.3, 205.3, 194.3, 207.8, 194.3, 204.3, 203.8, 200.8, 191.8, 191.8, 195.3, 194.3, 201.3, 191.3, 191.3, 204.3, 200.3, 195.3, 186.3, 186.3, 206.3, 187.8, 190.3, 176.8, 176.8, 202.8, 196.3, 179.3, 174.3, 192.8, 174.3, 190.3, 204.8, 209.8, 186.3, 186.3, 204.8, 186.3, 186.3, 186.3, 196.8, 187.3, 199.3, 190.8, 211.3, 204.8, 214.8, 203.8, 190.8, 194.3, 195.3, 193.3, 189.8, 183.8, 183.8, 183.8, 198.8, 190.3, 200.8, 191.8, 191.8, 185.3, 183.8, 191.8, 178.8, 182.8, 182.8, 196.3, 195.3, 186.3, 186.3, 186.3, 196.8, 179.3, 195.3, 195.3, 195.3, 195.3, 186.3, 186.3, 196.8, 166.8, 201.8, 205.8, 191.3, 229.8, 200.3, 200.3, 195.3, 205.8, 192.8, 192.8, 200.8, 213.3, 198.8, 208.3, 204.3, 204.3, 208.3, 204.3, 212.8, 224.8, 195.3, 186.3, 186.3, 186.3, 195.3, 218.8, 192.8, 204.8, 212.8, 187.3, 193.3, 193.3, 193.3, 212.3, 203.8, 222.8, 191.8, 191.8, 210.3, 193.3, 200.3, 191.3, 191.3, 191.3, 201.3, 209.8, 186.3, 186.3, 195.3, 201.3, 177.3, 203.8, 203.8, 195.3, 195.3, 185.8, 185.8, 177.3, 185.3, 185.8, 190.3, 201.8, 189.3, 189.3, 186.8, 213.3, 213.3, 186.8, 238.8, 200.3, 194.8, 185.8, 186.3, 225.3, 185.3, 214.8, 209.8, 221.8, 185.3, 193.8, 185.3, 186.3, 185.8, 180.8, 180.8, 185.8, 185.8, 204.3, 185.8, 187.8, 216.3, 207.3, 185.8, 183.8, 181.3, 185.8, 185.8, 164.8, 206.3, 191.3, 207.8, 207.8, 207.8, 195.8, 185.8, 191.3, 191.3, 191.3, 191.3, 209.3, 195.3, 186.3, 186.3, 210.3, 192.3, 178.3, 178.3, 178.3, 208.3, 206.3, 186.8, 208.3, 200.8, 186.8, 206.3, 186.3, 178.3, 191.3, 194.8, 190.3, 190.3, 181.3, 198.3, 198.3, 181.3, 221.8, 208.3, 208.3, 203.3, 189.8, 198.8, 186.8, 187.3, 186.8, 190.8, 190.8, 190.8, 190.8, 190.8, 198.8, 190.8, 187.3, 199.3, 199.3, 207.8, 181.3, 177.8, 177.8, 181.3, 200.3, 194.8, 195.8, 190.8, 190.8, 190.8, 195.3, 190.8, 208.3, 202.3, 200.3, 191.3, 191.3, 191.3, 195.3, 186.3, 186.3, 205.8, 198.3, 177.8, 184.3, 184.3, 195.8, 195.8, 186.3, 186.3, 177.8, 185.8, 186.3, 190.8, 185.3, 189.3, 189.3, 181.3, 177.8, 177.8, 181.3, 194.8, 200.3, 189.8, 186.3, 186.3, 190.8, 190.8, 186.3, 209.8, 174.8, 172.8, 172.8, 172.8, 178.8, 177.8, 177.8, 177.8, 177.8, 181.3, 177.8, 185.8, 204.8, 204.8, 190.8, 177.8, 174.8, 237.3, 250.8, 241.8, 236.8, 199.3, 181.3, 177.8, 177.8, 181.3, 218.3, 192.3, 221.8, 186.8, 177.8, 186.3, 201.3, 237.8, 195.3, 207.8, 189.8, 201.8, 173.3, 173.3, 168.8, 169.3, 233.8, 223.3, 221.8, 245.8, 214.3, 236.8, 253.8, 248.3, 191.8, 251.8, 195.3, 187.8, 195.3, 228.8, 224.8, 217.8, 246.3, 256.8, 228.8, 192.8, 222.3, 197.3, 203.8, 203.8, 197.3, 235.8, 195.3, 204.3, 200.3, 187.3, 196.3, 192.8, 183.8, 187.8, 187.8, 201.3, 208.3, 194.8, 208.8, 208.8, 194.8, 249.8, 207.3, 217.3, 206.8, 235.3, 206.8, 207.8, 220.8, 235.8, 203.3, 216.3, 195.3, 209.3, 209.3, 224.3, 218.3, 237.3, 219.3, 227.3, 227.3, 217.3, 208.3, 208.3, 225.3, 195.3, 199.3, 219.8, 195.8, 209.3, 208.3, 198.3, 208.3, 201.8, 199.3, 199.3, 199.3, 199.3, 200.3, 216.8, 236.3, 220.8, 199.3, 194.8, 207.8, 207.8, 215.3, 212.8, 212.8, 216.8, 207.8, 199.3, 208.3, 194.3, 221.3, 194.8, 207.3, 200.3, 226.8, 226.8, 226.8, 203.8, 204.3, 207.3, 222.8, 210.8, 206.8, 209.3, 211.8, 215.3, 198.8, 201.8, 183.8, 219.8, 220.3, 227.8, 199.3, 227.3, 226.3, 208.8, 226.3, 196.3, 200.3, 227.3, 202.8, 223.8, 195.8, 208.8, 218.3, 218.3, 208.3, 208.8, 215.3, 202.8, 207.8, 200.3, 194.8, 200.3, 226.8, 226.8, 194.8, 194.8, 264.3, 209.3, 235.8, 213.8, 237.8, 183.3, 194.8, 248.3, 208.8, 235.8, 194.8, 207.3, 211.8, 231.8, 211.8, 247.3, 244.8, 250.3, 240.3, 240.3, 197.8, 204.3, 204.3, 178.3, 243.8, 201.3, 202.3, 192.3, 191.8, 187.3, 187.3, 196.8, 196.8, 184.8, 241.8, 184.8, 205.3, 184.8, 223.3, 184.8, 187.3, 209.8, 223.8, 201.8, 192.3, 184.8, 206.3, 206.3, 184.8, 215.3, 245.8, 250.8, 215.8, 215.8, 215.3, 184.8, 184.8, 246.3, 184.8, 184.8, 223.8, 187.8, 197.3, 184.8, 190.8, 184.8, 190.8, 187.3, 187.3, 180.3, 187.3, 186.3, 186.3, 236.3, 209.8, 230.8, 184.8, 196.8, 205.3, 184.8, 210.3, 201.8, 204.8, 196.8, 184.8, 205.8, 186.3, 215.8, 184.8, 197.3, 202.3, 225.8, 215.8, 215.8, 192.8, 192.3, 204.8, 211.8, 209.3, 209.8, 188.3, 202.3, 191.3, 186.8, 201.8, 193.8, 223.8, 237.3, 232.8, 194.3, 232.8, 215.3, 223.8, 215.3, 188.3, 201.3, 215.8, 201.3, 215.8, 184.8, 197.8, 206.3, 206.3, 184.8, 197.8, 184.8, 191.8, 196.8, 201.8, 183.8, 201.3, 215.8, 215.8, 183.8, 183.8, 233.3, 201.8, 201.8, 184.8, 246.3, 238.3, 184.8, 196.8, 210.8, 186.8, 178.3, 221.3, 221.3, 185.8, 185.8, 252.3, 189.8, 189.8, 213.3, 198.8, 189.8, 180.8, 196.8, 196.8, 180.8, 233.3, 189.8, 205.3, 189.3, 223.3, 189.3, 193.3, 207.8, 223.8, 207.8, 192.3, 181.3, 197.3, 197.3, 190.3, 206.3, 216.8, 229.3, 207.8, 207.8, 206.3, 190.3, 190.3, 216.8, 181.3, 185.8, 229.3, 193.3, 197.3, 189.8, 193.3, 188.8, 177.3, 187.3, 187.3, 177.3, 187.3, 174.3, 174.3, 236.3, 207.8, 200.3, 180.8, 196.8, 205.8, 185.8, 201.8, 201.8, 204.8, 196.8, 185.8, 205.8, 193.3, 207.8, 189.8, 197.3, 200.3, 207.8, 207.8, 208.3, 193.3, 192.3, 204.8, 207.8, 210.8, 245.8, 203.8, 235.8, 235.8, 228.8, 221.8, 212.3, 198.8, 212.3, 207.8, 200.3, 207.8, 193.3, 200.3, 207.8, 200.3, 207.8, 181.8, 197.8, 206.3, 206.3, 191.3, 197.8, 186.3, 193.3, 196.8, 200.3, 183.8, 200.3, 207.8, 207.8, 183.8, 183.8, 221.8, 224.3, 216.8, 185.8, 211.8, 197.8, 180.8, 200.8, 204.3, 188.3, 195.3, 182.8, 182.8, 200.8, 216.8, 201.3, 187.8, 187.3, 187.3, 187.8, 224.8, 189.8, 195.8, 189.3, 213.8, 189.3, 193.8, 200.3, 214.3, 192.3, 192.8, 188.3, 187.8, 187.8, 190.3, 196.8, 223.3, 203.3, 206.3, 206.3, 196.8, 196.8, 190.3, 215.8, 188.3, 188.3, 201.3, 187.8, 189.8, 196.8, 188.8, 177.8, 177.8, 177.8, 177.8, 177.8, 177.8, 177.8, 227.8, 200.3, 217.8, 187.8, 187.3, 187.3, 188.3, 192.3, 192.3, 195.3, 187.3, 188.3, 196.3, 193.3, 199.8, 187.8, 201.3, 201.3, 206.3, 206.3, 206.3, 194.3, 182.8, 195.3, 202.3, 187.8, 205.3, 236.3, 210.8, 207.3, 200.3, 197.3, 205.8, 218.3, 205.8, 194.3, 201.3, 206.8, 213.8, 210.3, 188.3, 188.3, 196.8, 196.8, 191.3, 188.3, 188.3, 194.3, 187.3, 201.3, 174.3, 201.3, 206.3, 206.3, 174.3, 174.3, 228.3, 201.3, 206.3, 188.3, 215.3, 215.3, 188.3, 184.3, 187.8, 173.3, 177.3, 177.3, 177.3, 187.8, 208.8, 190.8, 197.3, 197.3, 197.3, 190.8, 203.3, 219.8, 221.3, 194.3, 190.8, 226.3, 208.8, 194.3, 204.3, 204.3, 187.8, 191.8, 191.8, 191.8, 191.3, 187.8, 199.8, 208.8, 198.3, 211.3, 203.8, 194.3, 192.8, 192.8, 203.8, 203.8, 203.8, 208.3, 188.8, 199.3, 208.8, 194.3, 187.3, 187.3, 202.8, 194.3, 198.8, 202.8, 202.3, 188.8, 188.8, 195.3, 180.8, 187.3, 187.3, 180.8, 185.3, 180.8, 185.3, 194.3, 188.8, 187.3, 189.3, 190.8, 187.3, 201.8, 173.8, 187.3, 174.8, 191.3, 173.3, 164.3, 164.3, 164.3, 164.3, 173.3, 164.3, 173.3, 180.8, 195.3, 195.3, 195.3, 180.8, 187.3, 180.8, 180.8, 182.8, 173.3, 180.8, 186.3, 189.8, 186.3, 186.3, 197.8, 208.8, 180.8, 186.3, 173.3, 159.8, 150.8, 150.8, 150.8, 150.8, 159.8, 150.8, 159.8, 159.8, 159.8, 155.8, 155.8, 159.8, 150.8, 156.3, 159.8, 156.3, 180.3, 159.8, 159.8, 150.8, 150.8, 150.8, 150.8, 150.8, 159.8, 159.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 190.8, 173.3, 210.3, 205.8, 196.8, 187.3, 200.3, 204.3, 191.3, 191.3, 200.3, 191.3, 191.3, 204.8, 194.3, 208.8, 208.8, 196.8, 210.3, 210.3, 196.8, 217.8, 210.3, 201.3, 214.3, 194.3, 217.8, 201.3, 214.8, 201.8, 201.3, 219.3, 214.8, 214.8, 217.8, 197.3, 210.3, 203.3, 212.8, 204.3, 204.3, 213.3, 193.8, 210.3, 230.8, 194.3, 200.8, 191.3, 194.3, 201.3, 201.3, 191.3, 190.8, 206.8, 210.3, 190.8, 200.3, 196.3, 192.3, 193.3, 191.3, 204.8, 208.3, 196.3, 194.3, 200.8, 195.8, 194.3, 200.8, 200.8, 200.8, 191.3, 194.3, 188.3, 194.3, 204.8, 204.8, 204.8, 204.8, 195.8, 188.3, 194.8, 191.3, 197.3, 209.3, 190.3, 196.8, 200.8, 196.8, 191.3, 191.3, 188.3, 196.8, 191.8, 190.3, 203.3, 207.3, 198.8, 182.3, 182.3, 173.3, 177.8, 188.8, 192.8, 179.8, 177.3, 177.3, 177.3, 191.3, 197.8, 208.8, 191.3, 204.8, 204.8, 204.8, 191.3, 218.3, 188.3, 201.8, 207.8, 228.8, 211.8, 203.8, 190.3, 190.3, 203.3, 216.8, 187.8, 191.3, 190.3, 187.8, 191.3, 211.3, 210.3, 210.3, 210.3, 188.8, 211.3, 210.3, 194.3, 187.8, 210.3, 187.8, 199.3, 198.3, 208.3, 186.3, 191.3, 186.3, 186.3, 186.3, 187.3, 186.3, 179.8, 177.3, 188.8, 185.8, 194.8, 179.8, 177.3, 177.3, 179.8, 177.3, 225.8, 198.3, 177.3, 177.3, 186.3, 179.8, 177.3, 177.3, 177.3, 189.3, 194.8, 187.8, 203.8, 203.8, 203.8, 187.8, 215.3, 215.3, 215.3, 208.8, 190.3, 189.8, 187.8, 189.8, 189.8, 189.8, 189.3, 186.3, 186.3, 179.8, 179.8, 177.3, 177.8, 189.8, 187.3, 186.3, 201.3, 173.3, 186.3, 225.3, 196.8, 211.3, 234.8, 177.8, 204.3, 189.3, 189.3, 190.3, 189.3, 198.8, 189.3, 205.3, 205.3, 189.3, 196.8, 214.3, 196.8, 207.8, 193.3, 193.3, 207.8, 207.8, 199.3, 222.8, 195.8, 207.8, 207.8, 207.8, 207.8, 207.8, 193.3, 207.8, 209.8, 196.3, 190.8, 201.3, 205.3, 185.3, 190.8, 190.8, 203.8, 194.3, 185.3, 198.8, 232.8, 217.3, 229.3, 190.3, 219.3, 257.8, 189.8, 185.8, 204.8, 229.3, 195.8, 198.8, 235.3, 196.3, 215.8, 211.8, 193.3, 193.3, 209.8, 204.8, 201.8, 193.3, 192.8, 196.3, 200.3, 189.8, 188.3, 195.3, 214.3, 198.8, 205.8, 198.8, 198.8, 197.8, 191.8, 187.3, 191.3, 187.8, 189.8, 189.8, 187.8, 189.8, 180.8, 188.3, 177.3, 177.3, 185.3, 186.3, 186.3, 188.8, 202.3, 194.3, 194.3, 206.8, 206.8, 187.8, 186.3, 187.8, 187.8, 185.8, 189.3, 189.3, 198.3, 189.8, 182.8, 182.3, 187.3, 179.3, 186.3, 189.3, 187.3, 189.8, 178.8, 178.3, 192.8, 186.8, 187.3, 191.8, 173.3, 182.3, 173.3, 164.3, 164.3, 164.3, 164.3, 173.3, 164.3, 173.3, 187.3, 211.8, 211.8, 211.8, 187.3, 184.8, 194.8, 185.3, 189.3, 173.3, 183.3, 192.3, 196.8, 190.3, 193.3, 205.8, 208.3, 185.3, 199.3, 173.3, 159.8, 159.8, 159.8, 159.8, 159.8, 159.8, 159.8, 159.8, 159.8, 150.8, 159.8, 159.8, 159.8, 159.8, 159.8, 159.8, 156.8, 180.3, 159.8, 180.3, 150.8, 150.8, 150.8, 159.8, 159.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 182.3, 185.8, 186.3, 186.8, 189.3, 177.3, 177.3, 177.3, 177.3, 226.3, 208.8, 202.8, 187.8, 190.8, 190.8, 189.3, 203.8, 211.8, 185.8, 194.3, 179.8, 177.3, 177.3, 179.8, 177.3, 188.8, 181.8, 189.3, 189.3, 189.3, 187.8, 197.8, 188.8, 188.8, 210.3, 188.8, 182.3, 184.3, 172.3, 189.8, 187.8, 189.8, 189.8, 187.8, 177.3, 177.3, 203.3, 196.8, 198.3, 189.3, 214.8, 222.3, 189.8, 188.8, 178.3, 186.8, 186.3, 209.3, 210.3, 191.3, 193.3, 187.3, 187.3, 191.3, 210.3, 200.8, 196.8, 201.3, 209.8, 215.3, 209.3, 195.8, 209.3, 205.3, 215.8, 205.3, 194.3, 189.8, 188.3, 191.8, 200.8, 200.8, 196.3, 213.8, 195.8, 219.8, 198.8, 191.3, 198.8, 194.3, 191.3, 191.3, 194.3, 200.8, 209.3, 240.3, 193.3, 192.3, 200.8, 198.8, 204.8, 218.3, 196.3, 186.3, 196.3, 195.8, 195.8, 189.3, 177.8, 184.8, 200.8, 184.8, 191.3, 191.3, 177.8, 201.8, 177.8, 177.8, 191.3, 205.3, 188.3, 190.3, 200.8, 201.3, 194.3, 191.3, 187.3, 193.3, 193.3, 202.3, 191.3, 187.8, 188.3, 187.8, 184.3, 189.8, 184.3, 192.8, 196.8, 196.3, 189.8, 188.3, 188.8, 196.8, 188.3, 189.8, 190.3, 186.8, 181.8, 181.8, 181.8, 190.8, 190.8, 189.8, 188.3, 191.3, 191.3, 184.8, 191.3, 181.8, 196.3, 184.3, 186.8, 188.3, 201.8, 198.8, 194.3, 186.3, 186.8, 191.3, 177.3, 177.3, 177.3, 177.3, 189.8, 194.8, 187.8, 187.8, 203.3, 203.3, 199.8, 203.8, 203.3, 198.8, 188.8, 189.8, 181.3, 190.8, 187.8, 181.3, 185.8, 181.3, 186.3, 189.8, 189.8, 187.3, 178.8, 189.8, 187.3, 189.8, 178.8, 187.3, 173.3, 182.3, 173.3, 164.3, 164.3, 164.3, 164.3, 173.3, 164.3, 173.3, 181.3, 195.3, 195.3, 195.3, 181.3, 184.8, 181.3, 187.3, 182.8, 173.3, 181.3, 186.3, 190.8, 190.8, 186.3, 189.3, 219.3, 191.3, 186.3, 173.3, 159.8, 150.8, 150.8, 159.8, 155.8, 159.8, 159.8, 159.8, 155.8, 155.8, 155.8, 159.8, 154.8, 156.3, 159.8, 155.8, 180.3, 159.8, 159.8, 150.8, 150.8, 150.8, 150.8, 150.8, 159.8, 159.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8, 150.8]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7tw9473yvpbi","colab_type":"code","colab":{}},"source":["Ts2 = Ts_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmfC7uno3Okw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595946319345,"user_tz":-60,"elapsed":1437,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"de805311-e78b-4179-b3f0-fba00b4c277f"},"source":["print(len(Ts1[1]))\n","print(len(Ts3[1]))\n","print(Ts3[0][-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100\n","1292\n","184.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oCMG2wGZvp_W","colab_type":"code","colab":{}},"source":["Ts3 = Ts_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1JLq8HQwN0d","colab_type":"code","colab":{}},"source":["Ts_s = [Ts1, Ts2, Ts3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTEQHRWbT03X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1599585842705,"user_tz":-60,"elapsed":564,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"d5a91418-b7c2-4ab7-a6cf-633fadf31f46"},"source":["# plt.plot(mean_Ts)\n","# plt.grid()\n","# plt.show()\n","\n","# log_episode_interval = 10\n","# episodes = np.arange(log_episode_interval - 1, len(final_rs) - 1, log_episode_interval)\n","# episode_rewards = [np.mean(final_rs[episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","# plt.plot(episode_rewards)\n","# plt.grid()\n","# plt.show()\n","\n","# log_episode_interval = 50\n","# episodes = np.arange(log_episode_interval - 1, len(Ts) - 1, log_episode_interval)\n","# min_Ts = [np.min(Ts[episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","# plt.plot(min_Ts)\n","# plt.grid()\n","# plt.show()\n","\n","# Trajectories plot\n","xs_ = [[] for _ in range(env.num_agents)]\n","ys_ = [[] for _ in range(env.num_agents)]\n","for i in range(env.num_agents):\n","    for pos in pos_tra[i]:\n","        xs_[i].append(pos[0])\n","        ys_[i].append(pos[1])\n","        # print(pos)\n","fig, ax = plt.subplots()\n","ax.set_xlabel(\"x (m)\")\n","ax.set_ylabel(\"y (m)\")\n","colors = ['blue', 'orange', 'green']\n","for i in range(env.num_agents):\n","    ax.plot(xs_[i], ys_[i], label='uav ' + str(i), color=colors[i])\n","    # ax.plot(xs_[i][0], ys_[i][0], label='uav ' + str(i), color=colors[i], marker='o', fillstyle='none')\n","    ax.plot(xs_[i][0], ys_[i][0], label='uav ' + str(i), color=colors[i], marker='o')\n","    ax.plot(xs_[i][-1], ys_[i][-1], label='uav ' + str(i), color=colors[i], marker='x')\n","\n","    # marker='o', markerfacecolor='none'\n","    # ax.scatter(xs_[i], ys_[i])\n","# ax.plot(a, d, 'k:', label='Data length')\n","# ax.plot(a, c + d, 'k', label='Total message length')\n","\n","# legend = ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n","\n","# print(env.uavs[0].tra_map)\n","\n","# Section B\n","# r_c_set = np.array(range(10, 70, 10))\n","# labels = [\"Untrained modified cooperative Q-learning algorithm\", \"Trained modified cooperative Q-learning algorithm\", \"Untrained modified independent Q-learning algorithm\", \"Trained modified independent Q-learning algorithm\"]\n","# fig, ax = plt.subplots()\n","# ax.set_xlabel(\"$r_c$\")\n","# # ax.set_ylabel(r'$\\overline{T}$', rotation=0)\n","# ax.set_ylabel(r'$s^2$', rotation=0)\n","# for i in range(4):\n","#     if i != 2:\n","#         # tmp = [np.mean(Ts_s[i][j]) for j in range(len(r_c_set))]\n","#         tmp = [np.var(Ts_s[i][j]) for j in range(len(r_c_set))]\n","#     else:\n","#         # tmp = [Ts_s[i][j][-1] for j in range(len(r_c_set))]\n","#         # tmp[0] = 164.8\n","#         tmp = [0 for j in range(len(r_c_set))]\n","#     ax.plot(r_c_set, tmp, label=labels[i], marker='o', markerfacecolor='none')\n","#     # ax.scatter(xs_[i], ys_[i])\n","# # ax.plot(a, d, 'k:', label='Data length')\n","# # ax.plot(a, c + d, 'k', label='Total message length')\n","\n","# legend = ax.legend(loc='upper right', shadow=True)\n","# ax.grid()\n","\n","# Section C\n","# mean T\n","# fig, ax = plt.subplots()\n","# labels = \"rc=\"\n","# ax.set_xlabel(\"$iterations$\")\n","# ax.set_ylabel(r'$\\overline{T}$', rotation=0)\n","# tmp = [[] for _ in range(3)]\n","# for i in range(3):\n","#     for j in range(0, len(Ts_s[2][i * 2 + 1]) - 99):\n","#         tmp[i].append(np.mean(Ts_s[2][i * 2 + 1][j:j+100]))\n","#     # r_c_set = np.array(range(len(Ts_s[2][i * 2 + 1]))) + 1\n","#     ax.plot(tmp[i], label= labels + str((i + 1) * 20))\n","# legend = ax.legend(loc='upper right', shadow=True)\n","\n","# learning curves\n","# log_episode_interval = 10\n","# fig, ax = plt.subplots()\n","# labels = \"rc=\"\n","# ax.set_xlabel(\"$episode$\")\n","# ax.set_ylabel(\"$r$\", rotation=0)\n","# for i in range(3):\n","#     episodes = np.arange(log_episode_interval - 1, len(final_rs_s[i * 2 + 1]) - 1, log_episode_interval)\n","#     episode_rewards = [np.mean(final_rs_s[i * 2 + 1][episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","#     ax.plot(episode_rewards, label = labels + str((i + 1) * 20))\n","# legend = ax.legend(loc='upper left', shadow=True)\n","\n","\n","# plt.xlabel(\"$r_c$\")\n","# plt.ylabel(r'$\\overline{T}$', rotation=0)\n","# plt.show()\n","\n","# plt.plot(r_c_set, var_Ts) \n","# plt.xlabel(\"$r_c$\")\n","# plt.ylabel(r'$s^2$', rotation=0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}