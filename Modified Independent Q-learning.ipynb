{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Modified Independent Q-learning.ipynb","provenance":[{"file_id":"1M8rpyQpgUtmHMiF_u-To8PVKvlmy9Wjx","timestamp":1600984356011},{"file_id":"1DFXV7ESPJfkbKLwC0ZPQn-K_xxEWjj3H","timestamp":1597505828642},{"file_id":"16Hs7pdOIIJ7ShXQmXquFh9WiwPN0do3P","timestamp":1595085101986},{"file_id":"1qkQcyKHLkyk-wcnCebXBHSkK5U5w3eU3","timestamp":1593788872734},{"file_id":"1ZlCvGgwjBrxD-10sZZljGv0s4bx8vYe6","timestamp":1593715134610}],"collapsed_sections":[],"mount_file_id":"1DFXV7ESPJfkbKLwC0ZPQn-K_xxEWjj3H","authorship_tag":"ABX9TyPuQqKwSxwjd+jPcgOtSVLb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Gjm30orCxyTb","colab_type":"code","colab":{}},"source":["import numpy as np\n","from numpy import linalg as LA\n","import random\n","from collections import defaultdict\n","from functools import partial\n","import math\n","\n","# state of each agent (UAV)\n","class LocalState():\n","    def __init__(self, k, l, pos, velocity, r_c, r_s, num_agents, action_num):\n","        self.k = k\n","        self.l = l\n","        self.pos = pos\n","        self.velocity = velocity\n","        self.acc = 0\n","        self.r_c = r_c\n","        self.r_s = r_s\n","        self.action_num = action_num\n","        self.neighbours = []\n","        \n","        self.gamma_map = np.zeros(((int)(k), (int)(l))) # coverage map\n","        self.target_gamma_idx = {}\n","        for i in range(num_agents):\n","            # if i == agent_id:\n","            #     continue\n","            self.target_gamma_idx[i] = [-1, -1]\n","\n","        self.Q = defaultdict(partial(np.zeros, self.action_num)) # Q table\n","        self.Q_ep_num = defaultdict(partial(np.zeros, self.action_num)) # Q table\n","        self.Q_trajs = []\n","        self.max_Q = {} \n","        # self.Q = defaultdict(partial(np.random.rand, self.action_num))\n","\n","        self.tra_map = np.zeros(((int)(k), (int)(l)))\n","        self.untraversed_idx = None\n","        self.done = False\n","        \n","\n","class Environment():\n","    def __init__(self, m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents):\n","        self.r_c = r_c\n","        self.r_s = r_s\n","        self.d = d\n","        self.h = h\n","        self.w = w\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.c_alpha = c_alpha\n","        self.c1_gamma = c1_gamma\n","        self.c2_gamma = c2_gamma\n","        self.c1_T = c1_T\n","        self.c2_T = c2_T\n","        self.r_ref = r_ref\n","        self.lambda_ = lambda_\n","        self.t_step = t_step\n","        self.c_r = c_r\n","        self.v_max = v_max\n","        self.m = m   # length\n","        self.n = n   # width\n","        self.k = (int)(np.ceil(n / (np.sqrt(2) * self.r_s)))   # rows\n","        self.l = (int)(np.ceil(m / (np.sqrt(2) * self.r_s)))   # columns\n","        self.num_agents = num_agents\n","        self.action_num = action_num\n","        self.epsilon = epsilon   # range to decide if target pos is reached\n","        self.T = 0.3\n","        self.action_gamma_point_mapping = {\n","              1: [0, 0],\n","              2: [0, 1],\n","              3: [-1, 1],\n","              4: [-1, 0],\n","              5: [-1, -1],\n","              6: [0, -1],\n","              7: [1, -1],\n","              8: [1, 0],\n","              9: [1, 1]\n","        }\n","        self.count_map = np.zeros(((int)(self.k), (int)(self.l)))\n","        hrz = ((self.l - 1) * self.m * self.k) / (self.l * self.v_max) + ((self.k - 1) * self.n) / (self.k * self.v_max)\n","        vtc = ((self.k - 1) * self.n * self.l) / (self.k * self.v_max) + ((self.l - 1) * self.m) / (self.l * self.v_max)\n","        self.T_min = np.min([hrz, vtc]) / self.num_agents\n","        self.uavs = [LocalState(self.k, self.l, None, None, self.r_c, self.r_s, self.num_agents, self.action_num) for i in range(self.num_agents)]\n","        self.uavs_pos = []\n","        uavs_gamma_p = [[-1, -1] for _ in range(self.num_agents)]\n","        # ids = [[1, 2], [5, 1], [2, 6]]\n","        # ids = [[6, 1], [6, 6], [2, 4]] # untrained_167.3\n","        ids = [[6, 3], [5, 1], [1, 6]] # trained_50_142.3\n","        for i in range(self.num_agents):\n","            is_same = True\n","            while is_same:\n","                is_same = False\n","                # index = random.randint(1, num_cells - 2)\n","                # row = random.randint(1, self.k - 2)\n","                # col = random.randint(1, self.k - 2)\n","                row = ids[i][0]\n","                col = ids[i][1]\n","                for ag_id in range(self.num_agents):\n","                    if ag_id == i:\n","                        continue\n","                    if (row == uavs_gamma_p[ag_id][0]) and (col == uavs_gamma_p[ag_id][1]):\n","                        is_same = True\n","                        break\n","            uavs_gamma_p[i] = [row, col]\n","            print([row, col])\n","            # self.uavs_pos.append(np.array(self.index_to_pos([row, col])) + np.array([0.5 + np.random.rand(1) / 2, 0.5 + np.random.rand(1) / 2]))\n","            self.uavs_pos.append(np.array(self.index_to_pos([row, col])) + np.array([0.5, 1.0]))\n","\n","        self.cac_xl = 1\n","        self.cac_yl = 1 \n","        self.cac_map = np.zeros(((int)(self.n / self.cac_yl), (int)(self.m / self.cac_xl)))\n","        # self.cac_n = \n","  \n","    def index_to_pos(self, index):\n","        x_l = self.m / self.l\n","        y_l = self.n / self.k\n","        pos_x = (index[1] + 0.5) * x_l\n","        pos_y = self.n - (index[0] + 0.5) * y_l   \n","\n","        return [pos_x, pos_y]\n","\n","    def index_to_pos_gnr(self, index):\n","        # x_l = self.m / self.l\n","        # y_l = self.n / self.k\n","        pos_x = (index[1] + 0.5) * self.cac_xl\n","        pos_y = self.n - (index[0] + 0.5) * self.cac_yl   \n","\n","        return [pos_x, pos_y]\n","\n","    def update_cac_map(self):\n","        for i in range(self.num_agents):\n","            cur_uav = self.uavs[i]\n","            for j in range(self.cac_map.shape[0]):\n","                for k in range(self.cac_map.shape[1]):\n","                    if LA.norm(np.array(cur_uav.pos) - np.array(self.index_to_pos_gnr([j, k]))) <= self.r_s:  \n","                        self.cac_map[j, k] = 1\n","\n","        return np.sum(self.cac_map[:, :]) / (self.cac_map.shape[0] * self.cac_map.shape[1])\n","\n","    def MAS_update(self, agent_id, action):\n","        agent_ids = list(range(self.num_agents))\n","        del agent_ids[agent_id]\n","        # print(agent_ids)\n","        cur_uav = self.uavs[agent_id]\n","\n","        # target_gamma_idx = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","        # if is_new_target:\n","        #     cur_uav.velocity = [0, 0]\n","        if action != 1:\n","            cur_uav.target_gamma_idx[agent_id] = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","            # cur_uav.velocity = [0, 0]\n","        # print(\"uav \", agent_id, \" \", cur_uav.target_gamma_idx[agent_id])\n","\n","        # update acceleration\n","        cq_alpha = 0\n","        for id in agent_ids:\n","            delta_pos = np.array(self.uavs[id].pos) - np.array(cur_uav.pos)\n","            delta_pos_mag = LA.norm(delta_pos)\n","            if delta_pos_mag <= self.r_c:\n","                z_sigma = np.sqrt(1 + np.square(delta_pos_mag)) - 1\n","                z_p = z_sigma / self.d\n","                if z_p >= 0 and z_p < self.h:\n","                    p_h = 1\n","                elif z_p >= self.h and z_p < 1:\n","                    p_h = 1.0 / 2.0 * (1.0 + math.cos(math.pi * (z_p - self.h) / (1 - self.h)))\n","                else:\n","                    p_h = 0\n","                d_sigma = np.sqrt(1 + np.square(self.d)) - 1\n","                p_sigma = z_sigma - d_sigma\n","                phi_r = p_sigma / np.sqrt(1 + np.square(p_sigma)) - 1\n","                phi = p_h * phi_r\n","                sigma = delta_pos / np.sqrt(1 + np.square(delta_pos_mag))\n","                cq_alpha += phi * sigma\n","                # print(\"agent \", id, \" repulsive force = \", phi * sigma)\n","        cq_alpha = self.c_alpha * cq_alpha\n","        target_gamma_pos = self.index_to_pos(cur_uav.target_gamma_idx[agent_id])\n","        cq_gamma = -self.c1_gamma * (np.array(cur_uav.pos) - np.array(target_gamma_pos)) - self.c2_gamma * np.array(cur_uav.velocity)\n","        cq = cq_alpha + cq_gamma\n","        # cq = cq_gamma\n","        # print(\"cq_alpha = \", cq_alpha)\n","        # print(\"cq_gamma = \", cq_gamma)\n","\n","        # print(\"target_gamma_pos =\", target_gamma_pos)\n","        # print(\"mag of cq_alpha = \", LA.norm(cq_alpha))\n","        # print(\"mag of cq_gamma = \", LA.norm(cq_gamma))\n","        # print(\"mag of cq = \", LA.norm(cq))\n","\n","        # print(\"cur_uav.pos = \", cur_uav.pos)\n","        # print(\"target_gamma_pos = \", target_gamma_pos)\n","        # print(\"cur_uav.velocity = \", cur_uav.velocity)\n","\n","        # print(\"mag of cur_uav.velocity = \", LA.norm(cur_uav.velocity))\n","\n","        cur_uav.acc = cq\n","        # if LA.norm(cq) < 0.00000001:\n","        #     cq = cq_gamma\n","\n","        # update velocity and position\n","        prev_vel = np.array(cur_uav.velocity)\n","        cur_uav.velocity = np.array(cur_uav.velocity) + self.t_step * cq\n","        v_mag = LA.norm(cur_uav.velocity)\n","        if v_mag > self.v_max:\n","            cur_uav.velocity *= self.v_max / v_mag\n","        # cur_uav.pos = np.array(cur_uav.pos) + self.t_step * np.array(cur_uav.velocity)\n","        cur_uav.pos = np.array(cur_uav.pos) + self.t_step * (prev_vel + np.array(cur_uav.velocity)) / 2\n","\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[0].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[1].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[2].pos)))\n","        # print(\"distance = \", LA.norm(np.array(cur_uav.pos) - np.array(target_gamma_pos)))\n","\n","        # print(a)\n","            # print(\"target is changed\")\n","        # self.T += self.t_step\n","\n","    def is_target_reached(self, agent_id):\n","        cur_uav = self.uavs[agent_id]\n","        target_pos = self.index_to_pos(cur_uav.target_gamma_idx[agent_id])\n","\n","        return LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) <= self.epsilon\n","\n","    def fuse_maps(self, agent_id, action_n, prev_action_n):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.neighbours = []\n","        for i in range(self.num_agents):\n","            if i == agent_id:\n","                continue\n","            else:\n","                if LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[i].pos)) <= self.r_c: \n","                    cur_uav.neighbours.append(i)\n","                    cur_uav.gamma_map[:, :] = np.add(cur_uav.gamma_map[:, :], self.uavs[i].gamma_map[:, :])\n","                    if action_n[i] == 1:\n","                        x = self.uavs[i].target_gamma_idx[i][0] - self.action_gamma_point_mapping[prev_action_n[i]][0]\n","                        y = self.uavs[i].target_gamma_idx[i][1] - self.action_gamma_point_mapping[prev_action_n[i]][1]\n","                    else:\n","                        x = self.uavs[i].target_gamma_idx[i][0]\n","                        y = self.uavs[i].target_gamma_idx[i][1]\n","                    cur_uav.target_gamma_idx[i] = np.array([x, y])\n","                    # cur_uav.target_gamma_idx[i] = self.uavs[i].target_gamma_idx[i]\n","                    # print(\"neighbour \", i, \" is found\")\n","                    # print(\"communicate\")\n","                    # print(c_maps[i])\n","                    # print(cur_uav.map[:, :, 1])\n","                else:\n","                    cur_uav.target_gamma_idx[i] = [-1, -1]\n","        cur_uav.gamma_map[:, :] = cur_uav.gamma_map[:, :] != 0\n","\n","        target_gamma_idx = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action_n[agent_id]])\n","        # cur_uav.target_gamma_idx[agent_id] = target_gamma_idx\n","        target_pos = self.index_to_pos(target_gamma_idx)\n","        is_target_reached = LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) <= self.epsilon\n","        # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","        # if is_target_reached or (LA.norm(cur_uav.acc) < 0.00001 and LA.norm(np.array(cur_uav.pos) - np.array(target_pos)) < 0.5):\n","        if is_target_reached:\n","            # cur_uav.gamma_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] = 1\n","            # # cur_uav.tra_map = np.zeros((self.k, self.l))\n","            # cur_uav.tra_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] = np.amax(cur_uav.tra_map) + 1\n","            # self.count_map[cur_uav.target_gamma_idx[agent_id][0], cur_uav.target_gamma_idx[agent_id][1]] += 1\n","            cur_uav.gamma_map[target_gamma_idx[0], target_gamma_idx[1]] = 1\n","            # cur_uav.tra_map = np.zeros((self.k, self.l))\n","            cur_uav.tra_map[target_gamma_idx[0], target_gamma_idx[1]] = np.amax(cur_uav.tra_map) + 1\n","            self.count_map[target_gamma_idx[0], target_gamma_idx[1]] += 1\n","            # print()\n","            # is_target_reached = True\n","                # print(\"target is reached\")\n","\n","        return repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","\n","    def update_target_gamma_point(self, agent_id, action):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.target_gamma_idx[agent_id] = np.array(cur_uav.target_gamma_idx[agent_id]) + np.array(self.action_gamma_point_mapping[action])\n","\n","    def add_Q_table(self, agent_id):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.Q_trajs.append(defaultdict(partial(np.zeros, self.action_num)))\n","\n","    def update_Q_table(self, agent_id, obs, action, reward):\n","        cur_uav = self.uavs[agent_id]\n","        cur_uav.Q_trajs[-1][obs][action] = reward\n","\n","    def update_max_Q(self, agent_id, obs, action, reward):\n","        cur_uav = self.uavs[agent_id]\n","        if reward > 0:\n","            if obs in cur_uav.max_Q:\n","                if reward >= list(cur_uav.max_Q[obs].keys())[0]:\n","                    cur_uav.max_Q[obs] = {reward: action}\n","            else:\n","                cur_uav.max_Q[obs] = {reward: action}\n","\n","    def update_Q_(self, agent_id, obs, action, reward):\n","        cur_uav = self.uavs[agent_id]\n","        if reward > 0:\n","            idx = np.where(cur_uav.Q[obs] == reward)\n","            if idx:\n","                cur_uav.Q[obs][idx] = 0\n","            if reward > cur_uav.Q[obs][action - 1]:\n","                cur_uav.Q[obs][action - 1] = reward\n","\n","    def update_Q_value(self, agent_id, obs, next_obs, action, reward, ep_reward, ep_step, done, last, episode):\n","        # print(\"update_Q_value\")\n","        cur_uav = self.uavs[agent_id]\n","        # done = self.is_episode_finished()\n","        if done:\n","            it_q = cur_uav.Q[obs][action] + self.alpha * (reward - cur_uav.Q[obs][action])\n","            sum_ngb_q = 0\n","            for ngb in cur_uav.neighbours:\n","                sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward - self.uavs[ngb].Q[obs][action])\n","            # new_Q = self.w * it_q + self.w * sum_ngb_q\n","            new_Q = it_q\n","            # if last:\n","            #     # print(\"uav \", agent_id)\n","            #     # print(\"it_q = \", it_q )\n","            #     # print(\"cur_uav.Q[obs][action] = \", cur_uav.Q[obs][action])\n","            #     # if new_Q > self.r_ref:\n","            #     #     print(\"reward = \", reward)\n","            #     # print(\"action \", action)\n","            #     # if new_Q < 0:\n","            #     #     cur_uav.Q[obs][action] = new_Q\n","            #     if new_Q > cur_uav.Q[obs][action]:\n","            #         cur_uav.Q[obs][action] = new_Q\n","            # else:\n","            #     cur_uav.Q[obs][action] = new_Q\n","            cur_uav.Q[obs][action] = new_Q  \n","            cur_uav.Q_ep_num[obs][action] = episode\n","            # cur_uav.Q[obs][action] = it_q\n","        else:\n","            # next_obs = repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","            # print(\"inside\")\n","            # print(repr([cur_uav.gamma_map, cur_uav.target_gamma_idx]))\n","            it_q = cur_uav.Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(cur_uav.Q[next_obs]) - cur_uav.Q[obs][action])\n","            # print(\"it_q = \", it_q)\n","            sum_ngb_q = 0\n","            for ngb in cur_uav.neighbours:\n","                sum_ngb_q += self.uavs[ngb].Q[obs][action] + self.alpha * (reward + self.lambda_ * np.max(self.uavs[ngb].Q[next_obs]) - self.uavs[ngb].Q[obs][action])\n","            # print(\"neighbours \", cur_uav.neighbours)\n","            # print(\"sum_ngb_q = \", sum_ngb_q)\n","            new_Q = self.w * it_q + self.w * sum_ngb_q\n","            # new_Q = it_q\n","            if last:\n","                # print(\"uav \", agent_id)\n","                # print(\"it_q = \", it_q )\n","                # print(\"cur_uav.Q[obs][action] = \", cur_uav.Q[obs][action])\n","                # if new_Q > self.r_ref:\n","                #     print(\"reward = \", reward)\n","                #     print(\"np.max(cur_uav.Q[next_obs]) = \", np.max(cur_uav.Q[next_obs]))\n","                # print(\"action \", action)\n","                # if new_Q < 0:\n","                #     cur_uav.Q[obs][action] = new_Q\n","                # cur_uav.Q[obs][action] > 0 and \n","\n","                # if ep_step == 1:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif new_Q > cur_uav.Q[obs][action]:\n","                #     cur_uav.Q[obs][action] = new_Q\n","\n","                # if ep_step == 1:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif ep_reward >= np.max(cur_uav.Q[next_obs]):\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # elif ep_reward < np.max(cur_uav.Q[next_obs]):\n","                #     cur_uav.Q[obs][action] = ep_reward\n","                # if (cur_uav.Q[obs][action] > 0) and (new_Q > cur_uav.Q[obs][action]):\n","                #     cur_uav.Q[obs][action] = new_Q\n","                # elif cur_uav.Q[obs][action] == 0:\n","                #     cur_uav.Q[obs][action] = ep_reward\n","\n","                num_forward_branches = np.count_nonzero(cur_uav.Q[next_obs])\n","                # ep_reward += reward\n","                if num_forward_branches > 1:\n","                    if ep_step == 1:\n","                        if new_Q > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = new_Q\n","                            cur_uav.Q_ep_num[obs][action] = episode\n","                    else:\n","                        if ep_reward > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = ep_reward\n","                            cur_uav.Q_ep_num[obs][action] = episode\n","                else:\n","                    if ep_step == 1:\n","                        if new_Q > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = new_Q\n","                            cur_uav.Q_ep_num[obs][action] = episode\n","                    else:\n","                        if ep_reward > cur_uav.Q[obs][action]:\n","                            cur_uav.Q[obs][action] = ep_reward\n","                            cur_uav.Q_ep_num[obs][action] = episode\n","            else:\n","                cur_uav.Q[obs][action] = new_Q \n","            # cur_uav.Q[obs][action] = new_Q\n","            # cur_uav.Q[obs][action] = self.w * it_q + self.w * sum_ngb_q   \n","            # cur_uav.Q[obs][action] = it_q\n","        # print(\"Q = \", cur_uav.Q[obs])     \n","        # print(\"minimum Q = \", np.min(cur_uav.Q[obs]))\n","        # if next_obs in cur_uav.Q:\n","        #     # self.num_repeated_obs += 1\n","        #     print(\"next_obs exists in Q\")\n","        # print(cur_uav.Q[next_obs])\n","\n","    # def update_Q_value(self, agent_id, obs_n, next_obs_n, action_n, reward_n):\n","    #     # print(\"update_Q_value\")\n","    #     cur_uav = self.uavs[agent_id]\n","    #     done = self.is_episode_finished()\n","    #     if done:\n","    #         it_q = cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] + self.alpha * (reward[agent_id] - cur_uav.Q[obs_n[agent_id]][action_n[agent_id]])\n","    #         sum_ngb_q = 0\n","    #         for ngb in cur_uav.neighbours:\n","    #             sum_ngb_q += self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]] + self.alpha * (reward[ngb] - self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]])\n","    #         cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] = self.w * it_q + self.w * sum_ngb_q \n","    #         # cur_uav.Q[obs][action] = it_q\n","    #     else:\n","    #         # next_obs = repr([cur_uav.gamma_map, cur_uav.target_gamma_idx])\n","    #         # print(\"inside\")\n","    #         # print(repr([cur_uav.gamma_map, cur_uav.target_gamma_idx]))\n","    #         it_q = cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] + self.alpha * (reward[agent_id] + self.lambda_ * np.max(cur_uav.Q[next_obs_n[agent_id]]) - cur_uav.Q[obs_n[agent_id]][action_n[agent_id]])\n","    #         # print(\"it_q = \", it_q)\n","    #         sum_ngb_q = 0\n","    #         for ngb in cur_uav.neighbours:\n","    #             sum_ngb_q += self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]] + self.alpha * (reward[ngb] + self.lambda_ * np.max(self.uavs[ngb].Q[next_obs_n[ngb]]) - self.uavs[ngb].Q[obs_n[ngb]][action_n[ngb]])\n","    #         # print(\"neighbours \", cur_uav.neighbours)\n","    #         # print(\"sum_ngb_q = \", sum_ngb_q)\n","    #         cur_uav.Q[obs_n[agent_id]][action_n[agent_id]] = self.w * it_q + self.w * sum_ngb_q \n","\n","    def is_same_target_gamma_point(self, agent_id, action_n):\n","        cur_uav = self.uavs[agent_id]\n","        x = cur_uav.target_gamma_idx[agent_id][0] + self.action_gamma_point_mapping[action_n[agent_id]][0]\n","        y = cur_uav.target_gamma_idx[agent_id][1] + self.action_gamma_point_mapping[action_n[agent_id]][1]\n","        is_same = False\n","        for j in range(self.num_agents):\n","            if j == agent_id:\n","                continue\n","            else:\n","                if LA.norm(np.array(cur_uav.pos) - np.array(self.uavs[j].pos)) <= self.r_c: \n","                    target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    # target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    # target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    if target_x == x and target_y == y:\n","                        is_same = True\n","                        break\n","\n","        return is_same\n","\n","    def action_space(self, agent_id, action_n):\n","        cur_uav = self.uavs[agent_id]\n","        x = cur_uav.target_gamma_idx[agent_id][0]\n","        y = cur_uav.target_gamma_idx[agent_id][1]\n","        action_gamma_point_mapping = {\n","              1: [0, 0],\n","              2: [0, 1],\n","              3: [-1, 1],\n","              4: [-1, 0],\n","              5: [-1, -1],\n","              6: [0, -1],\n","              7: [1, -1],\n","              8: [1, 0],\n","              9: [1, 1]\n","        } \n","        action_space_ = np.array(list(range(1, 10)))\n","        # eliminate actions not possible or end up covering the same place\n","        idx = [] \n","        for action, delta_idx in action_gamma_point_mapping.items():\n","            x_ = x + delta_idx[0]\n","            y_ = y + delta_idx[1]\n","            is_same = False\n","            if x_ < 0 or x_ > self.k - 1 or y_ < 0 or y_ > self.l - 1 or cur_uav.gamma_map[x_, y_] == 1:\n","                # del action_space_[action - 1]\n","                is_same = True\n","                # print(action_space_)\n","                # print(\"action deleted\")\n","                # print(action_space_)\n","            if not is_same:\n","                for j in cur_uav.neighbours:\n","                    # target_x = cur_uav.target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    # target_y = cur_uav.target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    target_x = self.uavs[j].target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","                    target_y = self.uavs[j].target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","                    if target_x == x_ and target_y == y_:\n","                        is_same = True\n","                        break\n","                    # if self.uavs[j].untraversed_idx != None:\n","                    #     if x_ == self.uavs[j].untraversed_idx[0] and y_ == self.uavs[j].untraversed_idx[1]:\n","                    #         is_same = True\n","            if is_same:\n","                idx.append(action - 1)\n","        action_space_ = np.delete(action_space_, idx)\n","\n","        # prioritize horizontal and vertical action\n","        # l_actions = [1, 2, 4, 6, 8]\n","        # s_actions = [3, 5, 7, 9]\n","        # hv_ac = []\n","        # diag_ac = []\n","        # for act in action_space_:\n","        #     if act in l_actions:\n","        #         hv_ac.append(act)\n","        #     if act in s_actions:\n","        #         diag_ac.append(act)\n","        # action_space_ = np.array(hv_ac)\n","        # if len(hv_ac) != 0:\n","        #     action_space_ = hv_ac\n","        # else:\n","        #     action_space_ = diag_ac\n","\n","        return np.array(action_space_)  \n","\n","    def select_action(self, agent_id, obs, action_n, explore_start, explore_stop, decay_rate, decay_step, second, rpt_map, exp_exp_tradeoff):\n","        # print(\"new action\")\n","        cur_uav = self.uavs[agent_id]\n","        action_space_ = self.action_space(agent_id, action_n)\n","        ac_size = action_space_.size\n","        if action_space_.size == 0:   # empty list\n","            explore_probability = -1\n","            # print(\"empty action list\")\n","            # find all untraversed points different to target gamma points of other agents on the gamma map\n","            target_ids = []\n","            target_gamma_idx = cur_uav.target_gamma_idx\n","            for i in range(self.k):\n","                for j in range(self.l):\n","                    is_selected = True\n","                    if cur_uav.gamma_map[i, j] == 1:\n","                        is_selected = False\n","                    if is_selected:\n","                        for k in cur_uav.neighbours:\n","                            # target_x = target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            # target_y = target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            target_x = self.uavs[k].target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            target_y = self.uavs[k].target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            if i == target_x and j == target_y:\n","                                is_selected = False\n","                                break\n","                            # if self.uavs[k].untraversed_idx != None:\n","                            #     if i == self.uavs[k].untraversed_idx[0] and j == self.uavs[k].untraversed_idx[1]:\n","                            #         is_selected = False\n","                    if is_selected == True:\n","                        target_ids.append([i, j])\n","            # find the nearest untraversed point\n","            target = None\n","            dist = 100000000 \n","            for t_idx in target_ids:\n","                # np.array(self.index_to_pos(target_gamma_idx[agent_id]))\n","                tmp_dist = LA.norm(self.index_to_pos(t_idx) - cur_uav.pos)\n","                if tmp_dist < dist:\n","                    dist = tmp_dist\n","                    target = t_idx\n","            if target != None:\n","                # find the nearest gamma point and obtain the corresponding action\n","                dist = 100000000\n","                action_space_ = self.action_gamma_point_mapping\n","                # l_actions = [1, 2, 4, 6, 8]\n","                s_actions = [3, 5, 7, 9]\n","                action_num = 9\n","                for a, delta_idx in action_space_.items():\n","                    # if (a == 1) or (a in s_actions):\n","                    if a == 1:\n","                        # action_num -= 1\n","                        continue\n","                    t_idx_x = target_gamma_idx[agent_id][0] + delta_idx[0]\n","                    t_idx_y = target_gamma_idx[agent_id][1] + delta_idx[1]\n","                    is_selected = True\n","                    if t_idx_x < 0 or t_idx_x > self.k - 1 or t_idx_y < 0 or t_idx_y > self.l - 1:\n","                        is_selected = False\n","                    if is_selected:\n","                        for k in cur_uav.neighbours:\n","                            # target_x = target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            # target_y = target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            target_x = self.uavs[k].target_gamma_idx[k][0] + self.action_gamma_point_mapping[action_n[k]][0]\n","                            target_y = self.uavs[k].target_gamma_idx[k][1] + self.action_gamma_point_mapping[action_n[k]][1]\n","                            if t_idx_x == target_x and t_idx_y == target_y:\n","                                is_selected = False\n","                    if is_selected == False:\n","                        # action_num -= 1\n","                        continue\n","                    t_idx = [t_idx_x, t_idx_y]\n","                    cur_uav.untraversed_idx = t_idx\n","                    tmp_dist = LA.norm(np.array(self.index_to_pos(target)) - np.array(self.index_to_pos(t_idx)))\n","                    if tmp_dist < dist:\n","                        dist = tmp_dist\n","                        action = a\n","                if action_num == 0:\n","                    action = 1\n","            else: # if there's no untraversed point\n","                cur_uav.done = True\n","                action = 1\n","                cur_uav.untraversed_idx = None\n","        else:\n","            # print(\"not empty action list\")\n","\n","            # print(action_space_)\n","            # eliminate the same gamma point of other agent\n","            # target_gamma_idx = cur_uav.target_gamma_idx\n","            # tmp_action_space_ = np.copy(action_space_)\n","            # mapping = self.action_gamma_point_mapping\n","            # idx = []\n","            # for action in action_space_:\n","            #     t_idx = [target_gamma_idx[agent_id][0] + mapping[action][0], target_gamma_idx[agent_id][1] + mapping[action][1]]\n","            #     for j in range(self.num_agents):\n","            #         if j == agent_id:\n","            #             continue\n","            #         else:\n","            #             target_x = target_gamma_idx[j][0] + self.action_gamma_point_mapping[action_n[j]][0]\n","            #             target_y = target_gamma_idx[j][1] + self.action_gamma_point_mapping[action_n[j]][1]\n","            #             if t_idx[0] == target_x and t_idx[1] == target_y:\n","            #                 # del tmp_action_space_[action]\n","            #                 idx.append(action - 1)\n","            # tmp_action_space_ = np.delete(tmp_action_space_, idx)\n","            \n","            # select best action based on maximum Q-value\n","            # if obs in cur_uav.Q:\n","            #     self.num_repeated_obs += 1\n","                # print(\"obs exists in Q\")\n","            # if cur_uav.Q[obs]:\n","            #     self.num_repeated_obs += 1\n","                # print(\"obs exists in Q\")\n","            Qs = cur_uav.Q[obs]\n","            # print(\"Qs = \", Qs)\n","            # print(\"original Q \", Qs)\n","            # print(\"minimum Q = \", np.argmin(Qs))\n","\n","            # prioritize horizontal and vertical action\n","            l_actions = [1, 2, 4, 6, 8]\n","            s_actions = [3, 5, 7, 9]\n","            hv_ac = []\n","            diag_ac = []\n","            for act in action_space_:\n","                if act in l_actions:\n","                    hv_ac.append(act)\n","                if act in s_actions:\n","                    diag_ac.append(act)\n","            if len(hv_ac) != 0:\n","                action_space_ = hv_ac\n","            else:\n","                action_space_ = diag_ac\n","\n","            global_map = self.fuse_all_maps()\n","            # res = (((np.sum(global_map[:, :]) / (self.k * self.l)) > 0.5) and (len(action_space_) == 2) and second) or (second == False)\n","            res = (((np.sum(global_map[:, :]) / (self.k * self.l)) > 0.5) and second) or (second == False)\n","\n","            Qs = np.array([Qs[action - 1] for action in action_space_])\n","            ## First we randomize a number\n","            exp_exp_tradeoff = np.random.rand()\n","            # Exploration probability is exponentially decaying\n","            explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n","            # and explore_probability >= 0.05\n","            # if explore_probability < 0.3:\n","            #     # prob_step = 1.0 / (self.k * self.l / self.num_agents)\n","            #     # explore_probability = ep_step * prob_step\n","            #     # if explore_probability > 1.0:\n","            #     #     explore_probability = 1.0\n","            #     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * 0.5 * decay_step)\n","                # print(\"Explore P \", explore_probability)\n","            # if (explore_probability < 0.05) and (explore_probability > 0.01):\n","            #     ep_total_steps = np.ceil(self.k * self.l / self.num_agents)\n","            #     if ep_step > (ep_total_steps - 10):\n","            #         explore_probability = 1.0\n","            #         exp_exp_tradeoff = 0\n","            #     else:\n","            #         explore_probability = 0\n","            #         exp_exp_tradeoff = 1.0\n","            # elif explore_probability <= 0.01:\n","            #     explore_probability = 0\n","            #     exp_exp_tradeoff = 1.0\n","            explore_strategy = 0\n","            # if explore_probability >= 0.7:\n","            #     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate / 8 * decay_step)\n","            if explore_probability < 0.05:\n","                explore_strategy = 0\n","                explore_probability = 0\n","                exp_exp_tradeoff = 1.0\n","            if explore_strategy == 0:\n","                if (explore_probability > exp_exp_tradeoff) and res:\n","                    # g_action_space = []\n","                    # b_action_space = []\n","                    # for a in action_space_:\n","                    #     if obs in rpt_map:\n","                    #         if len(rpt_map[obs][a]) > 0:\n","                    #             b_action_space.append(a)\n","                    #         else:\n","                    #             g_action_space.append(a)\n","                    #     else:\n","                    #         g_action_space.append(a)\n","                    # if len(b_action_space) > 0 and len(g_action_space) > 0:\n","                    #     rd_num = np.random.rand()\n","                    #     if rd_num <= 1.0:\n","                    #         action = random.choice(g_action_space)\n","                    #     else:\n","                    #         # Make a random action (exploration)\n","                    #         action = random.choice(b_action_space)\n","                    # else:\n","                    #     action = random.choice(action_space_)\n","                    action = random.choice(action_space_)\n","                else:\n","                    # if np.max(Qs) == 0:\n","                    #     g_action_space = []\n","                    #     # b_action_space = []\n","                    #     for a in action_space_:\n","                    #         if obs in rpt_map:\n","                    #             if len(rpt_map[obs][a]) == 0:\n","                    #                 g_action_space.append(a)\n","                    #         else:\n","                    #             g_action_space.append(a)\n","                    #     if len(g_action_space) > 0:\n","                    #         action = random.choice(g_action_space)\n","                    #     else:\n","                    #         action = random.choice(action_space_)\n","                    # else:\n","                    #     choice = np.argmax(Qs)\n","                    #     action = action_space_[int(choice)]\n","\n","                    # max_q = -100\n","                    # choice = None\n","                    # for q_table in cur_uav.Q_trajs:\n","                    #     qs = q_table[obs]\n","                    #     qs = np.array([qs[action - 1] for action in action_space_])\n","                    #     max_q_ = np.max(qs)\n","                    #     if (max_q_ > 0) and (max_q_ >= max_q):\n","                    #         max_q = max_q_\n","                    #         choice = np.argmax(qs)\n","                    # action = -1\n","                    # if obs in cur_uav.max_Q: \n","                    #     action = list(cur_uav.max_Q[obs].values())[0]\n","                    # if action not in action_space_:\n","                        # print(\"best action not in space\")\n","                    choice = np.argmax(Qs)\n","                    action = action_space_[int(choice)]\n","                    # pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","                    # # print(\"prob = \", pi)\n","                    # choice = sample(pi)\n","                    # action = action_space_[int(choice)]\n","            elif explore_strategy == 1:\n","                pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","                # print(\"prob = \", pi)\n","                choice = sample(pi)\n","                action = action_space_[int(choice)]\n","\n","            # pi = np.exp(Qs / self.beta) / np.sum(np.exp(Qs / self.beta))\n","            # explore_probability = pi\n","            # explore_probability = 0\n","            # choice = sample(pi)\n","            # print(\"restricted actions \", action_space_)\n","            # print(\"restricted Q \", Qs)\n","            # choice = np.argmax(Qs)\n","            # action = action_space_[int(choice)]\n","            # action = random.choice(action_space_)\n","\n","        return action, explore_probability, ac_size\n","\n","    def fuse_all_maps(self):\n","        global_map = np.zeros((self.k, self.l))\n","        for i, uav in enumerate(self.uavs):\n","            global_map[:, :] = np.add(global_map[:, :], uav.gamma_map[:, :])\n","        global_map[:, :] = global_map[:, :] != 0\n","\n","        return global_map\n","\n","    def reward_fn(self, agent_id, action):\n","        # print(\"action is \", action)\n","        l_actions = [1, 2, 4, 6, 8]\n","        s_actions = [3, 5, 7, 9]\n","        cur_uav = self.uavs[agent_id]\n","        global_map = self.fuse_all_maps()\n","        # print(action)\n","        # print(self.action_gamma_point_mapping[action][0])\n","        # print(cur_uav.target_gamma_idx[agent_id][0])\n","        next_gamma_x = self.action_gamma_point_mapping[action][0] + cur_uav.target_gamma_idx[agent_id][0]\n","        next_gamma_y = self.action_gamma_point_mapping[action][1] + cur_uav.target_gamma_idx[agent_id][1]\n","        # print(\"target gamma point \", cur_uav.target_gamma_idx[agent_id])\n","        # print(\"action \", action)\n","        # print(next_gamma_x)\n","        # print(next_gamma_y)\n","        # print()\n","        # print(\"global_map \", global_map[next_gamma_x, next_gamma_y])\n","        if action in l_actions and (global_map[next_gamma_x, next_gamma_y] == 0):\n","            reward = 0\n","        elif action in s_actions and (global_map[next_gamma_x, next_gamma_y] == 0):\n","            # reward = -0.3\n","            reward = 0\n","        else:\n","            # print(\"here\")\n","            k_r = self.count_map[next_gamma_x, next_gamma_y]\n","            # reward = -0.2 * np.exp(self.c_r * (k_r - 1))\n","            reward = 0\n","\n","        return reward\n","\n","    def eposidic_reward(self, done):\n","        global_map = self.fuse_all_maps()\n","        R_T = -1 \n","        if np.sum(global_map[:, :]) == self.k * self.l or done:\n","            if self.T > self.c1_T * self.T_min:\n","                R_T = 0\n","            elif self.T > self.c2_T * self.T_min and self.T <= self.c1_T * self.T_min:\n","                R_T = self.r_ref / 2 * (1 + math.cos((math.pi * (self.T - self.c2_T * self.T_min)) / (self.c1_T * self.T_min - self.c2_T * self.T_min)))\n","            elif self.T <= self.c1_T * self.T_min:\n","                R_T = self.r_ref\n","\n","        return R_T\n","\n","    # def reset(self):\n","    #     env.T = 0.3\n","    #     obs_n = []\n","    #     # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]]\n","    #     for i in range(self.num_agents):\n","    #         # # initialize position and velocity of each uav\n","    #         cur_uav = self.uavs[i]\n","    #         x = 1.5 * self.m / self.l + random.random() * (self.m - 3.0 * self.m / self.l)\n","    #         y = 1.5 * self.n / self.k + random.random() * (self.n - 3.0 * self.n / self.k)\n","    #         cur_uav.pos = [x, y]\n","    #         # cur_uav.pos = pos[i]\n","    #         cur_uav.velocity = [0, 0]\n","\n","    #         # initialize state s and action a randomly for each uav\n","    #         # find the closest gamma point to the current agent\n","    #         dist = 100000000\n","    #         for j in range((int)(self.k)):\n","    #             for k in range(self.l):\n","    #                 tmp_dist = LA.norm(np.array(self.index_to_pos([j, k])) - np.array(cur_uav.pos))\n","    #                 if tmp_dist < dist:\n","    #                     dist = tmp_dist\n","    #                     cur_uav.target_gamma_idx[i] = [j, k]\n","    #         actions = np.arange(1, 10)\n","    #         cur_uav_idx = cur_uav.target_gamma_idx[i]\n","    #         # make each uav have different target gamma point\n","    #         for action in actions:\n","    #             tmp_x = cur_uav_idx[0] + self.action_gamma_point_mapping[action][0]\n","    #             tmp_y = cur_uav_idx[1] + self.action_gamma_point_mapping[action][1]\n","    #             is_same = False\n","    #             for ag_id in range(self.num_agents):\n","    #                 if ag_id == i:\n","    #                     continue\n","    #                 idx = self.uavs[ag_id].target_gamma_idx[ag_id]\n","    #                 if idx[0] == tmp_x and idx[1] == tmp_y:\n","    #                     is_same = True\n","    #                     break\n","    #             if not is_same:\n","    #                 cur_uav.target_gamma_idx[i] = [tmp_x, tmp_y]\n","    #                 break\n","    #             else:\n","    #                 continue\n","    #         # cur_uav.target_gamma_idx[i] = [pos[i][0], pos[i][1]]\n","    #         cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","    #     for i in range(self.num_agents):\n","    #         _, obs = self.fuse_maps(i)\n","    #         obs_n.append(obs)\n","    #     self.count_map = np.zeros((self.k, self.l))\n","\n","    #     return obs_n\n","\n","    def reset(self):\n","        self.T = 0.3\n","        obs_n = []\n","        # pos = [[16.125, 35.375], [27.125, 33.375], [22.875, 22.875]]\n","        # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]]\n","        # pos = [[15.125, 35.375], [27.125, 33.375], [39.625, 35.375]]\n","        # pos = [[3.125, 46.875], [46.875, 46.875], [21.875, 3.125]]\n","        pos = self.uavs_pos\n","        chosen_actions = [1 for _ in range(self.num_agents)]\n","        # chosen_actions = [8, 2, 4]\n","        self.num_repeated_obs = 0\n","        \n","        for i in range(self.num_agents):\n","            cur_uav = self.uavs[i]\n","\n","            cur_uav.tra_map = np.zeros((self.k, self.l))\n","            cur_uav.untraversed_idx = None\n","            cur_uav.done = False\n","            # np.zeros(self.k, self.l)\n","            # reset gamma_map and target gamma points\n","            cur_uav.gamma_map = np.zeros((self.k, self.l))\n","            for l in range(num_agents):\n","            # if i == agent_id:\n","            #     continue\n","                cur_uav.target_gamma_idx[l] = [-1, -1]\n","\n","            # initialize position and velocity of each uav\n","            cur_uav.pos = pos[i]\n","            cur_uav.velocity = [0, 0]\n","\n","            # initialize state s and action a randomly for each uav\n","            # find the closest gamma point to the current agent\n","            dist = 100000000\n","            for j in range(self.k):\n","                for k in range(self.l):\n","                    tmp_dist = LA.norm(np.array(self.index_to_pos([j, k])) - np.array(cur_uav.pos))\n","                    if tmp_dist < dist:\n","                        dist = tmp_dist\n","                        cur_uav.target_gamma_idx[i] = [j, k]\n","            actions = np.arange(1, 10)\n","            # possible_actions = np.copy(actions)\n","            cur_uav_idx = cur_uav.target_gamma_idx[i]\n","            # print(\"target gamma point \", cur_uav_idx)\n","\n","            # make each uav have different target gamma point\n","            action_to_delete = []\n","            for action in actions:\n","                tmp_x = cur_uav_idx[0] + self.action_gamma_point_mapping[action][0]\n","                tmp_y = cur_uav_idx[1] + self.action_gamma_point_mapping[action][1]\n","                if tmp_x < 0 or tmp_x > self.k - 1 or tmp_y < 0 or tmp_y > self.l - 1:\n","                    action_to_delete.append(action - 1)\n","                for ag_id in range(self.num_agents):\n","                    if ag_id == i:\n","                        continue\n","                    idx = np.copy(self.uavs[ag_id].target_gamma_idx[ag_id])\n","                    idx[0] += self.action_gamma_point_mapping[chosen_actions[ag_id]][0]\n","                    idx[1] += self.action_gamma_point_mapping[chosen_actions[ag_id]][1]\n","                    if idx[0] == -1:\n","                        continue\n","                    if idx[0] == tmp_x and idx[1] == tmp_y:\n","                        action_to_delete.append(action - 1)          \n","            possible_actions = np.delete(actions, action_to_delete)\n","            chosen_actions[i] = random.choice(possible_actions)\n","\n","            # if i == 0:\n","            #     chosen_actions[i] = random.choice([1, 4])\n","\n","            cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","            # print(cur_uav.tra_map)\n","            # print(np.amax(cur_uav.tra_map) + 1)\n","\n","        # choice = random.choice([0, 1])\n","        # if choice == 0:\n","        #     chosen_actions = [1, 1, 1]\n","        # else:\n","        #     chosen_actions = [6, 4, 1]\n","        # print(self.uavs[0].target_gamma_idx[0])\n","        # print(self.uavs[1].target_gamma_idx[1])\n","        # print(self.uavs[2].target_gamma_idx[2])\n","        self.count_map = np.zeros((self.k, self.l))\n","        action_n = [1 for _ in range(self.num_agents)]\n","        for i in range(self.num_agents):\n","            obs = self.fuse_maps(i, action_n, action_n)\n","            obs_n.append(obs)\n","        # print(\"reset\")\n","        # print(self.uavs[0].target_gamma_idx[0])\n","        # print(self.uavs[1].target_gamma_idx[1])\n","        # print(self.uavs[2].target_gamma_idx[2])\n","\n","        return obs_n, chosen_actions\n","\n","    # def reset(self):\n","    #     env.T = 0.3\n","    #     obs_n = []\n","    #     # pos = [[16.75, 19.25], [21.75, 25.25], [21.75, 19.25]]\n","    #     pos = []\n","    #     num_cells = self.k * self.l\n","    #     # pos.append([i, i])\n","    #     for i in range(self.num_agents):\n","    #         # initialize state s and action a randomly for each uav\n","    #         cur_uav = self.uavs[i]\n","    #         is_same = True\n","    #         while is_same:\n","    #             is_same = False\n","    #             index = random.randint(0, num_cells - 1)\n","    #             row = (int)(index / self.l)\n","    #             col = index % self.l\n","    #             for ag_id in range(self.num_agents):\n","    #                 if ag_id == i:\n","    #                     continue\n","    #                 idx = self.uavs[ag_id].target_gamma_idx[ag_id]\n","    #                 if idx[0] == row and idx[1] == col:\n","    #                     is_same = True\n","    #         cur_uav.target_gamma_idx[i] = [row, col]\n","    #         # print([row, col])\n","\n","    #         # initialize position and velocity of each uav\n","    #         pos_c = self.index_to_pos([row, col])\n","    #         r_len = self.n / self.k / 2.0\n","    #         c_len = self.m / self.l / 2.0\n","    #         cur_uav.pos = [random.uniform(pos_c[0] - r_len, pos_c[0] + r_len), random.uniform(pos_c[1] - c_len, pos_c[1] + c_len)]\n","    #         cur_uav.velocity = [0, 0]\n","\n","    #         # cur_uav.target_gamma_idx[i] = [pos[i][0], pos[i][1]]\n","    #         cur_uav.tra_map[cur_uav.target_gamma_idx[i][0], cur_uav.target_gamma_idx[i][1]] = np.amax(cur_uav.tra_map) + 1\n","    #     for i in range(self.num_agents):\n","    #         obs = self.fuse_maps(i)\n","    #         obs_n.append(obs)\n","    #     self.count_map = np.zeros((self.k, self.l))\n","\n","    #     return obs_n\n","\n","    def is_episode_finished(self):\n","        global_map = self.fuse_all_maps()\n","\n","        return (np.sum(global_map[:, :]) == self.k * self.l) or (self.T > self.num_agents * 1 * self.T_min)\n","\n","    def is_fully_covered(self):\n","        global_map = self.fuse_all_maps()\n","\n","        return np.sum(global_map[:, :]) == self.k * self.l\n","\n","    def get_obs_n(self):\n","        return [repr([self.uavs[i].gamma_map, self.uavs[i].target_gamma_idx]) for i in range(self.num_agents)]\n","\n","    def get_obs(self, agent_id):\n","        return [self.uavs[agent_id].gamma_map, self.uavs[agent_id].target_gamma_idx]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"of7MRtTAO7Vo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1600131431425,"user_tz":-60,"elapsed":574,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"bc8cae92-5fd2-417e-9246-5fafae48a0e3"},"source":["def test_R(T):\n","    T_min = 131.25\n","    R_T = r_ref / 2 * (1 + math.cos((math.pi * (T - c2_T * T_min)) / (c1_T * T_min - c2_T * T_min)))\n","\n","    return R_T\n","\n","a11 = test_R(139.8)\n","a22 = test_R(139.8)\n","\n","print(a11 == a22)\n","# dit = {}\n","# dit[1] = 2\n","# dit[1] = 3\n","# print(list(dit.values())[0])\n","arr = [1]\n","if arr:\n","    print(arr)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-cd7708cd7b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_R\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m139.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0ma22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_R\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m139.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-cd7708cd7b11>\u001b[0m in \u001b[0;36mtest_R\u001b[0;34m(T)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_R\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mT_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m131.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mR_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_ref\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc2_T\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc1_T\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT_min\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc2_T\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'r_ref' is not defined"]}]},{"cell_type":"code","metadata":{"id":"tpR8hn0jxieV","colab_type":"code","colab":{}},"source":["def sample(pi):\n","  # print(pi)\n","  # normalize(pi)\n","  return np.random.choice(pi.size, size=1, p=pi)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhR-037wK1Pk","colab_type":"code","colab":{}},"source":["# import tensorflow.compat.v1 as tf    \n","# tf.compat.v1.disable_eager_execution() \n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np           \n","import random                \n","import time                  \n","from collections import deque\n","import matplotlib.pyplot as plt \n","\n","### Q-LEARNING PARAMETERS\n","alpha = 1.0\n","lambda_ = 1.0\n","w = 0.5\n","r_ref = 64\n","epsilon = 0.2\n","c_r = 1.0\n","total_episodes = 1000000\n","\n","# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n","explore_start = 1.0            # Initial exploration probability \n","explore_stop = 0.0            # minimum exploration probability \n","# decay_rate = 0.000002            # exponential decay rate for exploration probability\n","decay_rate = 0.000003            # exponential decay rate for exploration probability (around 1614 for rc = 10, more for other rc)\n","# decay_rate = 0.000001              # around 8000 for rc = 60\n","# decay_rate = 0.0\n","decay_step = 0\n","\n","# HYPERPARAMETERS for soft max strategy\n","beta = 0.1\n","\n","### MAS MOTION MODEL PARAMETERS\n","r_c = 50\n","v_max = 1.0\n","d = 7\n","h = 0.1\n","c_alpha = 0.5\n","c1_gamma = 0.5\n","c2_gamma = 0.8\n","# c1_T = 1.45\n","c1_T = 170 / 131.25 # give close to op most of time (8/10 <=143.3)\n","# c1_T = 150 / 131.25 \n","# c1_T = 1.75\n","c2_T = 140 / 131.25 \n","# c2_T = 1\n","\n","# c1_T = 1.52\n","# c2_T = 1.22\n","# c1_T = 1.372\n","# c2_T = 1.143\n","# c1_T = 1.18\n","# c2_T = 1.12\n","\n","### OTHER HYPERPARAMETERS\n","r_s = 4.5\n","m = 50\n","n = 50\n","num_agents = 3\n","t_step = 0.5\n","action_num = 9\n","\n","actions = np.arange(9) # All possible actions\n","# possible_actions = np.identity(4, dtype=int).tolist() # One-hot encoding of possible actions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqZWXni1L-jc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1600112869607,"user_tz":-60,"elapsed":624,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"67fb8dab-930d-437c-b632-8e8ab0f5bfdc"},"source":["env = Environment(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents)\n","ids = [[6, 3], [5, 1], [1, 6]]\n","for i in range(3):\n","    print(env.index_to_pos(ids[i]))\n","a = [1, 0, 1, 0]\n","# a = np.zeros(1)\n","# print(np.count_nonzero(a))\n","# print(len([]))\n","arr = np.array([1, 2, 3])\n","print(arr[np.where(arr == 1)])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[6, 1]\n","[6, 6]\n","[2, 4]\n","[21.875, 9.375]\n","[9.375, 15.625]\n","[40.625, 40.625]\n","[1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"01YHuDapxcb6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595000660176,"user_tz":-60,"elapsed":528,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"72771373-939f-4190-958d-c827c22d7541"},"source":["def compare_idx(idx1, idx2):\n","    if idx1[0] == idx2[0] and idx1[1] == idx2[1]:\n","        return True\n","    \n","    return False\n","\n","idx = [[4, 2], [3, 3], [4, 3]]\n","action_gamma_point_mapping = {\n","    1: [0, 0],\n","    2: [0, 1],\n","    3: [-1, 1],\n","    4: [-1, 0],\n","    5: [-1, -1],\n","    6: [0, -1],\n","    7: [1, -1],\n","    8: [1, 0],\n","    9: [1, 1]\n","}\n","count = 9 * 9 * 9\n","action_ns = []\n","for i in range(9):\n","    t1_x = idx[0][0] + action_gamma_point_mapping[i + 1][0]\n","    t1_y = idx[0][1] + action_gamma_point_mapping[i + 1][1]\n","    for j in range(9):\n","        t2_x = idx[1][0] + action_gamma_point_mapping[j + 1][0]\n","        t2_y = idx[1][1] + action_gamma_point_mapping[j + 1][1]\n","        for k in range(9):\n","            t3_x = idx[2][0] + action_gamma_point_mapping[k + 1][0]\n","            t3_y = idx[2][1] + action_gamma_point_mapping[k + 1][1]\n","\n","            if compare_idx([t1_x, t1_y], [t2_x, t2_y]) or compare_idx([t1_x, t1_y], [t3_x, t3_y]) or compare_idx([t2_x, t2_y], [t3_x, t3_y]):\n","                count -= 1\n","            else:\n","                action_ns.append([i + 1, j + 1, k + 1])\n","action_ns = [[7, 2, 2]]\n","print(count)\n","print(len(action_ns))\n","# print(action_ns[2])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["593\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3cYp5H-QTR16","colab_type":"code","colab":{}},"source":["from collections import deque\n","\n","# o_action_ns = np.copy(action_ns[0:3])\n","# print(o_action_ns)\n","# print(len(action_ns))\n","# epc_T = deque(np.zeros((6), dtype=np.float32), maxlen = 6)\n","# epc_T.append(1.0)\n","# print(ep_reward[5])\n","\n","def converge(epc_T):\n","    res = True\n","    for i in range(len(epc_T) - 1):\n","        if epc_T[i] != epc_T[i + 1]:\n","            res = False\n","            break\n","\n","    return res\n","\n","# converge(epc_T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FXyfb0ALzACt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595593758253,"user_tz":-60,"elapsed":436,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"a47c7ac4-a392-4718-af5a-f8951aaac383"},"source":["a = [1, 2, 3]\n","for i, num in enumerate(reversed(a)):\n","    print(i, num)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 3\n","1 2\n","2 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6-WqCDDlfR9v","colab_type":"code","colab":{}},"source":["def train(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents, explore_start, explore_stop, decay_rate, decay_step):\n","    is_decay_set = False\n","    env = Environment(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents)\n","    obs_prev_obs = [{} for _ in range(env.num_agents)]\n","    Ts = []\n","    mean_Ts = []\n","    final_rs = []\n","    num_T = 500\n","    s_actions = [3, 5, 7, 9]\n","    T_memory = [{} for _ in range(env.num_agents)]\n","    rpt_maps = [{} for _ in range(env.num_agents)]\n","    cac = []\n","    cac_T = []\n","\n","    # print(env.k)\n","    # print(env.l)\n","    # print(env.T_min)\n","\n","    # print(obs_n[0])\n","    # print(env.index_to_pos(env.uavs[0].target_gamma_idx[0]))\n","    # print(env.uavs[0].pos)\n","    # print(obs_n[1])\n","    # print(obs_n[2])\n","    # print(env.count_map)\n","\n","    # print(env.fuse_all_maps())\n","\n","    # print(env.index_to_pos([7, 7]))\n","    # tra_points_n = [[] for _ in range(env.num_agents)]\n","    # for i in range(env.num_agents):\n","    #     tra_points_n[i].append(env.uavs[i].pos)\n","    # print(tra_points_n)\n","    # # iteration = 1\n","\n","    # print(\"chosen actions \", action_n_)\n","    total_episodes = 8000\n","    action_ns_tra = [[] for i in range(env.num_agents)]\n","    decay_step = 0\n","    explore_prob = -1\n","    # same_gamma_point = False\n","    max_size = 12\n","    epc_T = deque(np.zeros((max_size), dtype=np.float32), maxlen = max_size)\n","    prev_T = 0\n","    rpt_ct = 0\n","    for episode in range(total_episodes):\n","        # if episode >= num_T - 1:\n","        #     if np.sum(np.diff(Ts[episode : episode + num_T + 1])) / num_T <= 1:\n","        #         print(\"converged\")\n","        #         break\n","        ep_step = 0\n","        diag_count = 0\n","        ac_size = [9 for i in range(env.num_agents)]\n","        memory = [[] for i in range(env.num_agents)]\n","        pos_tra = [[] for i in range(env.num_agents)]\n","        obs_n, action_n = env.reset()\n","\n","        action_n = [1 for _ in range(env.num_agents)]\n","        # for i in range(env.num_agents):\n","        #     # print(\"uav \", i, \" action_space = \", env.action_space(i, action_n))\n","        #     action_n[i], explore_p, ac_size[i] = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], 0)\n","        # print(env.get_obs(0)[0])\n","        # action_n = [1, 1, 1]\n","\n","        for i in range(env.num_agents):\n","            action_ns_tra[i].append(action_n[i])\n","        o_obs_n = np.copy(obs_n)\n","        # print(obs_n[0])\n","        # break\n","\n","        # print(\"chosen actions \", action_n)\n","\n","        # action_n = np.copy(action_n_)\n","        o_action_n = np.copy(action_n)\n","        prev_action_n = np.copy(action_n)\n","        done = False\n","        final_reward = 0\n","        reward = np.zeros(env.num_agents)\n","        # first_step = True\n","        # action_n = [1 for _ in range(env.num_agents)]\n","        # if episode == 100:\n","        #     env.c1_T = 1.37\n","        #     env.c2_T = 1.14\n","        # if rpt_ct == 10:\n","        # cac.append(env.update_cac_map())\n","        # cac_T.append(env.T)\n","        while not env.is_episode_finished():\n","            exp_exp_tradeoff = np.random.rand()\n","            for i in range(env.num_agents):\n","                if not env.uavs[i].done:\n","                    # print(\"uav \", i)\n","                    # reward = env.reward_fn(i, action_n[i])\n","                    # print(\"reward = \", reward)\n","                    # print(action_n[i])\n","\n","                    if env.T == 0.3:\n","                        reward[i] = env.reward_fn(i, action_n[i])\n","                    #     print(\"reward = \", reward)\n","\n","                    # if the same target gamma point is detected midway through due to r_c is too small\n","                    # update action\n","                    is_same_target_gamma_point = env.is_same_target_gamma_point(i, action_n)\n","                    if is_same_target_gamma_point:\n","                        # print(\"same target gamma point\")\n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # print(action_n[i])\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] -= env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] -= env.action_gamma_point_mapping[prev_action_n[i]][1]\n","                        prev_action_n[i], explore_p, ac_size[i] = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], exp_exp_tradeoff)\n","                        if prev_action_n[i] in s_actions:\n","                            diag_count += 1                            \n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        # print(\"changed action \", prev_action_n[i])\n","                        if explore_p != -1:\n","                            explore_prob = explore_p\n","                            # if (explore_p < 0.7) and (not is_decay_set):\n","                            #     decay_rate = 0.000008\n","                            #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                            #     is_decay_set = True\n","                        reward[i] = env.reward_fn(i, prev_action_n[i])\n","                        if action_n[i] != 1:\n","                            action_n[i] = prev_action_n[i]\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] += env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] += env.action_gamma_point_mapping[prev_action_n[i]][1]\n","\n","                    # print(\"uav \", i, \" action_n[i] = \", action_n[i])\n","                    env.MAS_update(i, action_n[i])   # update target gamma point of uav i\n","                    pos_tra[i].append(env.uavs[i].pos)\n","                    # next_obs = env.fuse_maps(i, action_n, prev_action_n)\n","                    action_n[i] = 1\n","                    # tra_points_n[i].append(env.uavs[i].pos)\n","\n","                    is_target_reached = env.is_target_reached(i)\n","                    if is_target_reached:\n","                        # print(\"uav \", i, \" \", \"target \", env.uavs[i].target_gamma_idx[i] ,\"is reached\")\n","                        # print(\"env.T \", env.T)\n","                        # reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                        # print(\"target gamma point\")\n","                        # print(env.uavs[0].target_gamma_idx[0])\n","                        # print(env.uavs[1].target_gamma_idx[1])\n","                        # print(env.uavs[2].target_gamma_idx[2])\n","                        # print()\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # next_obs_n = [env.fuse_maps(k, action_n, prev_action_n) for k in range(env.num_agents)]\n","\n","                        # print()\n","                        # print(next_obs)\n","                        episodic_reward = env.eposidic_reward(False)\n","                        if episodic_reward == -1:\n","                            # print(\"prev_action_n \", prev_action_n)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward[i], reward[i], 0, False, False)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            # if rpt_ct == 10:\n","                            #     print(\"uav\", i, \" \", env.uavs[i].Q[obs_n[i]])\n","                            action_n[i], explore_p, ac_size[i] = env.select_action(i, next_obs, action_n, explore_start, explore_stop, decay_rate, decay_step, False, rpt_maps[i], exp_exp_tradeoff)\n","                            if action_n[i] in s_actions:\n","                                diag_count += 1  \n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward[i], env.uavs[i].done, ac_size[i]])\n","                            if reward[i] < 0:\n","                                for ag in range(env.num_agents):\n","                                    for experience in memory[ag]:\n","                                        if experience[0] not in rpt_maps[ag]:\n","                                            tmp_map = {}\n","                                            for nc in range(action_num):\n","                                                tmp_map[nc + 1] = []\n","                                            tmp_map[experience[1]].append(next_obs)\n","                                            rpt_maps[ag][experience[0]] = tmp_map\n","                                        else:\n","                                            if next_obs not in rpt_maps[ag][experience[0]][experience[1]]:\n","                                                rpt_maps[ag][experience[0]][experience[1]].append(next_obs)\n","\n","                            if next_obs in obs_prev_obs[i]:\n","                                if obs_n[i] not in obs_prev_obs[i][next_obs]:\n","                                    obs_prev_obs[i][next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[i][next_obs] = [obs_n[i]]\n","                            if explore_p != -1:\n","                                # print(explore_p)\n","                                explore_prob = explore_p\n","                                # if (explore_p < 0.7) and (not is_decay_set):\n","                                #     print(\"set\")\n","                                #     decay_rate = 0.000008\n","                                #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                                #     is_decay_set = True\n","                            action_ns_tra[i].append(action_n[i])\n","                            # if not env.uavs[i].done:\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # else:\n","                            #     episodic_reward = env.eposidic_reward(True)\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward, env.is_episode_finished(), True)\n","                            # if obs_n[i] == next_obs:\n","                            #     print(\"states are the same\")\n","                            # else:\n","                            #     print(\"states are not the same\")\n","                            if not env.uavs[i].done:\n","                                prev_action_n[i] = action_n[i]\n","                            # print(action_n)\n","                            # print()\n","                                obs_n[i] = next_obs\n","                        else:\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, env.is_episode_finished())\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, episodic_reward, 0, True, False)\n","                            # if rpt_ct == 10:\n","                            #     print(\"uav\", i, \" \", env.uavs[i].Q[obs_n[i]])\n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward[i], True, ac_size[i]])\n","                            if reward[i] < 0:\n","                                for ag in range(env.num_agents):\n","                                    for experience in memory[ag]:\n","                                        if experience[0] not in rpt_maps[ag]:\n","                                            tmp_map = {}\n","                                            for nc in range(action_num):\n","                                                tmp_map[nc + 1] = []\n","                                            tmp_map[experience[1]].append(next_obs)\n","                                            rpt_maps[ag][experience[0]] = tmp_map\n","                                        else:\n","                                            if next_obs not in rpt_maps[ag][experience[0]][experience[1]]:\n","                                                rpt_maps[ag][experience[0]][experience[1]].append(next_obs)\n","\n","                            if next_obs in obs_prev_obs[i]:\n","                                if obs_n[i] not in obs_prev_obs[i][next_obs]:\n","                                    obs_prev_obs[i][next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[i][next_obs] = [obs_n[i]]\n","                            # agent_ids = list(range(env.num_agents))\n","                            # del agent_ids[i]\n","                            # for id in range(len(memory[i]), -1, -1):\n","                            #     more_than_one = True\n","                            #     for ag_id in agent_ids:\n","                            #         if memory[ag_id][id][5] < 2:\n","                            #             more_than_one = False\n","                            #             print\n","                            #             break\n","                            #     if more_than_one: \n","                            #         experience = memory[i][id]:\n","                            # for experience in reversed(memory[i]):\n","                            #     if experience[5] > 1:\n","                            #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, episodic_reward, True)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            final_reward = episodic_reward\n","\n","                            # print(\"uav \", i, \" obtain \" \"episodic_reward = \", episodic_reward)\n","\n","                            done = True\n","                            break\n","                        reward[i] = env.reward_fn(i, action_n[i])\n","                    # else:\n","                    #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, reward)\n","                    #     reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                    # print(\"uav \", i, \" \", env.uavs[i].target_gamma_idx[i])\n","\n","                    # for ag_id in range(env.num_agents):\n","                    #     tmp_x = env.uavs[ag_id].target_gamma_idx[ag_id][0] + env.action_gamma_point_mapping[action_n[ag_id]][0]\n","                    #     tmp_y = env.uavs[ag_id].target_gamma_idx[ag_id][1] + env.action_gamma_point_mapping[action_n[ag_id]][1]\n","                    #     print(\"uav \", ag_id, \" \", [tmp_x, tmp_y])\n","                    # print()\n","\n","                    # print(action_n)\n","                    # print(\"uav \", i, \" \", env.get_obs_n()[i])\n","                    # print(\"is_target_reached\")\n","                    # print(is_target_reached)\n","            # Increase decay_step\n","            # if episode >= 4000:\n","            decay_step +=1\n","\n","            # ep_step += 1\n","\n","            if done:\n","                break\n","\n","            env.T += env.t_step\n","            # if rpt_ct == 10:\n","            # cac.append(env.update_cac_map())\n","            # cac_T.append(env.T)\n","\n","            a1 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[1][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[1][1]\n","            a2 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[2][1]\n","            a3 = env.uavs[0].target_gamma_idx[1][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[1][1] == env.uavs[0].target_gamma_idx[2][1]\n","            if (a1 or a2 or a3) and (env.uavs[0].target_gamma_idx[0][0] != -1 and env.uavs[0].target_gamma_idx[1][0] != -1 and env.uavs[0].target_gamma_idx[2][0] != -1):\n","                # print(\"same gamma point\")\n","                # print(env.T)\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[0].target_gamma_idx[1])\n","                # print(env.uavs[0].target_gamma_idx[2])\n","                # print()\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[1].target_gamma_idx[1])\n","                # print(env.uavs[2].target_gamma_idx[2])\n","                same_gamma_point = True\n","                # break\n","            # cur_pos_x = [env.uavs[i].pos[0] for i in range(env.num_agents)]\n","            # cur_pos_y = [env.uavs[i].pos[1] for i in range(env.num_agents)]\n","            # target_pos_x = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[0] for i in range(env.num_agents)]\n","            # target_pos_y = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[1] for i in range(env.num_agents)]\n","            # fig, ax = plt.subplots()\n","            # ax.plot(cur_pos_x, cur_pos_y, 'o')\n","            # ax.plot(target_pos_x, target_pos_y, 'x')\n","            # plt.show()\n","            ep_step += 1\n","            # print()\n","            # if ep_step == 10:\n","            #     break\n","        # episodic_reward = env.eposidic_reward()\n","        # for i in range(env.num_agents):\n","        #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, episodic_reward)\n","        #     print(\"episodic_reward = \", episodic_reward)\n","            # print(env.uavs[i].Q)\n","        # print(\"len of memory[i] = \", len(memory[0]))\n","        # print(\"Explore P \", explore_prob)\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        # Q-table update\n","        for i in range(env.num_agents):\n","            # if i == 2:\n","            #     continue\n","            # first_branch_set = False\n","            # env.add_Q_table(i)\n","            for step, experience in enumerate(reversed(memory[i])):\n","                # env.update_Q_table(i, experience[0], experience[1] - 1, final_reward)\n","\n","                # num_prev_branches = len(obs_prev_obs[i][experience[2]])\n","                # if step == 0:\n","                # #     # print(\"done = \", experience[4])\n","                #     # print(final_reward + experience[3])\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, num_prev_branches, True, True, episode)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward, final_reward + experience[3], num_prev_branches, False, True, episode)\n","\n","                # env.update_max_Q(i, experience[0], experience[1], final_reward)\n","\n","                env.update_Q_(i, experience[0], experience[1], final_reward)\n","\n","                # if experience[0] in obs_prev_obs:\n","                #     if (len(obs_prev_obs[experience[0]]) > 1) and (first_branch_set == False):\n","                #         first_branch_set = True\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 1, False, True)\n","                #     else:\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","\n","                # if step == 0:\n","                #     # print(\"done = \", experience[4])\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, step, True, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, step, False, True)\n","\n","        #     print()\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        if env.is_fully_covered():\n","            str_ep = \" is\"\n","            Ts.append(env.T)\n","            final_rs.append(final_reward)\n","            if len(Ts) >= 100:\n","                mean_Ts.append(np.mean(Ts[-100:]))\n","        else:\n","            str_ep = \" not\"\n","            # print()\n","            # print(o_action_n)\n","            # print(prev_action_n)\n","            # for ag_id in range(env.num_agents):\n","            #     cur_uav = env.uavs[ag_id]\n","            #     # target_pos = env.index_to_pos(cur_uav.target_gamma_idx[ag_id])\n","            #     # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","            #     # print(\"velocity = \", LA.norm(cur_uav.velocity))\n","            #     print(cur_uav.gamma_map)\n","            #     print(cur_uav.target_gamma_idx)\n","            #     print(cur_uav.done)\n","            # break\n","        print(\"episode \", episode, str_ep, \" fully covered\")\n","        print(env.T)\n","        if prev_T == env.T:\n","            rpt_ct += 1\n","        else:\n","            rpt_ct = 0\n","        prev_T = env.T\n","        for i in range(env.num_agents):\n","            T_memory[i][env.T] = memory[i]\n","        # print(\"len of T_memory = \", len(T_memory[0]))\n","        # print(\"len of Ts = \")\n","        epc_T.append(env.T)\n","        if converge(epc_T):\n","            print(\"episode \", episode)\n","            print(env.T)\n","            break\n","        # max_q = np.max([np.max(env.uavs[0].Q[o_obs_n[0]]), np.max(env.uavs[1].Q[o_obs_n[1]]), np.max(env.uavs[2].Q[o_obs_n[2]])])\n","        # if np.max(env.uavs[0].Q[o_obs_n[0]]) > 64 or np.max(env.uavs[1].Q[o_obs_n[1]]) > 64 or np.max(env.uavs[2].Q[o_obs_n[2]]) > 64:\n","        #     break\n","        # if len(Ts) > 0:\n","        #     min_T = np.min(Ts)\n","        #     if min_T\n","        # if (max_q not in env.uavs[0].Q[o_obs_n[0]]) or (max_q not in env.uavs[1].Q[o_obs_n[1]]) or (max_q not in env.uavs[2].Q[o_obs_n[2]]):\n","        #     print(\"max not in\")\n","        #     break \n","        # print(env.uavs[0].Q)\n","        print()\n","        # if env.T < 165:\n","        #     break\n","        # print(env.num_repeated_obs)\n","        # # print(env.T)\n","        # print(env.uavs[0].tra_map)\n","        # print()\n","        # print(env.uavs[1].tra_map)\n","        # print()\n","        # print(env.uavs[2].tra_map)\n","        # print()\n","        # if same_gamma_point:\n","        #     break\n","        # if len(Ts) == 100:\n","        #     break\n","    # print(env.uavs[0].Q[o_action_n[0]])\n","    print()\n","    # print(env.uavs[1].Q[o_action_n[1]])\n","    # print()\n","    # print(env.uavs[2].Q[o_action_n[2]])\n","    # print()\n","    print(\"chosen actions \", o_action_n)\n","    print(env.count_map)\n","    print()\n","    if Ts:\n","        print(\"mean T = \", np.mean(Ts))\n","        print(\"min T = \", np.min(Ts))\n","    print(\"diag_count = \", diag_count)\n","    print()\n","    \n","    return Ts, final_rs, env, o_obs_n, obs_prev_obs, pos_tra, T_memory, cac, cac_T, diag_count, np.sum(env.count_map) - env.k * env.l\n","    # Ts_s.append(Ts)\n","    # final_rs_s.append(final_rs)\n","    # ep_T.append(env.T)\n","    # print(\"chosen actions \", o_action_n, \" env.T \", env.T)\n","# for i in range(env.num_agents):\n","#     print(\"uav \", i)\n","#     for experience in memory[i]:\n","#         print(env.uavs[i].Q[experience[0]])\n","#     print()\n","    # c_Ts[time] = np.min(Ts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMqG5M-Kzlrz","colab_type":"code","colab":{}},"source":["def train_initial_action_n(explore_start, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs):\n","    Ts = []\n","    mean_Ts = []\n","    final_rs = []\n","    # diag_count = 0\n","    s_actions = [3, 5, 7, 9]\n","\n","    total_episodes = 1\n","    action_ns_tra = [[] for i in range(env.num_agents)]\n","    # explore_start = 1.0\n","    decay_step = 0\n","    explore_prob = -1\n","    # same_gamma_point = False\n","    max_size = 12\n","    epc_T = deque(np.zeros((max_size), dtype=np.float32), maxlen = max_size)\n","    for episode in range(total_episodes):\n","        # if episode >= num_T - 1:\n","        #     if np.sum(np.diff(Ts[episode : episode + num_T + 1])) / num_T <= 1:\n","        #         print(\"converged\")\n","        #         break\n","        ep_step = 0\n","        ac_size = [9 for i in range(env.num_agents)]\n","        memory = [[] for i in range(env.num_agents)]\n","        pos_tra = [[] for i in range(env.num_agents)]\n","        obs_n, action_n = env.reset()\n","        action_n = np.copy(action_n_)\n","\n","        action_n = [1 for _ in range(env.num_agents)]\n","        # for i in range(env.num_agents):\n","        #     # print(\"uav \", i, \" action_space = \", env.action_space(i, action_n))\n","        #     action_n[i], explore_p, ac_size[i] = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, ep_step, 0)\n","        # print(env.get_obs(0)[0])\n","\n","        for i in range(env.num_agents):\n","            action_ns_tra[i].append(action_n[i])\n","        o_obs_n = np.copy(obs_n)\n","        # print(obs_n[0])\n","        # break\n","        print(\"chosen actions \", action_n)\n","        # action_n = np.copy(action_n_)\n","        o_action_n = np.copy(action_n)\n","        prev_action_n = np.copy(action_n)\n","        done = False\n","        final_reward = 0\n","        diag_count = 0\n","        # first_step = True\n","        # action_n = [1 for _ in range(env.num_agents)]\n","        # if episode == 100:\n","        #     env.c1_T = 1.37\n","        #     env.c2_T = 1.14\n","        while not env.is_episode_finished():\n","            for i in range(env.num_agents):\n","                if not env.uavs[i].done:\n","                    # print(\"uav \", i)\n","                    # reward = env.reward_fn(i, action_n[i])\n","                    # print(\"reward = \", reward)\n","                    # print(action_n[i])\n","\n","                    if env.T == 0.3:\n","                        reward = env.reward_fn(i, action_n[i])\n","                    #     print(\"reward = \", reward)\n","\n","                    # if the same target gamma point is detected midway through due to r_c is too small\n","                    # update action\n","                    is_same_target_gamma_point = env.is_same_target_gamma_point(i, action_n)\n","                    if is_same_target_gamma_point:\n","                        # print(\"same target gamma point\")\n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # print(action_n[i])\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] -= env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] -= env.action_gamma_point_mapping[prev_action_n[i]][1]\n","                        prev_action_n[i], explore_p, ac_size[i] = env.select_action(i, obs_n[i], action_n, explore_start, explore_stop, decay_rate, decay_step, False, 0)\n","                        if prev_action_n[i] in s_actions:\n","                            diag_count += 1  \n","                        # print(env.uavs[i].target_gamma_idx[i])\n","                        # print(\"changed action \", prev_action_n[i])\n","                        if explore_p != -1:\n","                            explore_prob = explore_p\n","                            # if (explore_p < 0.7) and (not is_decay_set):\n","                            #     decay_rate = 0.000008\n","                            #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                            #     is_decay_set = True\n","                        reward = env.reward_fn(i, prev_action_n[i])\n","                        if action_n[i] != 1:\n","                            action_n[i] = prev_action_n[i]\n","                        if action_n[i] == 1:\n","                            env.uavs[i].target_gamma_idx[i][0] += env.action_gamma_point_mapping[prev_action_n[i]][0]\n","                            env.uavs[i].target_gamma_idx[i][1] += env.action_gamma_point_mapping[prev_action_n[i]][1]\n","\n","                    # print(\"uav \", i, \" action_n[i] = \", action_n[i])\n","                    env.MAS_update(i, action_n[i])   # update target gamma point of uav i\n","                    pos_tra[i].append(env.uavs[i].pos)\n","                    # next_obs = env.fuse_maps(i, action_n, prev_action_n)\n","                    action_n[i] = 1\n","                    # tra_points_n[i].append(env.uavs[i].pos)\n","\n","                    is_target_reached = env.is_target_reached(i)\n","                    if is_target_reached:\n","                        # print(\"uav \", i, \" \", \"target \", env.uavs[i].target_gamma_idx[i] ,\"is reached\")\n","                        # print(\"env.T \", env.T)\n","                        # reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                        # print(\"target gamma point\")\n","                        # print(env.uavs[0].target_gamma_idx[0])\n","                        # print(env.uavs[1].target_gamma_idx[1])\n","                        # print(env.uavs[2].target_gamma_idx[2])\n","                        # print()\n","                        next_obs = env.fuse_maps(i, action_n, prev_action_n)   # update gamma map and target gamma points of other uavs\n","                        # next_obs_n = [env.fuse_maps(k, action_n, prev_action_n) for k in range(env.num_agents)]\n","\n","                        # print()\n","                        # print(next_obs)\n","                        episodic_reward = env.eposidic_reward(False)\n","                        if episodic_reward == -1:\n","                            # print(\"prev_action_n \", prev_action_n)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            action_n[i], explore_p, ac_size[i] = env.select_action(i, next_obs, action_n, explore_start, explore_stop, decay_rate, decay_step, False, 0)\n","                            if action_n[i] in s_actions:\n","                                diag_count += 1  \n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward, env.uavs[i].done, ac_size[i]])\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            if explore_p != -1:\n","                                explore_prob = explore_p\n","                                # if (explore_p < 0.7) and (not is_decay_set):\n","                                #     print(\"set\")\n","                                #     decay_rate = 0.000008\n","                                #     decay_step = np.log((0.7 - explore_stop) / (explore_start - explore_stop)) / (-decay_rate)\n","                                #     is_decay_set = True\n","                            action_ns_tra[i].append(action_n[i])\n","                            # if not env.uavs[i].done:\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward)\n","                            # else:\n","                            #     episodic_reward = env.eposidic_reward(True)\n","                            #     env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward)\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, reward, env.is_episode_finished(), True)\n","                            # if obs_n[i] == next_obs:\n","                            #     print(\"states are the same\")\n","                            # else:\n","                            #     print(\"states are not the same\")\n","                            # if not env.uavs[i].done:\n","                            prev_action_n[i] = action_n[i]\n","                            # print(action_n)\n","                            # print()\n","                            obs_n[i] = next_obs\n","                        else:\n","                            # env.update_Q_value(i, obs_n[i], next_obs, prev_action_n[i] - 1, episodic_reward, env.is_episode_finished())\n","                            memory[i].append([obs_n[i], prev_action_n[i], next_obs, reward, True, ac_size[i]])\n","                            if next_obs in obs_prev_obs:\n","                                if obs_n[i] not in obs_prev_obs[next_obs]:\n","                                    obs_prev_obs[next_obs].append(obs_n[i])\n","                            else:\n","                                obs_prev_obs[next_obs] = [obs_n[i]]\n","                            # agent_ids = list(range(env.num_agents))\n","                            # del agent_ids[i]\n","                            # for id in range(len(memory[i]), -1, -1):\n","                            #     more_than_one = True\n","                            #     for ag_id in agent_ids:\n","                            #         if memory[ag_id][id][5] < 2:\n","                            #             more_than_one = False\n","                            #             print\n","                            #             break\n","                            #     if more_than_one: \n","                            #         experience = memory[i][id]:\n","                            # for experience in reversed(memory[i]):\n","                            #     if experience[5] > 1:\n","                            #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, episodic_reward, True)\n","                            # if i == 0:\n","                            #     print(env.uavs[i].Q[obs_n[i]])\n","                            final_reward = episodic_reward\n","                            # print(\"uav \", i, \" obtain \" \"episodic_reward = \", episodic_reward)\n","                            done = True\n","                            break\n","                        reward = env.reward_fn(i, action_n[i])\n","                    # else:\n","                    #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, reward)\n","                    #     reward = env.reward_fn(i, action_n[i])\n","                        # print(\"reward = \", reward)\n","                        # print()\n","                    # print(\"uav \", i, \" \", env.uavs[i].target_gamma_idx[i])\n","\n","                    # for ag_id in range(env.num_agents):\n","                    #     tmp_x = env.uavs[ag_id].target_gamma_idx[ag_id][0] + env.action_gamma_point_mapping[action_n[ag_id]][0]\n","                    #     tmp_y = env.uavs[ag_id].target_gamma_idx[ag_id][1] + env.action_gamma_point_mapping[action_n[ag_id]][1]\n","                    #     print(\"uav \", ag_id, \" \", [tmp_x, tmp_y])\n","                    # print()\n","\n","                    # print(action_n)\n","                    # print(\"uav \", i, \" \", env.get_obs_n()[i])\n","                    # print(\"is_target_reached\")\n","                    # print(is_target_reached)\n","            # Increase decay_step\n","            decay_step +=1\n","\n","            # ep_step += 1\n","\n","            if done:\n","                break\n","\n","            env.T += env.t_step\n","            a1 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[1][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[1][1]\n","            a2 = env.uavs[0].target_gamma_idx[0][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[0][1] == env.uavs[0].target_gamma_idx[2][1]\n","            a3 = env.uavs[0].target_gamma_idx[1][0] == env.uavs[0].target_gamma_idx[2][0] and env.uavs[0].target_gamma_idx[1][1] == env.uavs[0].target_gamma_idx[2][1]\n","            if (a1 or a2 or a3) and (env.uavs[0].target_gamma_idx[0][0] != -1 and env.uavs[0].target_gamma_idx[1][0] != -1 and env.uavs[0].target_gamma_idx[2][0] != -1):\n","                # print(\"same gamma point\")\n","                # print(env.T)\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[0].target_gamma_idx[1])\n","                # print(env.uavs[0].target_gamma_idx[2])\n","                # print()\n","                # print(env.uavs[0].target_gamma_idx[0])\n","                # print(env.uavs[1].target_gamma_idx[1])\n","                # print(env.uavs[2].target_gamma_idx[2])\n","                same_gamma_point = True\n","                # break\n","            # cur_pos_x = [env.uavs[i].pos[0] for i in range(env.num_agents)]\n","            # cur_pos_y = [env.uavs[i].pos[1] for i in range(env.num_agents)]\n","            # target_pos_x = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[0] for i in range(env.num_agents)]\n","            # target_pos_y = [env.index_to_pos(env.uavs[i].target_gamma_idx[i])[1] for i in range(env.num_agents)]\n","            # fig, ax = plt.subplots()\n","            # ax.plot(cur_pos_x, cur_pos_y, 'o')\n","            # ax.plot(target_pos_x, target_pos_y, 'x')\n","            # plt.show()\n","            ep_step += 1\n","            # print()\n","            # if ep_step == 10:\n","            #     break\n","        # episodic_reward = env.eposidic_reward()\n","        # for i in range(env.num_agents):\n","        #     env.update_Q_value(i, obs_n[i], prev_action_n[i] - 1, episodic_reward)\n","        #     print(\"episodic_reward = \", episodic_reward)\n","            # print(env.uavs[i].Q)\n","        # print(\"len of memory[i] = \", len(memory[0]))\n","        print(\"Explore P \", explore_prob)\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        # Q-table update\n","        for i in range(env.num_agents):\n","            # if i == 2:\n","            #     continue\n","            first_branch_set = False\n","            for step, experience in enumerate(reversed(memory[i])):\n","                num_prev_branches = len(obs_prev_obs[experience[2]])\n","                if step == 0:\n","                #     # print(\"done = \", experience[4])\n","                    print(final_reward + experience[3])\n","                    env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, num_prev_branches, True, True)\n","                else:\n","                    env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, num_prev_branches, False, True)\n","\n","                # if experience[0] in obs_prev_obs:\n","                #     if (len(obs_prev_obs[experience[0]]) > 1) and (first_branch_set == False):\n","                #         first_branch_set = True\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 1, False, True)\n","                #     else:\n","                #         env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, 0, False, True)\n","\n","                # if step == 0:\n","                #     # print(\"done = \", experience[4])\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, final_reward + experience[3], final_reward, step, True, True)\n","                # else:\n","                #     env.update_Q_value(i, experience[0], experience[2], experience[1] - 1, experience[3], final_reward, step, False, True)\n","\n","        #     print()\n","        # print(env.uavs[0].Q[o_obs_n[0]])\n","        # print(env.uavs[1].Q[o_obs_n[1]])\n","        # print(env.uavs[2].Q[o_obs_n[2]])\n","\n","        if env.is_fully_covered():\n","            str_ep = \" is\"\n","            Ts.append(env.T)\n","            final_rs.append(final_reward)\n","            if len(Ts) >= 100:\n","                mean_Ts.append(np.mean(Ts[-100:]))\n","        else:\n","            str_ep = \" not\"\n","            # print()\n","            # print(o_action_n)\n","            # print(prev_action_n)\n","            # for ag_id in range(env.num_agents):\n","            #     cur_uav = env.uavs[ag_id]\n","            #     # target_pos = env.index_to_pos(cur_uav.target_gamma_idx[ag_id])\n","            #     # print(\"distance to target = \", LA.norm(np.array(cur_uav.pos) - np.array(target_pos)))\n","            #     # print(\"velocity = \", LA.norm(cur_uav.velocity))\n","            #     print(cur_uav.gamma_map)\n","            #     print(cur_uav.target_gamma_idx)\n","            #     print(cur_uav.done)\n","            # break\n","        print(\"episode \", episode, str_ep, \" fully covered\")\n","        print(env.T)\n","        epc_T.append(env.T)\n","        if converge(epc_T):\n","            break\n","        # max_q = np.max([np.max(env.uavs[0].Q[o_obs_n[0]]), np.max(env.uavs[1].Q[o_obs_n[1]]), np.max(env.uavs[2].Q[o_obs_n[2]])])\n","        # if np.max(env.uavs[0].Q[o_obs_n[0]]) > 64 or np.max(env.uavs[1].Q[o_obs_n[1]]) > 64 or np.max(env.uavs[2].Q[o_obs_n[2]]) > 64:\n","        #     break\n","        # if len(Ts) > 0:\n","        #     min_T = np.min(Ts)\n","        #     if min_T\n","        # if (max_q not in env.uavs[0].Q[o_obs_n[0]]) or (max_q not in env.uavs[1].Q[o_obs_n[1]]) or (max_q not in env.uavs[2].Q[o_obs_n[2]]):\n","        #     print(\"max not in\")\n","        #     break \n","        # print(env.uavs[0].Q)\n","        # print()\n","        # if env.T < 165:\n","        #     break\n","        # print(env.num_repeated_obs)\n","        # # print(env.T)\n","        # print(env.uavs[0].tra_map)\n","        # print()\n","        # print(env.uavs[1].tra_map)\n","        # print()\n","        # print(env.uavs[2].tra_map)\n","        # print()\n","        # if same_gamma_point:\n","        #     break\n","        # if len(Ts) == 100:\n","        #     break\n","    # print(env.uavs[0].Q[o_action_n[0]])\n","    # print()\n","    # print(env.uavs[1].Q[o_action_n[1]])\n","    # print()\n","    # print(env.uavs[2].Q[o_action_n[2]])\n","    # print()\n","    print(env.count_map)\n","    print()\n","    print(\"mean T = \", np.mean(Ts))\n","    print(\"min T = \", np.min(Ts))\n","    print(\"diag_count = \", diag_count)\n","\n","    return Ts, final_rs\n","    # Ts_s.append(Ts)\n","    # final_rs_s.append(final_rs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ao400vtChAHd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600134431729,"user_tz":-60,"elapsed":626788,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"bd30fb38-2a12-450d-e5bc-3f2bd0ff4306"},"source":["import csv\n","\n","Ts_s = []\n","final_rs_s = []\n","# r_c_set = list(range(10, 70, 10))\n","r_c_set = [20]\n","num_times = 1\n","# c_Ts = np.zeros(num_times)\n","c_Ts = []\n","for time in range(num_times):\n","    for r_c in r_c_set:\n","        Ts, final_rs, env, o_obs_n, obs_prev_obs, pos_tra, T_memory, cac, cac_T, dg_c, rpt_c = train(m, n, r_c, r_s, d, h, w, alpha, beta, c_alpha, c1_gamma, c2_gamma, epsilon, c_r, t_step, v_max, c1_T, c2_T, r_ref, lambda_, action_num, num_agents, explore_start, explore_stop, decay_rate, decay_step)\n","    # c_Ts[time] = np.min(Ts)\n","    if Ts:\n","        c_Ts.append(Ts[-1])\n","    if dg_c == 0 and rpt_c == 0:\n","        break\n","    # else:\n","    #     c_Ts = np.delete(c_Ts, time)\n","    # print(\"c_Ts = \", c_Ts[time])\n","\n","# plt.plot(cac_T, cac)\n","# coordinates = []\n","# for i in range(len(cac)):\n","#     coordinates.append([cac_T[i], cac[i]])\n","# print(np.array(coordinates).shape)\n","# # with open('out.txt', 'wb') as f:\n","# #     csv.writer(f, delimiter=' ').writerows(np.array(coordinates))\n","with open('rc_20_sec_convergence', 'w') as fh:\n","    spamwriter = csv.writer(fh)\n","    for t in Ts:\n","        spamwriter.writerow([t])\n","# print(np.mean(c_Ts)) \n","# print(np.var(c_Ts))\n","# print(np.mean(c_Ts)) # r_c = 20\n","# print(np.var(c_Ts))\n","\n","    # x = {}\n","    # for i in range(9):\n","    #     if env.uavs[0].Q[o_obs_n[0]][i] == 0:\n","    #         continue\n","    #     for j in range(9):\n","    #         if env.uavs[1].Q[o_obs_n[1]][j] == 0:\n","    #             continue\n","    #         for k in range(9):\n","    #             if env.uavs[2].Q[o_obs_n[2]][k] == 0:\n","    #                 continue\n","    #             # x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = str(i + 1) + str(j + 1) + str(k + 1)\n","    #             if i == 3 and j == 0 and k ==0:\n","    #                 print(env.uavs[0].Q[o_obs_n[0]][3])\n","    #                 print(env.uavs[1].Q[o_obs_n[1]][0])\n","    #                 print(env.uavs[2].Q[o_obs_n[2]][0])\n","    #             x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = [i + 1, j + 1, k + 1]\n","\n","    # # x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n","    # # x = {k:v for k, v in sorted(x.items(), key=lambda item: item[1])}\n","    # x = x.items()\n","    # x = sorted(x, reverse=True)\n","    # num = 0\n","    # for\n","    # action_n_ = [] \n","    # for i in range(env.num_agents):\n","    #     action_n_.append(sorted(T_memory[i].items())[0][1][0][1])\n","    # Ts, final_rs = train_initial_action_n(1.0, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs)\n","\n","    # for action_n_ in x:\n","    #     Ts, final_rs = train_initial_action_n(0.2, explore_stop, decay_rate, decay_step, action_n_[1], env, obs_prev_obs)\n","    #     Ts_s.append(np.min(Ts))\n","    #     num += 1\n","    #     if num == 4:\n","    #         break\n","    # final_rs_s.append(final_rs)\n","    # c_Ts[time] = np.min(Ts)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","episode  1031  is  fully covered\n","229.8\n","\n","episode  1032  is  fully covered\n","196.8\n","\n","episode  1033  not  fully covered\n","393.8\n","\n","episode  1034  is  fully covered\n","194.3\n","\n","episode  1035  is  fully covered\n","173.3\n","\n","episode  1036  is  fully covered\n","181.8\n","\n","episode  1037  is  fully covered\n","168.3\n","\n","episode  1038  is  fully covered\n","171.8\n","\n","episode  1039  not  fully covered\n","393.8\n","\n","episode  1040  is  fully covered\n","173.8\n","\n","episode  1041  is  fully covered\n","202.8\n","\n","episode  1042  is  fully covered\n","228.3\n","\n","episode  1043  is  fully covered\n","209.8\n","\n","episode  1044  is  fully covered\n","189.8\n","\n","episode  1045  is  fully covered\n","203.8\n","\n","episode  1046  is  fully covered\n","171.8\n","\n","episode  1047  is  fully covered\n","207.3\n","\n","episode  1048  is  fully covered\n","148.3\n","\n","episode  1049  is  fully covered\n","180.3\n","\n","episode  1050  is  fully covered\n","190.3\n","\n","episode  1051  is  fully covered\n","161.3\n","\n","episode  1052  is  fully covered\n","184.3\n","\n","episode  1053  is  fully covered\n","185.8\n","\n","episode  1054  is  fully covered\n","192.3\n","\n","episode  1055  is  fully covered\n","197.8\n","\n","episode  1056  is  fully covered\n","239.3\n","\n","episode  1057  is  fully covered\n","201.3\n","\n","episode  1058  is  fully covered\n","194.3\n","\n","episode  1059  is  fully covered\n","164.8\n","\n","episode  1060  is  fully covered\n","235.3\n","\n","episode  1061  is  fully covered\n","166.3\n","\n","episode  1062  is  fully covered\n","162.8\n","\n","episode  1063  is  fully covered\n","197.3\n","\n","episode  1064  not  fully covered\n","393.8\n","\n","episode  1065  is  fully covered\n","178.8\n","\n","episode  1066  is  fully covered\n","161.8\n","\n","episode  1067  not  fully covered\n","393.8\n","\n","episode  1068  is  fully covered\n","224.3\n","\n","episode  1069  is  fully covered\n","221.8\n","\n","episode  1070  is  fully covered\n","182.8\n","\n","episode  1071  is  fully covered\n","196.3\n","\n","episode  1072  is  fully covered\n","163.8\n","\n","episode  1073  is  fully covered\n","178.3\n","\n","episode  1074  is  fully covered\n","197.3\n","\n","episode  1075  is  fully covered\n","155.3\n","\n","episode  1076  is  fully covered\n","257.3\n","\n","episode  1077  is  fully covered\n","211.8\n","\n","episode  1078  is  fully covered\n","183.3\n","\n","episode  1079  is  fully covered\n","164.8\n","\n","episode  1080  is  fully covered\n","211.3\n","\n","episode  1081  is  fully covered\n","206.8\n","\n","episode  1082  is  fully covered\n","185.3\n","\n","episode  1083  is  fully covered\n","212.3\n","\n","episode  1084  is  fully covered\n","160.8\n","\n","episode  1085  is  fully covered\n","187.8\n","\n","episode  1086  is  fully covered\n","198.8\n","\n","episode  1087  is  fully covered\n","241.8\n","\n","episode  1088  is  fully covered\n","166.8\n","\n","episode  1089  is  fully covered\n","174.3\n","\n","episode  1090  is  fully covered\n","169.3\n","\n","episode  1091  is  fully covered\n","169.3\n","\n","episode  1092  is  fully covered\n","179.8\n","\n","episode  1093  is  fully covered\n","175.8\n","\n","episode  1094  is  fully covered\n","171.3\n","\n","episode  1095  is  fully covered\n","188.3\n","\n","episode  1096  is  fully covered\n","180.3\n","\n","episode  1097  is  fully covered\n","235.8\n","\n","episode  1098  is  fully covered\n","177.8\n","\n","episode  1099  is  fully covered\n","194.3\n","\n","episode  1100  is  fully covered\n","203.8\n","\n","episode  1101  is  fully covered\n","160.8\n","\n","episode  1102  is  fully covered\n","147.8\n","\n","episode  1103  is  fully covered\n","157.8\n","\n","episode  1104  is  fully covered\n","187.3\n","\n","episode  1105  is  fully covered\n","173.3\n","\n","episode  1106  is  fully covered\n","163.3\n","\n","episode  1107  is  fully covered\n","182.3\n","\n","episode  1108  is  fully covered\n","183.3\n","\n","episode  1109  is  fully covered\n","180.8\n","\n","episode  1110  is  fully covered\n","162.3\n","\n","episode  1111  is  fully covered\n","276.8\n","\n","episode  1112  is  fully covered\n","193.3\n","\n","episode  1113  is  fully covered\n","156.3\n","\n","episode  1114  is  fully covered\n","181.8\n","\n","episode  1115  is  fully covered\n","171.3\n","\n","episode  1116  is  fully covered\n","147.8\n","\n","episode  1117  is  fully covered\n","187.3\n","\n","episode  1118  is  fully covered\n","186.8\n","\n","episode  1119  is  fully covered\n","202.3\n","\n","episode  1120  is  fully covered\n","170.8\n","\n","episode  1121  is  fully covered\n","150.8\n","\n","episode  1122  is  fully covered\n","196.8\n","\n","episode  1123  is  fully covered\n","185.3\n","\n","episode  1124  is  fully covered\n","149.3\n","\n","episode  1125  is  fully covered\n","186.8\n","\n","episode  1126  is  fully covered\n","195.8\n","\n","episode  1127  is  fully covered\n","157.8\n","\n","episode  1128  is  fully covered\n","157.8\n","\n","episode  1129  is  fully covered\n","174.8\n","\n","episode  1130  is  fully covered\n","184.8\n","\n","episode  1131  is  fully covered\n","252.3\n","\n","episode  1132  is  fully covered\n","223.8\n","\n","episode  1133  is  fully covered\n","178.3\n","\n","episode  1134  is  fully covered\n","163.3\n","\n","episode  1135  is  fully covered\n","169.8\n","\n","episode  1136  is  fully covered\n","199.3\n","\n","episode  1137  is  fully covered\n","172.3\n","\n","episode  1138  is  fully covered\n","179.8\n","\n","episode  1139  is  fully covered\n","154.8\n","\n","episode  1140  is  fully covered\n","234.3\n","\n","episode  1141  is  fully covered\n","174.8\n","\n","episode  1142  is  fully covered\n","203.3\n","\n","episode  1143  is  fully covered\n","226.3\n","\n","episode  1144  is  fully covered\n","195.8\n","\n","episode  1145  is  fully covered\n","162.3\n","\n","episode  1146  is  fully covered\n","161.3\n","\n","episode  1147  is  fully covered\n","164.8\n","\n","episode  1148  is  fully covered\n","180.8\n","\n","episode  1149  is  fully covered\n","190.8\n","\n","episode  1150  is  fully covered\n","161.8\n","\n","episode  1151  is  fully covered\n","234.8\n","\n","episode  1152  is  fully covered\n","202.8\n","\n","episode  1153  is  fully covered\n","151.3\n","\n","episode  1154  is  fully covered\n","176.8\n","\n","episode  1155  is  fully covered\n","231.3\n","\n","episode  1156  is  fully covered\n","154.8\n","\n","episode  1157  is  fully covered\n","168.8\n","\n","episode  1158  is  fully covered\n","171.3\n","\n","episode  1159  is  fully covered\n","208.8\n","\n","episode  1160  is  fully covered\n","199.8\n","\n","episode  1161  is  fully covered\n","219.3\n","\n","episode  1162  is  fully covered\n","183.8\n","\n","episode  1163  is  fully covered\n","175.3\n","\n","episode  1164  is  fully covered\n","213.3\n","\n","episode  1165  is  fully covered\n","203.3\n","\n","episode  1166  is  fully covered\n","197.3\n","\n","episode  1167  not  fully covered\n","393.8\n","\n","episode  1168  is  fully covered\n","164.3\n","\n","episode  1169  is  fully covered\n","193.3\n","\n","episode  1170  is  fully covered\n","171.8\n","\n","episode  1171  is  fully covered\n","177.8\n","\n","episode  1172  is  fully covered\n","189.8\n","\n","episode  1173  is  fully covered\n","147.8\n","\n","episode  1174  is  fully covered\n","186.8\n","\n","episode  1175  is  fully covered\n","189.3\n","\n","episode  1176  is  fully covered\n","194.8\n","\n","episode  1177  is  fully covered\n","206.8\n","\n","episode  1178  is  fully covered\n","175.8\n","\n","episode  1179  is  fully covered\n","190.8\n","\n","episode  1180  is  fully covered\n","223.3\n","\n","episode  1181  is  fully covered\n","212.3\n","\n","episode  1182  is  fully covered\n","189.3\n","\n","episode  1183  is  fully covered\n","160.8\n","\n","episode  1184  is  fully covered\n","165.3\n","\n","episode  1185  is  fully covered\n","227.8\n","\n","episode  1186  is  fully covered\n","173.8\n","\n","episode  1187  is  fully covered\n","203.3\n","\n","episode  1188  is  fully covered\n","197.3\n","\n","episode  1189  is  fully covered\n","161.3\n","\n","episode  1190  is  fully covered\n","167.3\n","\n","episode  1191  is  fully covered\n","170.3\n","\n","episode  1192  is  fully covered\n","160.3\n","\n","episode  1193  is  fully covered\n","175.8\n","\n","episode  1194  is  fully covered\n","184.3\n","\n","episode  1195  is  fully covered\n","147.8\n","\n","episode  1196  is  fully covered\n","189.3\n","\n","episode  1197  is  fully covered\n","162.8\n","\n","episode  1198  is  fully covered\n","235.3\n","\n","episode  1199  is  fully covered\n","169.3\n","\n","episode  1200  is  fully covered\n","184.8\n","\n","episode  1201  is  fully covered\n","190.8\n","\n","episode  1202  is  fully covered\n","209.3\n","\n","episode  1203  is  fully covered\n","175.3\n","\n","episode  1204  is  fully covered\n","182.3\n","\n","episode  1205  is  fully covered\n","177.8\n","\n","episode  1206  is  fully covered\n","194.3\n","\n","episode  1207  is  fully covered\n","155.3\n","\n","episode  1208  is  fully covered\n","159.8\n","\n","episode  1209  is  fully covered\n","158.3\n","\n","episode  1210  is  fully covered\n","213.8\n","\n","episode  1211  is  fully covered\n","190.3\n","\n","episode  1212  is  fully covered\n","180.3\n","\n","episode  1213  is  fully covered\n","175.8\n","\n","episode  1214  is  fully covered\n","194.8\n","\n","episode  1215  is  fully covered\n","185.8\n","\n","episode  1216  is  fully covered\n","164.3\n","\n","episode  1217  is  fully covered\n","170.8\n","\n","episode  1218  is  fully covered\n","187.8\n","\n","episode  1219  is  fully covered\n","202.3\n","\n","episode  1220  is  fully covered\n","207.8\n","\n","episode  1221  is  fully covered\n","233.3\n","\n","episode  1222  is  fully covered\n","196.8\n","\n","episode  1223  not  fully covered\n","393.8\n","\n","episode  1224  is  fully covered\n","270.8\n","\n","episode  1225  is  fully covered\n","223.3\n","\n","episode  1226  is  fully covered\n","224.8\n","\n","episode  1227  is  fully covered\n","186.8\n","\n","episode  1228  is  fully covered\n","156.3\n","\n","episode  1229  is  fully covered\n","182.3\n","\n","episode  1230  is  fully covered\n","170.3\n","\n","episode  1231  not  fully covered\n","393.8\n","\n","episode  1232  is  fully covered\n","169.3\n","\n","episode  1233  is  fully covered\n","178.3\n","\n","episode  1234  is  fully covered\n","191.3\n","\n","episode  1235  is  fully covered\n","194.8\n","\n","episode  1236  is  fully covered\n","203.3\n","\n","episode  1237  is  fully covered\n","169.8\n","\n","episode  1238  is  fully covered\n","173.8\n","\n","episode  1239  is  fully covered\n","211.8\n","\n","episode  1240  is  fully covered\n","150.8\n","\n","episode  1241  is  fully covered\n","151.3\n","\n","episode  1242  is  fully covered\n","151.3\n","\n","episode  1243  is  fully covered\n","191.3\n","\n","episode  1244  is  fully covered\n","142.3\n","\n","episode  1245  is  fully covered\n","204.3\n","\n","episode  1246  is  fully covered\n","213.8\n","\n","episode  1247  is  fully covered\n","207.3\n","\n","episode  1248  is  fully covered\n","198.3\n","\n","episode  1249  is  fully covered\n","192.3\n","\n","episode  1250  not  fully covered\n","393.8\n","\n","episode  1251  is  fully covered\n","231.8\n","\n","episode  1252  is  fully covered\n","198.8\n","\n","episode  1253  is  fully covered\n","172.8\n","\n","episode  1254  is  fully covered\n","167.8\n","\n","episode  1255  is  fully covered\n","167.8\n","\n","episode  1256  is  fully covered\n","181.8\n","\n","episode  1257  is  fully covered\n","190.3\n","\n","episode  1258  is  fully covered\n","219.8\n","\n","episode  1259  is  fully covered\n","220.3\n","\n","episode  1260  is  fully covered\n","230.8\n","\n","episode  1261  is  fully covered\n","162.3\n","\n","episode  1262  is  fully covered\n","172.8\n","\n","episode  1263  not  fully covered\n","393.8\n","\n","episode  1264  is  fully covered\n","179.8\n","\n","episode  1265  not  fully covered\n","393.8\n","\n","episode  1266  is  fully covered\n","173.3\n","\n","episode  1267  is  fully covered\n","188.8\n","\n","episode  1268  is  fully covered\n","180.8\n","\n","episode  1269  is  fully covered\n","176.3\n","\n","episode  1270  is  fully covered\n","185.8\n","\n","episode  1271  is  fully covered\n","160.8\n","\n","episode  1272  is  fully covered\n","142.3\n","\n","episode  1273  is  fully covered\n","220.8\n","\n","episode  1274  not  fully covered\n","393.8\n","\n","episode  1275  is  fully covered\n","246.8\n","\n","episode  1276  is  fully covered\n","156.8\n","\n","episode  1277  is  fully covered\n","176.8\n","\n","episode  1278  is  fully covered\n","186.3\n","\n","episode  1279  is  fully covered\n","177.3\n","\n","episode  1280  is  fully covered\n","163.8\n","\n","episode  1281  is  fully covered\n","173.8\n","\n","episode  1282  is  fully covered\n","155.3\n","\n","episode  1283  is  fully covered\n","194.8\n","\n","episode  1284  is  fully covered\n","186.3\n","\n","episode  1285  is  fully covered\n","149.8\n","\n","episode  1286  is  fully covered\n","166.8\n","\n","episode  1287  is  fully covered\n","156.3\n","\n","episode  1288  is  fully covered\n","208.8\n","\n","episode  1289  is  fully covered\n","154.8\n","\n","episode  1290  is  fully covered\n","157.8\n","\n","episode  1291  is  fully covered\n","167.8\n","\n","episode  1292  is  fully covered\n","157.8\n","\n","episode  1293  is  fully covered\n","161.8\n","\n","episode  1294  is  fully covered\n","171.8\n","\n","episode  1295  is  fully covered\n","213.8\n","\n","episode  1296  is  fully covered\n","222.8\n","\n","episode  1297  is  fully covered\n","154.8\n","\n","episode  1298  is  fully covered\n","194.3\n","\n","episode  1299  is  fully covered\n","192.3\n","\n","episode  1300  is  fully covered\n","157.3\n","\n","episode  1301  is  fully covered\n","193.8\n","\n","episode  1302  is  fully covered\n","178.3\n","\n","episode  1303  is  fully covered\n","142.3\n","\n","episode  1304  is  fully covered\n","181.8\n","\n","episode  1305  is  fully covered\n","203.3\n","\n","episode  1306  is  fully covered\n","158.8\n","\n","episode  1307  is  fully covered\n","203.3\n","\n","episode  1308  is  fully covered\n","177.3\n","\n","episode  1309  is  fully covered\n","174.3\n","\n","episode  1310  is  fully covered\n","151.3\n","\n","episode  1311  is  fully covered\n","171.3\n","\n","episode  1312  is  fully covered\n","162.3\n","\n","episode  1313  is  fully covered\n","179.8\n","\n","episode  1314  is  fully covered\n","190.8\n","\n","episode  1315  is  fully covered\n","144.8\n","\n","episode  1316  is  fully covered\n","179.8\n","\n","episode  1317  is  fully covered\n","175.3\n","\n","episode  1318  is  fully covered\n","207.8\n","\n","episode  1319  is  fully covered\n","198.8\n","\n","episode  1320  is  fully covered\n","208.3\n","\n","episode  1321  is  fully covered\n","220.8\n","\n","episode  1322  is  fully covered\n","189.3\n","\n","episode  1323  is  fully covered\n","157.8\n","\n","episode  1324  is  fully covered\n","187.3\n","\n","episode  1325  is  fully covered\n","179.3\n","\n","episode  1326  is  fully covered\n","202.3\n","\n","episode  1327  is  fully covered\n","191.8\n","\n","episode  1328  is  fully covered\n","182.3\n","\n","episode  1329  is  fully covered\n","218.8\n","\n","episode  1330  not  fully covered\n","393.8\n","\n","episode  1331  is  fully covered\n","156.3\n","\n","episode  1332  is  fully covered\n","164.3\n","\n","episode  1333  is  fully covered\n","220.8\n","\n","episode  1334  is  fully covered\n","218.3\n","\n","episode  1335  is  fully covered\n","167.3\n","\n","episode  1336  is  fully covered\n","175.8\n","\n","episode  1337  is  fully covered\n","142.3\n","\n","episode  1338  is  fully covered\n","178.3\n","\n","episode  1339  is  fully covered\n","184.3\n","\n","episode  1340  is  fully covered\n","195.3\n","\n","episode  1341  is  fully covered\n","202.8\n","\n","episode  1342  is  fully covered\n","151.8\n","\n","episode  1343  is  fully covered\n","233.8\n","\n","episode  1344  is  fully covered\n","194.8\n","\n","episode  1345  is  fully covered\n","205.3\n","\n","episode  1346  is  fully covered\n","170.8\n","\n","episode  1347  is  fully covered\n","188.8\n","\n","episode  1348  not  fully covered\n","393.8\n","\n","episode  1349  is  fully covered\n","193.8\n","\n","episode  1350  is  fully covered\n","185.8\n","\n","episode  1351  is  fully covered\n","201.8\n","\n","episode  1352  is  fully covered\n","191.8\n","\n","episode  1353  is  fully covered\n","168.3\n","\n","episode  1354  is  fully covered\n","165.3\n","\n","episode  1355  is  fully covered\n","209.3\n","\n","episode  1356  is  fully covered\n","190.8\n","\n","episode  1357  is  fully covered\n","142.3\n","\n","episode  1358  is  fully covered\n","200.3\n","\n","episode  1359  is  fully covered\n","233.8\n","\n","episode  1360  is  fully covered\n","171.8\n","\n","episode  1361  is  fully covered\n","148.3\n","\n","episode  1362  is  fully covered\n","189.8\n","\n","episode  1363  is  fully covered\n","190.3\n","\n","episode  1364  not  fully covered\n","393.8\n","\n","episode  1365  is  fully covered\n","180.3\n","\n","episode  1366  is  fully covered\n","163.8\n","\n","episode  1367  is  fully covered\n","177.3\n","\n","episode  1368  is  fully covered\n","187.8\n","\n","episode  1369  is  fully covered\n","203.8\n","\n","episode  1370  is  fully covered\n","194.3\n","\n","episode  1371  is  fully covered\n","174.8\n","\n","episode  1372  is  fully covered\n","188.3\n","\n","episode  1373  is  fully covered\n","147.3\n","\n","episode  1374  is  fully covered\n","172.3\n","\n","episode  1375  is  fully covered\n","185.3\n","\n","episode  1376  is  fully covered\n","167.3\n","\n","episode  1377  is  fully covered\n","164.3\n","\n","episode  1378  is  fully covered\n","142.8\n","\n","episode  1379  is  fully covered\n","215.3\n","\n","episode  1380  is  fully covered\n","164.3\n","\n","episode  1381  is  fully covered\n","161.3\n","\n","episode  1382  is  fully covered\n","167.3\n","\n","episode  1383  is  fully covered\n","151.3\n","\n","episode  1384  is  fully covered\n","168.8\n","\n","episode  1385  is  fully covered\n","197.3\n","\n","episode  1386  is  fully covered\n","250.8\n","\n","episode  1387  is  fully covered\n","171.8\n","\n","episode  1388  is  fully covered\n","200.8\n","\n","episode  1389  is  fully covered\n","165.3\n","\n","episode  1390  is  fully covered\n","181.3\n","\n","episode  1391  is  fully covered\n","191.8\n","\n","episode  1392  is  fully covered\n","157.3\n","\n","episode  1393  not  fully covered\n","393.8\n","\n","episode  1394  is  fully covered\n","226.3\n","\n","episode  1395  is  fully covered\n","256.8\n","\n","episode  1396  is  fully covered\n","165.8\n","\n","episode  1397  is  fully covered\n","154.8\n","\n","episode  1398  is  fully covered\n","177.3\n","\n","episode  1399  is  fully covered\n","168.8\n","\n","episode  1400  is  fully covered\n","160.3\n","\n","episode  1401  is  fully covered\n","192.8\n","\n","episode  1402  is  fully covered\n","185.3\n","\n","episode  1403  is  fully covered\n","147.3\n","\n","episode  1404  is  fully covered\n","142.3\n","\n","episode  1405  is  fully covered\n","169.3\n","\n","episode  1406  is  fully covered\n","163.8\n","\n","episode  1407  is  fully covered\n","212.3\n","\n","episode  1408  is  fully covered\n","222.3\n","\n","episode  1409  is  fully covered\n","201.3\n","\n","episode  1410  is  fully covered\n","147.3\n","\n","episode  1411  is  fully covered\n","201.3\n","\n","episode  1412  is  fully covered\n","220.3\n","\n","episode  1413  is  fully covered\n","182.3\n","\n","episode  1414  is  fully covered\n","184.3\n","\n","episode  1415  not  fully covered\n","393.8\n","\n","episode  1416  is  fully covered\n","194.3\n","\n","episode  1417  is  fully covered\n","178.3\n","\n","episode  1418  is  fully covered\n","155.3\n","\n","episode  1419  not  fully covered\n","393.8\n","\n","episode  1420  is  fully covered\n","213.3\n","\n","episode  1421  is  fully covered\n","142.3\n","\n","episode  1422  is  fully covered\n","148.3\n","\n","episode  1423  is  fully covered\n","163.3\n","\n","episode  1424  is  fully covered\n","164.8\n","\n","episode  1425  is  fully covered\n","155.8\n","\n","episode  1426  is  fully covered\n","173.3\n","\n","episode  1427  is  fully covered\n","181.3\n","\n","episode  1428  is  fully covered\n","197.8\n","\n","episode  1429  is  fully covered\n","154.8\n","\n","episode  1430  is  fully covered\n","232.3\n","\n","episode  1431  is  fully covered\n","202.3\n","\n","episode  1432  is  fully covered\n","173.3\n","\n","episode  1433  is  fully covered\n","206.8\n","\n","episode  1434  not  fully covered\n","393.8\n","\n","episode  1435  is  fully covered\n","164.8\n","\n","episode  1436  is  fully covered\n","151.3\n","\n","episode  1437  is  fully covered\n","177.8\n","\n","episode  1438  is  fully covered\n","179.3\n","\n","episode  1439  is  fully covered\n","229.8\n","\n","episode  1440  is  fully covered\n","162.3\n","\n","episode  1441  is  fully covered\n","216.3\n","\n","episode  1442  is  fully covered\n","198.3\n","\n","episode  1443  is  fully covered\n","171.3\n","\n","episode  1444  is  fully covered\n","147.3\n","\n","episode  1445  is  fully covered\n","193.3\n","\n","episode  1446  is  fully covered\n","191.8\n","\n","episode  1447  is  fully covered\n","202.3\n","\n","episode  1448  is  fully covered\n","193.8\n","\n","episode  1449  is  fully covered\n","166.8\n","\n","episode  1450  is  fully covered\n","193.3\n","\n","episode  1451  is  fully covered\n","155.8\n","\n","episode  1452  is  fully covered\n","174.3\n","\n","episode  1453  is  fully covered\n","168.8\n","\n","episode  1454  is  fully covered\n","169.3\n","\n","episode  1455  is  fully covered\n","191.3\n","\n","episode  1456  is  fully covered\n","184.3\n","\n","episode  1457  is  fully covered\n","222.8\n","\n","episode  1458  is  fully covered\n","171.8\n","\n","episode  1459  is  fully covered\n","184.3\n","\n","episode  1460  is  fully covered\n","169.3\n","\n","episode  1461  is  fully covered\n","189.3\n","\n","episode  1462  is  fully covered\n","168.3\n","\n","episode  1463  is  fully covered\n","188.3\n","\n","episode  1464  is  fully covered\n","235.8\n","\n","episode  1465  is  fully covered\n","210.8\n","\n","episode  1466  is  fully covered\n","158.8\n","\n","episode  1467  is  fully covered\n","182.3\n","\n","episode  1468  is  fully covered\n","191.3\n","\n","episode  1469  is  fully covered\n","184.8\n","\n","episode  1470  is  fully covered\n","155.3\n","\n","episode  1471  is  fully covered\n","165.3\n","\n","episode  1472  is  fully covered\n","164.8\n","\n","episode  1473  is  fully covered\n","174.3\n","\n","episode  1474  is  fully covered\n","220.8\n","\n","episode  1475  is  fully covered\n","196.8\n","\n","episode  1476  is  fully covered\n","167.3\n","\n","episode  1477  is  fully covered\n","207.8\n","\n","episode  1478  is  fully covered\n","214.3\n","\n","episode  1479  is  fully covered\n","168.3\n","\n","episode  1480  is  fully covered\n","155.8\n","\n","episode  1481  is  fully covered\n","177.8\n","\n","episode  1482  is  fully covered\n","207.8\n","\n","episode  1483  is  fully covered\n","168.8\n","\n","episode  1484  is  fully covered\n","173.8\n","\n","episode  1485  is  fully covered\n","169.3\n","\n","episode  1486  is  fully covered\n","184.3\n","\n","episode  1487  is  fully covered\n","167.3\n","\n","episode  1488  is  fully covered\n","162.3\n","\n","episode  1489  is  fully covered\n","191.3\n","\n","episode  1490  is  fully covered\n","166.8\n","\n","episode  1491  is  fully covered\n","181.8\n","\n","episode  1492  is  fully covered\n","167.3\n","\n","episode  1493  is  fully covered\n","176.8\n","\n","episode  1494  is  fully covered\n","161.3\n","\n","episode  1495  is  fully covered\n","177.3\n","\n","episode  1496  is  fully covered\n","217.3\n","\n","episode  1497  is  fully covered\n","186.3\n","\n","episode  1498  is  fully covered\n","194.8\n","\n","episode  1499  is  fully covered\n","184.3\n","\n","episode  1500  is  fully covered\n","150.8\n","\n","episode  1501  is  fully covered\n","161.3\n","\n","episode  1502  is  fully covered\n","191.8\n","\n","episode  1503  is  fully covered\n","221.3\n","\n","episode  1504  is  fully covered\n","156.8\n","\n","episode  1505  is  fully covered\n","237.8\n","\n","episode  1506  is  fully covered\n","167.3\n","\n","episode  1507  is  fully covered\n","144.3\n","\n","episode  1508  is  fully covered\n","206.3\n","\n","episode  1509  is  fully covered\n","198.3\n","\n","episode  1510  is  fully covered\n","180.3\n","\n","episode  1511  is  fully covered\n","160.3\n","\n","episode  1512  not  fully covered\n","393.8\n","\n","episode  1513  is  fully covered\n","185.8\n","\n","episode  1514  is  fully covered\n","175.3\n","\n","episode  1515  is  fully covered\n","169.3\n","\n","episode  1516  is  fully covered\n","188.3\n","\n","episode  1517  is  fully covered\n","158.3\n","\n","episode  1518  is  fully covered\n","149.3\n","\n","episode  1519  is  fully covered\n","148.8\n","\n","episode  1520  is  fully covered\n","154.8\n","\n","episode  1521  is  fully covered\n","206.3\n","\n","episode  1522  is  fully covered\n","220.8\n","\n","episode  1523  is  fully covered\n","204.3\n","\n","episode  1524  is  fully covered\n","186.3\n","\n","episode  1525  is  fully covered\n","148.3\n","\n","episode  1526  is  fully covered\n","172.8\n","\n","episode  1527  is  fully covered\n","195.3\n","\n","episode  1528  is  fully covered\n","177.8\n","\n","episode  1529  is  fully covered\n","229.8\n","\n","episode  1530  is  fully covered\n","200.8\n","\n","episode  1531  is  fully covered\n","182.8\n","\n","episode  1532  is  fully covered\n","193.8\n","\n","episode  1533  is  fully covered\n","212.8\n","\n","episode  1534  is  fully covered\n","231.3\n","\n","episode  1535  is  fully covered\n","161.3\n","\n","episode  1536  is  fully covered\n","237.8\n","\n","episode  1537  is  fully covered\n","177.8\n","\n","episode  1538  is  fully covered\n","215.8\n","\n","episode  1539  is  fully covered\n","151.3\n","\n","episode  1540  is  fully covered\n","151.3\n","\n","episode  1541  is  fully covered\n","178.8\n","\n","episode  1542  is  fully covered\n","215.3\n","\n","episode  1543  is  fully covered\n","165.8\n","\n","episode  1544  is  fully covered\n","213.3\n","\n","episode  1545  is  fully covered\n","219.8\n","\n","episode  1546  is  fully covered\n","155.8\n","\n","episode  1547  is  fully covered\n","233.8\n","\n","episode  1548  is  fully covered\n","214.8\n","\n","episode  1549  is  fully covered\n","186.3\n","\n","episode  1550  is  fully covered\n","142.3\n","\n","episode  1551  is  fully covered\n","167.8\n","\n","episode  1552  is  fully covered\n","142.3\n","\n","episode  1553  is  fully covered\n","236.3\n","\n","episode  1554  is  fully covered\n","147.8\n","\n","episode  1555  is  fully covered\n","187.3\n","\n","episode  1556  is  fully covered\n","181.3\n","\n","episode  1557  is  fully covered\n","177.3\n","\n","episode  1558  is  fully covered\n","142.3\n","\n","episode  1559  is  fully covered\n","207.3\n","\n","episode  1560  is  fully covered\n","174.8\n","\n","episode  1561  is  fully covered\n","170.8\n","\n","episode  1562  is  fully covered\n","162.8\n","\n","episode  1563  is  fully covered\n","188.8\n","\n","episode  1564  is  fully covered\n","176.3\n","\n","episode  1565  is  fully covered\n","161.3\n","\n","episode  1566  is  fully covered\n","168.3\n","\n","episode  1567  is  fully covered\n","210.3\n","\n","episode  1568  is  fully covered\n","151.8\n","\n","episode  1569  is  fully covered\n","230.3\n","\n","episode  1570  is  fully covered\n","218.3\n","\n","episode  1571  is  fully covered\n","209.3\n","\n","episode  1572  is  fully covered\n","154.8\n","\n","episode  1573  is  fully covered\n","175.8\n","\n","episode  1574  is  fully covered\n","150.3\n","\n","episode  1575  is  fully covered\n","192.8\n","\n","episode  1576  is  fully covered\n","164.8\n","\n","episode  1577  is  fully covered\n","204.3\n","\n","episode  1578  is  fully covered\n","147.3\n","\n","episode  1579  is  fully covered\n","203.3\n","\n","episode  1580  is  fully covered\n","189.3\n","\n","episode  1581  is  fully covered\n","194.8\n","\n","episode  1582  is  fully covered\n","182.8\n","\n","episode  1583  is  fully covered\n","195.3\n","\n","episode  1584  is  fully covered\n","149.3\n","\n","episode  1585  is  fully covered\n","188.3\n","\n","episode  1586  is  fully covered\n","154.8\n","\n","episode  1587  is  fully covered\n","168.8\n","\n","episode  1588  is  fully covered\n","228.8\n","\n","episode  1589  is  fully covered\n","161.8\n","\n","episode  1590  is  fully covered\n","142.3\n","\n","episode  1591  is  fully covered\n","142.3\n","\n","episode  1592  is  fully covered\n","175.3\n","\n","episode  1593  is  fully covered\n","178.8\n","\n","episode  1594  is  fully covered\n","184.8\n","\n","episode  1595  is  fully covered\n","151.8\n","\n","episode  1596  is  fully covered\n","176.3\n","\n","episode  1597  is  fully covered\n","154.3\n","\n","episode  1598  is  fully covered\n","187.8\n","\n","episode  1599  is  fully covered\n","238.8\n","\n","episode  1600  is  fully covered\n","217.8\n","\n","episode  1601  is  fully covered\n","211.3\n","\n","episode  1602  is  fully covered\n","183.8\n","\n","episode  1603  is  fully covered\n","202.8\n","\n","episode  1604  is  fully covered\n","162.8\n","\n","episode  1605  is  fully covered\n","213.3\n","\n","episode  1606  is  fully covered\n","202.3\n","\n","episode  1607  is  fully covered\n","207.3\n","\n","episode  1608  is  fully covered\n","142.8\n","\n","episode  1609  is  fully covered\n","159.8\n","\n","episode  1610  is  fully covered\n","164.3\n","\n","episode  1611  is  fully covered\n","198.8\n","\n","episode  1612  is  fully covered\n","194.8\n","\n","episode  1613  is  fully covered\n","167.8\n","\n","episode  1614  is  fully covered\n","144.8\n","\n","episode  1615  is  fully covered\n","169.3\n","\n","episode  1616  is  fully covered\n","184.3\n","\n","episode  1617  is  fully covered\n","182.8\n","\n","episode  1618  is  fully covered\n","197.3\n","\n","episode  1619  is  fully covered\n","161.3\n","\n","episode  1620  is  fully covered\n","161.3\n","\n","episode  1621  is  fully covered\n","170.3\n","\n","episode  1622  is  fully covered\n","170.3\n","\n","episode  1623  is  fully covered\n","162.3\n","\n","episode  1624  not  fully covered\n","393.8\n","\n","episode  1625  is  fully covered\n","142.3\n","\n","episode  1626  is  fully covered\n","161.8\n","\n","episode  1627  is  fully covered\n","151.8\n","\n","episode  1628  is  fully covered\n","155.3\n","\n","episode  1629  is  fully covered\n","150.3\n","\n","episode  1630  is  fully covered\n","208.3\n","\n","episode  1631  is  fully covered\n","181.3\n","\n","episode  1632  is  fully covered\n","241.3\n","\n","episode  1633  is  fully covered\n","164.3\n","\n","episode  1634  is  fully covered\n","158.3\n","\n","episode  1635  is  fully covered\n","156.8\n","\n","episode  1636  is  fully covered\n","148.3\n","\n","episode  1637  is  fully covered\n","267.8\n","\n","episode  1638  is  fully covered\n","181.8\n","\n","episode  1639  is  fully covered\n","169.8\n","\n","episode  1640  is  fully covered\n","234.8\n","\n","episode  1641  is  fully covered\n","191.3\n","\n","episode  1642  is  fully covered\n","186.8\n","\n","episode  1643  is  fully covered\n","275.8\n","\n","episode  1644  is  fully covered\n","162.3\n","\n","episode  1645  is  fully covered\n","142.3\n","\n","episode  1646  is  fully covered\n","229.3\n","\n","episode  1647  is  fully covered\n","201.8\n","\n","episode  1648  is  fully covered\n","184.8\n","\n","episode  1649  is  fully covered\n","166.8\n","\n","episode  1650  is  fully covered\n","154.8\n","\n","episode  1651  is  fully covered\n","157.3\n","\n","episode  1652  is  fully covered\n","188.8\n","\n","episode  1653  is  fully covered\n","215.3\n","\n","episode  1654  is  fully covered\n","161.3\n","\n","episode  1655  is  fully covered\n","215.8\n","\n","episode  1656  is  fully covered\n","149.3\n","\n","episode  1657  is  fully covered\n","185.8\n","\n","episode  1658  is  fully covered\n","180.3\n","\n","episode  1659  is  fully covered\n","170.3\n","\n","episode  1660  is  fully covered\n","151.3\n","\n","episode  1661  is  fully covered\n","169.8\n","\n","episode  1662  is  fully covered\n","196.8\n","\n","episode  1663  is  fully covered\n","156.3\n","\n","episode  1664  is  fully covered\n","217.3\n","\n","episode  1665  is  fully covered\n","175.8\n","\n","episode  1666  is  fully covered\n","190.3\n","\n","episode  1667  is  fully covered\n","174.8\n","\n","episode  1668  is  fully covered\n","207.3\n","\n","episode  1669  is  fully covered\n","231.8\n","\n","episode  1670  is  fully covered\n","166.8\n","\n","episode  1671  is  fully covered\n","142.3\n","\n","episode  1672  is  fully covered\n","163.8\n","\n","episode  1673  is  fully covered\n","160.8\n","\n","episode  1674  is  fully covered\n","167.8\n","\n","episode  1675  is  fully covered\n","231.8\n","\n","episode  1676  is  fully covered\n","177.8\n","\n","episode  1677  is  fully covered\n","191.3\n","\n","episode  1678  is  fully covered\n","147.8\n","\n","episode  1679  is  fully covered\n","141.8\n","\n","episode  1680  is  fully covered\n","162.8\n","\n","episode  1681  is  fully covered\n","141.8\n","\n","episode  1682  is  fully covered\n","148.3\n","\n","episode  1683  is  fully covered\n","178.3\n","\n","episode  1684  is  fully covered\n","141.8\n","\n","episode  1685  is  fully covered\n","158.3\n","\n","episode  1686  is  fully covered\n","158.3\n","\n","episode  1687  is  fully covered\n","232.3\n","\n","episode  1688  is  fully covered\n","142.3\n","\n","episode  1689  is  fully covered\n","171.8\n","\n","episode  1690  is  fully covered\n","225.8\n","\n","episode  1691  is  fully covered\n","213.3\n","\n","episode  1692  not  fully covered\n","393.8\n","\n","episode  1693  is  fully covered\n","161.3\n","\n","episode  1694  is  fully covered\n","248.8\n","\n","episode  1695  is  fully covered\n","216.3\n","\n","episode  1696  is  fully covered\n","212.8\n","\n","episode  1697  is  fully covered\n","193.8\n","\n","episode  1698  not  fully covered\n","393.8\n","\n","episode  1699  is  fully covered\n","180.8\n","\n","episode  1700  is  fully covered\n","183.8\n","\n","episode  1701  is  fully covered\n","154.8\n","\n","episode  1702  is  fully covered\n","141.8\n","\n","episode  1703  is  fully covered\n","173.3\n","\n","episode  1704  is  fully covered\n","181.3\n","\n","episode  1705  is  fully covered\n","155.8\n","\n","episode  1706  is  fully covered\n","236.8\n","\n","episode  1707  is  fully covered\n","223.3\n","\n","episode  1708  is  fully covered\n","151.8\n","\n","episode  1709  is  fully covered\n","209.3\n","\n","episode  1710  is  fully covered\n","162.3\n","\n","episode  1711  is  fully covered\n","159.3\n","\n","episode  1712  is  fully covered\n","168.8\n","\n","episode  1713  is  fully covered\n","190.8\n","\n","episode  1714  is  fully covered\n","160.8\n","\n","episode  1715  is  fully covered\n","213.3\n","\n","episode  1716  is  fully covered\n","194.8\n","\n","episode  1717  is  fully covered\n","155.8\n","\n","episode  1718  is  fully covered\n","189.8\n","\n","episode  1719  is  fully covered\n","188.3\n","\n","episode  1720  is  fully covered\n","157.8\n","\n","episode  1721  not  fully covered\n","393.8\n","\n","episode  1722  is  fully covered\n","211.8\n","\n","episode  1723  is  fully covered\n","150.3\n","\n","episode  1724  is  fully covered\n","150.8\n","\n","episode  1725  is  fully covered\n","171.3\n","\n","episode  1726  is  fully covered\n","196.8\n","\n","episode  1727  is  fully covered\n","161.3\n","\n","episode  1728  is  fully covered\n","190.3\n","\n","episode  1729  is  fully covered\n","182.8\n","\n","episode  1730  is  fully covered\n","207.8\n","\n","episode  1731  is  fully covered\n","162.8\n","\n","episode  1732  is  fully covered\n","189.3\n","\n","episode  1733  is  fully covered\n","172.3\n","\n","episode  1734  is  fully covered\n","182.3\n","\n","episode  1735  is  fully covered\n","229.8\n","\n","episode  1736  is  fully covered\n","157.3\n","\n","episode  1737  is  fully covered\n","220.3\n","\n","episode  1738  is  fully covered\n","150.8\n","\n","episode  1739  is  fully covered\n","208.8\n","\n","episode  1740  is  fully covered\n","170.3\n","\n","episode  1741  is  fully covered\n","229.8\n","\n","episode  1742  is  fully covered\n","163.3\n","\n","episode  1743  is  fully covered\n","141.8\n","\n","episode  1744  is  fully covered\n","142.3\n","\n","episode  1745  is  fully covered\n","171.3\n","\n","episode  1746  is  fully covered\n","154.8\n","\n","episode  1747  is  fully covered\n","267.3\n","\n","episode  1748  is  fully covered\n","141.8\n","\n","episode  1749  is  fully covered\n","216.8\n","\n","episode  1750  is  fully covered\n","187.8\n","\n","episode  1751  is  fully covered\n","213.3\n","\n","episode  1752  is  fully covered\n","168.3\n","\n","episode  1753  is  fully covered\n","149.3\n","\n","episode  1754  is  fully covered\n","209.8\n","\n","episode  1755  is  fully covered\n","190.8\n","\n","episode  1756  is  fully covered\n","237.8\n","\n","episode  1757  is  fully covered\n","184.8\n","\n","episode  1758  is  fully covered\n","154.3\n","\n","episode  1759  is  fully covered\n","198.3\n","\n","episode  1760  is  fully covered\n","161.8\n","\n","episode  1761  is  fully covered\n","173.8\n","\n","episode  1762  is  fully covered\n","198.8\n","\n","episode  1763  not  fully covered\n","393.8\n","\n","episode  1764  is  fully covered\n","160.8\n","\n","episode  1765  is  fully covered\n","207.8\n","\n","episode  1766  is  fully covered\n","142.8\n","\n","episode  1767  is  fully covered\n","171.3\n","\n","episode  1768  is  fully covered\n","162.3\n","\n","episode  1769  is  fully covered\n","165.8\n","\n","episode  1770  is  fully covered\n","141.8\n","\n","episode  1771  is  fully covered\n","234.3\n","\n","episode  1772  is  fully covered\n","147.3\n","\n","episode  1773  is  fully covered\n","195.8\n","\n","episode  1774  is  fully covered\n","161.8\n","\n","episode  1775  is  fully covered\n","161.3\n","\n","episode  1776  is  fully covered\n","170.3\n","\n","episode  1777  is  fully covered\n","189.3\n","\n","episode  1778  is  fully covered\n","144.3\n","\n","episode  1779  is  fully covered\n","205.8\n","\n","episode  1780  is  fully covered\n","192.8\n","\n","episode  1781  is  fully covered\n","154.8\n","\n","episode  1782  is  fully covered\n","195.8\n","\n","episode  1783  is  fully covered\n","232.8\n","\n","episode  1784  is  fully covered\n","211.3\n","\n","episode  1785  is  fully covered\n","178.3\n","\n","episode  1786  is  fully covered\n","157.8\n","\n","episode  1787  is  fully covered\n","172.3\n","\n","episode  1788  is  fully covered\n","162.3\n","\n","episode  1789  is  fully covered\n","167.8\n","\n","episode  1790  is  fully covered\n","141.8\n","\n","episode  1791  is  fully covered\n","201.8\n","\n","episode  1792  is  fully covered\n","198.3\n","\n","episode  1793  is  fully covered\n","168.3\n","\n","episode  1794  is  fully covered\n","166.3\n","\n","episode  1795  is  fully covered\n","148.3\n","\n","episode  1796  is  fully covered\n","208.3\n","\n","episode  1797  is  fully covered\n","176.3\n","\n","episode  1798  is  fully covered\n","150.8\n","\n","episode  1799  is  fully covered\n","163.8\n","\n","episode  1800  is  fully covered\n","164.8\n","\n","episode  1801  is  fully covered\n","141.8\n","\n","episode  1802  is  fully covered\n","178.3\n","\n","episode  1803  is  fully covered\n","155.8\n","\n","episode  1804  is  fully covered\n","148.3\n","\n","episode  1805  is  fully covered\n","194.3\n","\n","episode  1806  is  fully covered\n","187.3\n","\n","episode  1807  is  fully covered\n","164.8\n","\n","episode  1808  is  fully covered\n","154.3\n","\n","episode  1809  is  fully covered\n","183.8\n","\n","episode  1810  is  fully covered\n","151.3\n","\n","episode  1811  is  fully covered\n","157.8\n","\n","episode  1812  is  fully covered\n","144.8\n","\n","episode  1813  is  fully covered\n","171.8\n","\n","episode  1814  is  fully covered\n","174.8\n","\n","episode  1815  is  fully covered\n","160.8\n","\n","episode  1816  is  fully covered\n","202.3\n","\n","episode  1817  is  fully covered\n","151.3\n","\n","episode  1818  is  fully covered\n","155.8\n","\n","episode  1819  is  fully covered\n","197.3\n","\n","episode  1820  is  fully covered\n","154.8\n","\n","episode  1821  is  fully covered\n","168.3\n","\n","episode  1822  is  fully covered\n","151.3\n","\n","episode  1823  is  fully covered\n","142.3\n","\n","episode  1824  is  fully covered\n","222.3\n","\n","episode  1825  is  fully covered\n","167.3\n","\n","episode  1826  is  fully covered\n","150.8\n","\n","episode  1827  is  fully covered\n","141.8\n","\n","episode  1828  is  fully covered\n","165.3\n","\n","episode  1829  is  fully covered\n","163.3\n","\n","episode  1830  is  fully covered\n","189.3\n","\n","episode  1831  is  fully covered\n","201.8\n","\n","episode  1832  is  fully covered\n","141.8\n","\n","episode  1833  is  fully covered\n","150.3\n","\n","episode  1834  is  fully covered\n","164.3\n","\n","episode  1835  is  fully covered\n","156.3\n","\n","episode  1836  is  fully covered\n","195.8\n","\n","episode  1837  is  fully covered\n","159.3\n","\n","episode  1838  is  fully covered\n","141.8\n","\n","episode  1839  is  fully covered\n","160.8\n","\n","episode  1840  is  fully covered\n","190.8\n","\n","episode  1841  is  fully covered\n","212.8\n","\n","episode  1842  is  fully covered\n","141.8\n","\n","episode  1843  is  fully covered\n","213.8\n","\n","episode  1844  is  fully covered\n","171.8\n","\n","episode  1845  is  fully covered\n","161.3\n","\n","episode  1846  is  fully covered\n","175.3\n","\n","episode  1847  is  fully covered\n","167.3\n","\n","episode  1848  is  fully covered\n","184.3\n","\n","episode  1849  is  fully covered\n","154.8\n","\n","episode  1850  is  fully covered\n","151.8\n","\n","episode  1851  is  fully covered\n","155.8\n","\n","episode  1852  is  fully covered\n","211.3\n","\n","episode  1853  is  fully covered\n","193.3\n","\n","episode  1854  is  fully covered\n","167.8\n","\n","episode  1855  is  fully covered\n","196.3\n","\n","episode  1856  is  fully covered\n","190.8\n","\n","episode  1857  is  fully covered\n","141.8\n","\n","episode  1858  is  fully covered\n","219.8\n","\n","episode  1859  is  fully covered\n","174.8\n","\n","episode  1860  is  fully covered\n","142.3\n","\n","episode  1861  is  fully covered\n","171.3\n","\n","episode  1862  is  fully covered\n","151.3\n","\n","episode  1863  is  fully covered\n","156.8\n","\n","episode  1864  is  fully covered\n","180.8\n","\n","episode  1865  is  fully covered\n","161.8\n","\n","episode  1866  is  fully covered\n","147.8\n","\n","episode  1867  is  fully covered\n","141.8\n","\n","episode  1868  is  fully covered\n","141.8\n","\n","episode  1869  is  fully covered\n","141.8\n","\n","episode  1870  is  fully covered\n","167.8\n","\n","episode  1871  is  fully covered\n","234.8\n","\n","episode  1872  is  fully covered\n","197.3\n","\n","episode  1873  is  fully covered\n","165.8\n","\n","episode  1874  is  fully covered\n","175.8\n","\n","episode  1875  is  fully covered\n","167.8\n","\n","episode  1876  is  fully covered\n","158.8\n","\n","episode  1877  is  fully covered\n","163.3\n","\n","episode  1878  is  fully covered\n","198.8\n","\n","episode  1879  is  fully covered\n","167.3\n","\n","episode  1880  is  fully covered\n","206.8\n","\n","episode  1881  is  fully covered\n","141.8\n","\n","episode  1882  is  fully covered\n","154.8\n","\n","episode  1883  is  fully covered\n","160.8\n","\n","episode  1884  is  fully covered\n","161.3\n","\n","episode  1885  is  fully covered\n","199.8\n","\n","episode  1886  is  fully covered\n","182.3\n","\n","episode  1887  is  fully covered\n","214.3\n","\n","episode  1888  is  fully covered\n","167.8\n","\n","episode  1889  is  fully covered\n","161.3\n","\n","episode  1890  is  fully covered\n","141.8\n","\n","episode  1891  is  fully covered\n","162.8\n","\n","episode  1892  is  fully covered\n","191.8\n","\n","episode  1893  is  fully covered\n","204.3\n","\n","episode  1894  is  fully covered\n","152.8\n","\n","episode  1895  is  fully covered\n","165.3\n","\n","episode  1896  is  fully covered\n","141.8\n","\n","episode  1897  is  fully covered\n","172.8\n","\n","episode  1898  is  fully covered\n","141.8\n","\n","episode  1899  is  fully covered\n","209.8\n","\n","episode  1900  is  fully covered\n","141.8\n","\n","episode  1901  is  fully covered\n","155.8\n","\n","episode  1902  is  fully covered\n","142.3\n","\n","episode  1903  is  fully covered\n","142.8\n","\n","episode  1904  is  fully covered\n","151.3\n","\n","episode  1905  is  fully covered\n","151.8\n","\n","episode  1906  is  fully covered\n","157.8\n","\n","episode  1907  is  fully covered\n","151.3\n","\n","episode  1908  is  fully covered\n","153.8\n","\n","episode  1909  is  fully covered\n","189.8\n","\n","episode  1910  is  fully covered\n","151.3\n","\n","episode  1911  is  fully covered\n","151.8\n","\n","episode  1912  not  fully covered\n","393.8\n","\n","episode  1913  is  fully covered\n","196.8\n","\n","episode  1914  is  fully covered\n","190.8\n","\n","episode  1915  is  fully covered\n","141.8\n","\n","episode  1916  is  fully covered\n","194.3\n","\n","episode  1917  is  fully covered\n","167.8\n","\n","episode  1918  is  fully covered\n","184.3\n","\n","episode  1919  is  fully covered\n","157.8\n","\n","episode  1920  is  fully covered\n","142.3\n","\n","episode  1921  is  fully covered\n","160.3\n","\n","episode  1922  is  fully covered\n","161.3\n","\n","episode  1923  is  fully covered\n","268.3\n","\n","episode  1924  is  fully covered\n","161.3\n","\n","episode  1925  is  fully covered\n","151.3\n","\n","episode  1926  is  fully covered\n","141.8\n","\n","episode  1927  is  fully covered\n","166.8\n","\n","episode  1928  is  fully covered\n","170.3\n","\n","episode  1929  is  fully covered\n","172.8\n","\n","episode  1930  is  fully covered\n","243.8\n","\n","episode  1931  is  fully covered\n","202.8\n","\n","episode  1932  is  fully covered\n","191.3\n","\n","episode  1933  is  fully covered\n","150.3\n","\n","episode  1934  is  fully covered\n","141.8\n","\n","episode  1935  is  fully covered\n","154.8\n","\n","episode  1936  is  fully covered\n","157.8\n","\n","episode  1937  is  fully covered\n","173.8\n","\n","episode  1938  is  fully covered\n","169.3\n","\n","episode  1939  is  fully covered\n","155.8\n","\n","episode  1940  is  fully covered\n","196.3\n","\n","episode  1941  is  fully covered\n","210.8\n","\n","episode  1942  is  fully covered\n","204.3\n","\n","episode  1943  is  fully covered\n","142.3\n","\n","episode  1944  is  fully covered\n","154.8\n","\n","episode  1945  is  fully covered\n","208.8\n","\n","episode  1946  is  fully covered\n","251.8\n","\n","episode  1947  is  fully covered\n","141.8\n","\n","episode  1948  is  fully covered\n","175.8\n","\n","episode  1949  is  fully covered\n","163.8\n","\n","episode  1950  is  fully covered\n","180.8\n","\n","episode  1951  is  fully covered\n","160.8\n","\n","episode  1952  is  fully covered\n","161.3\n","\n","episode  1953  is  fully covered\n","151.3\n","\n","episode  1954  is  fully covered\n","158.8\n","\n","episode  1955  is  fully covered\n","151.3\n","\n","episode  1956  is  fully covered\n","196.3\n","\n","episode  1957  is  fully covered\n","150.3\n","\n","episode  1958  is  fully covered\n","154.8\n","\n","episode  1959  is  fully covered\n","158.8\n","\n","episode  1960  is  fully covered\n","150.3\n","\n","episode  1961  is  fully covered\n","158.8\n","\n","episode  1962  is  fully covered\n","141.8\n","\n","episode  1963  is  fully covered\n","141.8\n","\n","episode  1964  is  fully covered\n","164.8\n","\n","episode  1965  is  fully covered\n","159.3\n","\n","episode  1966  is  fully covered\n","151.8\n","\n","episode  1967  is  fully covered\n","210.8\n","\n","episode  1968  is  fully covered\n","188.8\n","\n","episode  1969  is  fully covered\n","142.8\n","\n","episode  1970  is  fully covered\n","194.8\n","\n","episode  1971  is  fully covered\n","188.8\n","\n","episode  1972  is  fully covered\n","141.8\n","\n","episode  1973  is  fully covered\n","141.8\n","\n","episode  1974  is  fully covered\n","141.8\n","\n","episode  1975  is  fully covered\n","180.3\n","\n","episode  1976  is  fully covered\n","171.8\n","\n","episode  1977  is  fully covered\n","154.8\n","\n","episode  1978  is  fully covered\n","180.8\n","\n","episode  1979  is  fully covered\n","185.3\n","\n","episode  1980  is  fully covered\n","153.8\n","\n","episode  1981  is  fully covered\n","142.8\n","\n","episode  1982  is  fully covered\n","194.3\n","\n","episode  1983  is  fully covered\n","163.8\n","\n","episode  1984  is  fully covered\n","174.8\n","\n","episode  1985  is  fully covered\n","200.8\n","\n","episode  1986  is  fully covered\n","196.3\n","\n","episode  1987  not  fully covered\n","393.8\n","\n","episode  1988  is  fully covered\n","141.8\n","\n","episode  1989  is  fully covered\n","193.3\n","\n","episode  1990  is  fully covered\n","154.8\n","\n","episode  1991  is  fully covered\n","164.8\n","\n","episode  1992  is  fully covered\n","168.3\n","\n","episode  1993  is  fully covered\n","150.3\n","\n","episode  1994  is  fully covered\n","164.8\n","\n","episode  1995  is  fully covered\n","147.3\n","\n","episode  1996  is  fully covered\n","165.8\n","\n","episode  1997  is  fully covered\n","206.3\n","\n","episode  1998  is  fully covered\n","230.8\n","\n","episode  1999  is  fully covered\n","151.8\n","\n","episode  2000  is  fully covered\n","165.8\n","\n","episode  2001  is  fully covered\n","141.8\n","\n","episode  2002  is  fully covered\n","142.3\n","\n","episode  2003  is  fully covered\n","175.8\n","\n","episode  2004  is  fully covered\n","174.8\n","\n","episode  2005  is  fully covered\n","141.8\n","\n","episode  2006  is  fully covered\n","155.3\n","\n","episode  2007  is  fully covered\n","154.3\n","\n","episode  2008  is  fully covered\n","186.8\n","\n","episode  2009  is  fully covered\n","141.8\n","\n","episode  2010  is  fully covered\n","155.8\n","\n","episode  2011  is  fully covered\n","233.3\n","\n","episode  2012  is  fully covered\n","160.8\n","\n","episode  2013  is  fully covered\n","141.8\n","\n","episode  2014  is  fully covered\n","141.8\n","\n","episode  2015  is  fully covered\n","163.8\n","\n","episode  2016  is  fully covered\n","225.8\n","\n","episode  2017  is  fully covered\n","160.3\n","\n","episode  2018  is  fully covered\n","175.8\n","\n","episode  2019  is  fully covered\n","209.3\n","\n","episode  2020  is  fully covered\n","217.3\n","\n","episode  2021  is  fully covered\n","226.3\n","\n","episode  2022  is  fully covered\n","161.3\n","\n","episode  2023  is  fully covered\n","147.8\n","\n","episode  2024  is  fully covered\n","141.8\n","\n","episode  2025  is  fully covered\n","141.8\n","\n","episode  2026  is  fully covered\n","156.3\n","\n","episode  2027  is  fully covered\n","141.8\n","\n","episode  2028  is  fully covered\n","214.8\n","\n","episode  2029  is  fully covered\n","157.8\n","\n","episode  2030  is  fully covered\n","209.3\n","\n","episode  2031  is  fully covered\n","160.3\n","\n","episode  2032  is  fully covered\n","189.8\n","\n","episode  2033  is  fully covered\n","185.8\n","\n","episode  2034  is  fully covered\n","150.8\n","\n","episode  2035  is  fully covered\n","161.8\n","\n","episode  2036  is  fully covered\n","159.3\n","\n","episode  2037  is  fully covered\n","156.3\n","\n","episode  2038  is  fully covered\n","156.3\n","\n","episode  2039  is  fully covered\n","180.8\n","\n","episode  2040  is  fully covered\n","184.3\n","\n","episode  2041  is  fully covered\n","177.3\n","\n","episode  2042  is  fully covered\n","157.8\n","\n","episode  2043  is  fully covered\n","157.8\n","\n","episode  2044  is  fully covered\n","155.8\n","\n","episode  2045  is  fully covered\n","167.8\n","\n","episode  2046  is  fully covered\n","166.3\n","\n","episode  2047  is  fully covered\n","142.3\n","\n","episode  2048  is  fully covered\n","155.3\n","\n","episode  2049  is  fully covered\n","161.8\n","\n","episode  2050  is  fully covered\n","150.8\n","\n","episode  2051  is  fully covered\n","170.3\n","\n","episode  2052  is  fully covered\n","144.8\n","\n","episode  2053  is  fully covered\n","178.8\n","\n","episode  2054  is  fully covered\n","190.3\n","\n","episode  2055  is  fully covered\n","164.8\n","\n","episode  2056  is  fully covered\n","142.3\n","\n","episode  2057  is  fully covered\n","148.3\n","\n","episode  2058  is  fully covered\n","181.3\n","\n","episode  2059  is  fully covered\n","141.8\n","\n","episode  2060  is  fully covered\n","141.8\n","\n","episode  2061  is  fully covered\n","209.3\n","\n","episode  2062  is  fully covered\n","141.8\n","\n","episode  2063  is  fully covered\n","164.3\n","\n","episode  2064  is  fully covered\n","165.8\n","\n","episode  2065  is  fully covered\n","141.8\n","\n","episode  2066  is  fully covered\n","211.3\n","\n","episode  2067  is  fully covered\n","167.8\n","\n","episode  2068  is  fully covered\n","164.8\n","\n","episode  2069  is  fully covered\n","191.3\n","\n","episode  2070  is  fully covered\n","179.3\n","\n","episode  2071  not  fully covered\n","393.8\n","\n","episode  2072  is  fully covered\n","161.3\n","\n","episode  2073  is  fully covered\n","186.3\n","\n","episode  2074  is  fully covered\n","141.8\n","\n","episode  2075  is  fully covered\n","141.8\n","\n","episode  2076  is  fully covered\n","165.3\n","\n","episode  2077  is  fully covered\n","166.3\n","\n","episode  2078  is  fully covered\n","179.3\n","\n","episode  2079  is  fully covered\n","141.8\n","\n","episode  2080  is  fully covered\n","163.3\n","\n","episode  2081  is  fully covered\n","196.8\n","\n","episode  2082  is  fully covered\n","154.8\n","\n","episode  2083  is  fully covered\n","169.8\n","\n","episode  2084  is  fully covered\n","202.3\n","\n","episode  2085  is  fully covered\n","182.8\n","\n","episode  2086  is  fully covered\n","161.8\n","\n","episode  2087  is  fully covered\n","195.8\n","\n","episode  2088  is  fully covered\n","141.8\n","\n","episode  2089  is  fully covered\n","207.3\n","\n","episode  2090  is  fully covered\n","141.8\n","\n","episode  2091  is  fully covered\n","150.8\n","\n","episode  2092  is  fully covered\n","167.8\n","\n","episode  2093  is  fully covered\n","167.8\n","\n","episode  2094  is  fully covered\n","151.8\n","\n","episode  2095  is  fully covered\n","186.3\n","\n","episode  2096  is  fully covered\n","147.8\n","\n","episode  2097  is  fully covered\n","141.8\n","\n","episode  2098  is  fully covered\n","173.3\n","\n","episode  2099  is  fully covered\n","151.3\n","\n","episode  2100  is  fully covered\n","201.3\n","\n","episode  2101  is  fully covered\n","168.8\n","\n","episode  2102  is  fully covered\n","185.3\n","\n","episode  2103  is  fully covered\n","261.8\n","\n","episode  2104  is  fully covered\n","141.8\n","\n","episode  2105  is  fully covered\n","175.8\n","\n","episode  2106  is  fully covered\n","151.8\n","\n","episode  2107  is  fully covered\n","170.8\n","\n","episode  2108  is  fully covered\n","141.8\n","\n","episode  2109  is  fully covered\n","166.8\n","\n","episode  2110  is  fully covered\n","141.8\n","\n","episode  2111  is  fully covered\n","150.8\n","\n","episode  2112  is  fully covered\n","165.3\n","\n","episode  2113  is  fully covered\n","151.8\n","\n","episode  2114  is  fully covered\n","171.8\n","\n","episode  2115  is  fully covered\n","167.8\n","\n","episode  2116  is  fully covered\n","142.3\n","\n","episode  2117  is  fully covered\n","141.8\n","\n","episode  2118  is  fully covered\n","187.8\n","\n","episode  2119  is  fully covered\n","155.3\n","\n","episode  2120  is  fully covered\n","141.8\n","\n","episode  2121  is  fully covered\n","171.8\n","\n","episode  2122  is  fully covered\n","150.3\n","\n","episode  2123  is  fully covered\n","150.3\n","\n","episode  2124  is  fully covered\n","147.3\n","\n","episode  2125  is  fully covered\n","141.8\n","\n","episode  2126  is  fully covered\n","156.3\n","\n","episode  2127  is  fully covered\n","193.3\n","\n","episode  2128  is  fully covered\n","216.8\n","\n","episode  2129  is  fully covered\n","154.8\n","\n","episode  2130  is  fully covered\n","213.8\n","\n","episode  2131  is  fully covered\n","198.8\n","\n","episode  2132  is  fully covered\n","148.3\n","\n","episode  2133  is  fully covered\n","149.3\n","\n","episode  2134  is  fully covered\n","154.8\n","\n","episode  2135  is  fully covered\n","190.3\n","\n","episode  2136  is  fully covered\n","165.8\n","\n","episode  2137  is  fully covered\n","221.3\n","\n","episode  2138  is  fully covered\n","199.3\n","\n","episode  2139  is  fully covered\n","167.8\n","\n","episode  2140  is  fully covered\n","141.8\n","\n","episode  2141  is  fully covered\n","150.3\n","\n","episode  2142  is  fully covered\n","142.8\n","\n","episode  2143  is  fully covered\n","150.8\n","\n","episode  2144  is  fully covered\n","144.3\n","\n","episode  2145  is  fully covered\n","176.8\n","\n","episode  2146  is  fully covered\n","205.3\n","\n","episode  2147  is  fully covered\n","165.8\n","\n","episode  2148  is  fully covered\n","211.3\n","\n","episode  2149  is  fully covered\n","150.3\n","\n","episode  2150  is  fully covered\n","197.3\n","\n","episode  2151  is  fully covered\n","141.8\n","\n","episode  2152  is  fully covered\n","141.8\n","\n","episode  2153  is  fully covered\n","154.8\n","\n","episode  2154  is  fully covered\n","154.8\n","\n","episode  2155  is  fully covered\n","150.3\n","\n","episode  2156  is  fully covered\n","163.3\n","\n","episode  2157  is  fully covered\n","141.8\n","\n","episode  2158  not  fully covered\n","393.8\n","\n","episode  2159  is  fully covered\n","148.3\n","\n","episode  2160  is  fully covered\n","141.8\n","\n","episode  2161  is  fully covered\n","185.8\n","\n","episode  2162  is  fully covered\n","155.3\n","\n","episode  2163  is  fully covered\n","141.8\n","\n","episode  2164  is  fully covered\n","164.8\n","\n","episode  2165  is  fully covered\n","156.3\n","\n","episode  2166  is  fully covered\n","141.8\n","\n","episode  2167  is  fully covered\n","171.3\n","\n","episode  2168  is  fully covered\n","141.8\n","\n","episode  2169  is  fully covered\n","169.8\n","\n","episode  2170  is  fully covered\n","251.3\n","\n","episode  2171  is  fully covered\n","141.8\n","\n","episode  2172  is  fully covered\n","141.8\n","\n","episode  2173  is  fully covered\n","148.3\n","\n","episode  2174  is  fully covered\n","141.8\n","\n","episode  2175  is  fully covered\n","141.8\n","\n","episode  2176  is  fully covered\n","180.8\n","\n","episode  2177  is  fully covered\n","175.3\n","\n","episode  2178  is  fully covered\n","141.8\n","\n","episode  2179  is  fully covered\n","161.3\n","\n","episode  2180  is  fully covered\n","155.3\n","\n","episode  2181  is  fully covered\n","181.3\n","\n","episode  2182  is  fully covered\n","243.3\n","\n","episode  2183  is  fully covered\n","166.8\n","\n","episode  2184  is  fully covered\n","181.3\n","\n","episode  2185  is  fully covered\n","162.8\n","\n","episode  2186  is  fully covered\n","162.3\n","\n","episode  2187  is  fully covered\n","200.3\n","\n","episode  2188  is  fully covered\n","147.8\n","\n","episode  2189  is  fully covered\n","172.8\n","\n","episode  2190  is  fully covered\n","157.8\n","\n","episode  2191  is  fully covered\n","169.3\n","\n","episode  2192  is  fully covered\n","151.8\n","\n","episode  2193  is  fully covered\n","141.8\n","\n","episode  2194  is  fully covered\n","206.8\n","\n","episode  2195  is  fully covered\n","141.8\n","\n","episode  2196  is  fully covered\n","181.3\n","\n","episode  2197  is  fully covered\n","188.3\n","\n","episode  2198  is  fully covered\n","182.3\n","\n","episode  2199  is  fully covered\n","147.3\n","\n","episode  2200  is  fully covered\n","175.8\n","\n","episode  2201  is  fully covered\n","141.8\n","\n","episode  2202  is  fully covered\n","166.8\n","\n","episode  2203  is  fully covered\n","195.3\n","\n","episode  2204  is  fully covered\n","141.8\n","\n","episode  2205  is  fully covered\n","141.8\n","\n","episode  2206  is  fully covered\n","141.8\n","\n","episode  2207  is  fully covered\n","147.8\n","\n","episode  2208  is  fully covered\n","172.3\n","\n","episode  2209  is  fully covered\n","161.3\n","\n","episode  2210  is  fully covered\n","155.8\n","\n","episode  2211  is  fully covered\n","141.8\n","\n","episode  2212  is  fully covered\n","157.8\n","\n","episode  2213  is  fully covered\n","155.3\n","\n","episode  2214  is  fully covered\n","141.8\n","\n","episode  2215  is  fully covered\n","191.8\n","\n","episode  2216  is  fully covered\n","154.8\n","\n","episode  2217  is  fully covered\n","208.3\n","\n","episode  2218  is  fully covered\n","167.3\n","\n","episode  2219  is  fully covered\n","215.8\n","\n","episode  2220  is  fully covered\n","141.8\n","\n","episode  2221  is  fully covered\n","175.8\n","\n","episode  2222  is  fully covered\n","141.8\n","\n","episode  2223  is  fully covered\n","209.3\n","\n","episode  2224  is  fully covered\n","155.8\n","\n","episode  2225  is  fully covered\n","206.3\n","\n","episode  2226  is  fully covered\n","154.3\n","\n","episode  2227  is  fully covered\n","165.8\n","\n","episode  2228  is  fully covered\n","181.3\n","\n","episode  2229  is  fully covered\n","141.8\n","\n","episode  2230  is  fully covered\n","141.8\n","\n","episode  2231  is  fully covered\n","141.8\n","\n","episode  2232  is  fully covered\n","200.8\n","\n","episode  2233  is  fully covered\n","173.3\n","\n","episode  2234  is  fully covered\n","200.8\n","\n","episode  2235  is  fully covered\n","142.3\n","\n","episode  2236  is  fully covered\n","141.8\n","\n","episode  2237  is  fully covered\n","209.3\n","\n","episode  2238  is  fully covered\n","141.8\n","\n","episode  2239  is  fully covered\n","141.8\n","\n","episode  2240  is  fully covered\n","145.3\n","\n","episode  2241  is  fully covered\n","179.3\n","\n","episode  2242  not  fully covered\n","393.8\n","\n","episode  2243  is  fully covered\n","154.8\n","\n","episode  2244  is  fully covered\n","189.3\n","\n","episode  2245  is  fully covered\n","141.8\n","\n","episode  2246  is  fully covered\n","206.3\n","\n","episode  2247  is  fully covered\n","168.8\n","\n","episode  2248  is  fully covered\n","142.3\n","\n","episode  2249  is  fully covered\n","154.8\n","\n","episode  2250  is  fully covered\n","141.8\n","\n","episode  2251  is  fully covered\n","141.8\n","\n","episode  2252  is  fully covered\n","141.8\n","\n","episode  2253  is  fully covered\n","175.3\n","\n","episode  2254  not  fully covered\n","393.8\n","\n","episode  2255  is  fully covered\n","176.8\n","\n","episode  2256  is  fully covered\n","142.3\n","\n","episode  2257  is  fully covered\n","206.3\n","\n","episode  2258  is  fully covered\n","180.3\n","\n","episode  2259  is  fully covered\n","141.8\n","\n","episode  2260  is  fully covered\n","154.8\n","\n","episode  2261  is  fully covered\n","169.3\n","\n","episode  2262  is  fully covered\n","141.8\n","\n","episode  2263  is  fully covered\n","154.3\n","\n","episode  2264  is  fully covered\n","174.8\n","\n","episode  2265  is  fully covered\n","167.8\n","\n","episode  2266  is  fully covered\n","157.8\n","\n","episode  2267  is  fully covered\n","154.8\n","\n","episode  2268  is  fully covered\n","158.3\n","\n","episode  2269  is  fully covered\n","147.3\n","\n","episode  2270  is  fully covered\n","179.8\n","\n","episode  2271  is  fully covered\n","170.8\n","\n","episode  2272  is  fully covered\n","164.8\n","\n","episode  2273  is  fully covered\n","155.3\n","\n","episode  2274  is  fully covered\n","141.8\n","\n","episode  2275  is  fully covered\n","142.3\n","\n","episode  2276  is  fully covered\n","141.8\n","\n","episode  2277  is  fully covered\n","180.3\n","\n","episode  2278  is  fully covered\n","161.3\n","\n","episode  2279  is  fully covered\n","188.8\n","\n","episode  2280  is  fully covered\n","147.3\n","\n","episode  2281  is  fully covered\n","182.3\n","\n","episode  2282  is  fully covered\n","171.8\n","\n","episode  2283  is  fully covered\n","194.3\n","\n","episode  2284  is  fully covered\n","196.3\n","\n","episode  2285  is  fully covered\n","169.3\n","\n","episode  2286  is  fully covered\n","141.8\n","\n","episode  2287  is  fully covered\n","187.3\n","\n","episode  2288  is  fully covered\n","209.3\n","\n","episode  2289  is  fully covered\n","177.3\n","\n","episode  2290  is  fully covered\n","147.3\n","\n","episode  2291  is  fully covered\n","141.8\n","\n","episode  2292  is  fully covered\n","173.8\n","\n","episode  2293  is  fully covered\n","160.3\n","\n","episode  2294  is  fully covered\n","150.8\n","\n","episode  2295  is  fully covered\n","157.8\n","\n","episode  2296  is  fully covered\n","177.3\n","\n","episode  2297  is  fully covered\n","209.3\n","\n","episode  2298  is  fully covered\n","141.8\n","\n","episode  2299  is  fully covered\n","147.8\n","\n","episode  2300  is  fully covered\n","183.3\n","\n","episode  2301  is  fully covered\n","141.8\n","\n","episode  2302  is  fully covered\n","161.3\n","\n","episode  2303  is  fully covered\n","186.8\n","\n","episode  2304  is  fully covered\n","141.8\n","\n","episode  2305  is  fully covered\n","197.3\n","\n","episode  2306  is  fully covered\n","147.3\n","\n","episode  2307  is  fully covered\n","183.8\n","\n","episode  2308  is  fully covered\n","181.3\n","\n","episode  2309  is  fully covered\n","179.8\n","\n","episode  2310  is  fully covered\n","177.3\n","\n","episode  2311  is  fully covered\n","190.3\n","\n","episode  2312  is  fully covered\n","190.3\n","\n","episode  2313  is  fully covered\n","147.3\n","\n","episode  2314  is  fully covered\n","144.8\n","\n","episode  2315  is  fully covered\n","215.8\n","\n","episode  2316  is  fully covered\n","141.8\n","\n","episode  2317  is  fully covered\n","175.8\n","\n","episode  2318  is  fully covered\n","209.3\n","\n","episode  2319  is  fully covered\n","141.8\n","\n","episode  2320  is  fully covered\n","174.3\n","\n","episode  2321  is  fully covered\n","187.3\n","\n","episode  2322  is  fully covered\n","185.8\n","\n","episode  2323  is  fully covered\n","141.8\n","\n","episode  2324  is  fully covered\n","202.8\n","\n","episode  2325  is  fully covered\n","162.3\n","\n","episode  2326  is  fully covered\n","194.8\n","\n","episode  2327  is  fully covered\n","188.8\n","\n","episode  2328  is  fully covered\n","195.8\n","\n","episode  2329  is  fully covered\n","141.8\n","\n","episode  2330  is  fully covered\n","147.8\n","\n","episode  2331  is  fully covered\n","197.8\n","\n","episode  2332  is  fully covered\n","209.3\n","\n","episode  2333  is  fully covered\n","154.8\n","\n","episode  2334  is  fully covered\n","141.8\n","\n","episode  2335  is  fully covered\n","141.8\n","\n","episode  2336  is  fully covered\n","148.3\n","\n","episode  2337  is  fully covered\n","141.8\n","\n","episode  2338  is  fully covered\n","164.3\n","\n","episode  2339  is  fully covered\n","236.8\n","\n","episode  2340  is  fully covered\n","141.8\n","\n","episode  2341  is  fully covered\n","166.3\n","\n","episode  2342  is  fully covered\n","141.8\n","\n","episode  2343  is  fully covered\n","169.3\n","\n","episode  2344  is  fully covered\n","148.3\n","\n","episode  2345  is  fully covered\n","185.3\n","\n","episode  2346  is  fully covered\n","147.3\n","\n","episode  2347  is  fully covered\n","196.3\n","\n","episode  2348  is  fully covered\n","141.8\n","\n","episode  2349  is  fully covered\n","164.3\n","\n","episode  2350  is  fully covered\n","183.8\n","\n","episode  2351  is  fully covered\n","157.8\n","\n","episode  2352  is  fully covered\n","155.8\n","\n","episode  2353  is  fully covered\n","245.8\n","\n","episode  2354  is  fully covered\n","177.8\n","\n","episode  2355  is  fully covered\n","206.8\n","\n","episode  2356  is  fully covered\n","166.8\n","\n","episode  2357  is  fully covered\n","168.3\n","\n","episode  2358  is  fully covered\n","179.8\n","\n","episode  2359  is  fully covered\n","156.8\n","\n","episode  2360  is  fully covered\n","193.3\n","\n","episode  2361  is  fully covered\n","141.8\n","\n","episode  2362  is  fully covered\n","224.3\n","\n","episode  2363  is  fully covered\n","206.8\n","\n","episode  2364  is  fully covered\n","168.3\n","\n","episode  2365  is  fully covered\n","141.8\n","\n","episode  2366  is  fully covered\n","141.8\n","\n","episode  2367  is  fully covered\n","156.3\n","\n","episode  2368  is  fully covered\n","187.3\n","\n","episode  2369  is  fully covered\n","141.8\n","\n","episode  2370  is  fully covered\n","151.8\n","\n","episode  2371  is  fully covered\n","157.8\n","\n","episode  2372  is  fully covered\n","141.8\n","\n","episode  2373  is  fully covered\n","149.3\n","\n","episode  2374  is  fully covered\n","159.8\n","\n","episode  2375  is  fully covered\n","141.8\n","\n","episode  2376  is  fully covered\n","141.8\n","\n","episode  2377  is  fully covered\n","202.8\n","\n","episode  2378  is  fully covered\n","190.3\n","\n","episode  2379  is  fully covered\n","157.8\n","\n","episode  2380  is  fully covered\n","181.3\n","\n","episode  2381  is  fully covered\n","215.8\n","\n","episode  2382  is  fully covered\n","141.8\n","\n","episode  2383  is  fully covered\n","157.8\n","\n","episode  2384  is  fully covered\n","157.8\n","\n","episode  2385  is  fully covered\n","170.3\n","\n","episode  2386  is  fully covered\n","153.8\n","\n","episode  2387  is  fully covered\n","165.3\n","\n","episode  2388  is  fully covered\n","141.8\n","\n","episode  2389  is  fully covered\n","221.8\n","\n","episode  2390  is  fully covered\n","153.3\n","\n","episode  2391  is  fully covered\n","166.3\n","\n","episode  2392  is  fully covered\n","156.3\n","\n","episode  2393  is  fully covered\n","156.8\n","\n","episode  2394  is  fully covered\n","227.8\n","\n","episode  2395  is  fully covered\n","141.8\n","\n","episode  2396  is  fully covered\n","156.8\n","\n","episode  2397  is  fully covered\n","141.8\n","\n","episode  2398  is  fully covered\n","141.8\n","\n","episode  2399  is  fully covered\n","154.8\n","\n","episode  2400  is  fully covered\n","141.8\n","\n","episode  2401  is  fully covered\n","254.8\n","\n","episode  2402  is  fully covered\n","206.3\n","\n","episode  2403  is  fully covered\n","141.8\n","\n","episode  2404  is  fully covered\n","160.3\n","\n","episode  2405  is  fully covered\n","172.8\n","\n","episode  2406  is  fully covered\n","209.3\n","\n","episode  2407  is  fully covered\n","141.8\n","\n","episode  2408  is  fully covered\n","158.3\n","\n","episode  2409  is  fully covered\n","148.3\n","\n","episode  2410  is  fully covered\n","156.3\n","\n","episode  2411  is  fully covered\n","141.8\n","\n","episode  2412  is  fully covered\n","185.8\n","\n","episode  2413  is  fully covered\n","162.3\n","\n","episode  2414  is  fully covered\n","163.8\n","\n","episode  2415  is  fully covered\n","185.8\n","\n","episode  2416  is  fully covered\n","209.3\n","\n","episode  2417  is  fully covered\n","185.3\n","\n","episode  2418  is  fully covered\n","160.8\n","\n","episode  2419  is  fully covered\n","189.8\n","\n","episode  2420  is  fully covered\n","147.3\n","\n","episode  2421  is  fully covered\n","141.8\n","\n","episode  2422  is  fully covered\n","141.8\n","\n","episode  2423  is  fully covered\n","169.3\n","\n","episode  2424  is  fully covered\n","164.3\n","\n","episode  2425  is  fully covered\n","165.3\n","\n","episode  2426  is  fully covered\n","156.3\n","\n","episode  2427  is  fully covered\n","160.8\n","\n","episode  2428  is  fully covered\n","141.8\n","\n","episode  2429  is  fully covered\n","141.8\n","\n","episode  2430  is  fully covered\n","195.3\n","\n","episode  2431  is  fully covered\n","142.3\n","\n","episode  2432  is  fully covered\n","141.8\n","\n","episode  2433  is  fully covered\n","227.3\n","\n","episode  2434  is  fully covered\n","150.8\n","\n","episode  2435  is  fully covered\n","149.3\n","\n","episode  2436  is  fully covered\n","141.8\n","\n","episode  2437  is  fully covered\n","195.3\n","\n","episode  2438  is  fully covered\n","155.8\n","\n","episode  2439  is  fully covered\n","150.3\n","\n","episode  2440  is  fully covered\n","208.3\n","\n","episode  2441  is  fully covered\n","190.3\n","\n","episode  2442  is  fully covered\n","198.8\n","\n","episode  2443  is  fully covered\n","141.8\n","\n","episode  2444  not  fully covered\n","393.8\n","\n","episode  2445  is  fully covered\n","163.8\n","\n","episode  2446  not  fully covered\n","393.8\n","\n","episode  2447  is  fully covered\n","161.3\n","\n","episode  2448  is  fully covered\n","150.8\n","\n","episode  2449  is  fully covered\n","141.8\n","\n","episode  2450  is  fully covered\n","164.3\n","\n","episode  2451  is  fully covered\n","156.3\n","\n","episode  2452  is  fully covered\n","167.8\n","\n","episode  2453  is  fully covered\n","180.8\n","\n","episode  2454  is  fully covered\n","161.3\n","\n","episode  2455  is  fully covered\n","151.8\n","\n","episode  2456  is  fully covered\n","141.8\n","\n","episode  2457  is  fully covered\n","141.8\n","\n","episode  2458  is  fully covered\n","158.3\n","\n","episode  2459  is  fully covered\n","156.3\n","\n","episode  2460  is  fully covered\n","154.8\n","\n","episode  2461  is  fully covered\n","141.8\n","\n","episode  2462  is  fully covered\n","141.8\n","\n","episode  2463  is  fully covered\n","154.8\n","\n","episode  2464  is  fully covered\n","164.3\n","\n","episode  2465  is  fully covered\n","141.8\n","\n","episode  2466  is  fully covered\n","248.3\n","\n","episode  2467  is  fully covered\n","142.3\n","\n","episode  2468  is  fully covered\n","141.8\n","\n","episode  2469  is  fully covered\n","141.8\n","\n","episode  2470  is  fully covered\n","192.8\n","\n","episode  2471  is  fully covered\n","163.8\n","\n","episode  2472  is  fully covered\n","154.8\n","\n","episode  2473  is  fully covered\n","199.8\n","\n","episode  2474  is  fully covered\n","176.3\n","\n","episode  2475  is  fully covered\n","141.8\n","\n","episode  2476  is  fully covered\n","141.8\n","\n","episode  2477  is  fully covered\n","158.3\n","\n","episode  2478  is  fully covered\n","141.8\n","\n","episode  2479  is  fully covered\n","151.3\n","\n","episode  2480  is  fully covered\n","142.3\n","\n","episode  2481  is  fully covered\n","148.3\n","\n","episode  2482  is  fully covered\n","150.3\n","\n","episode  2483  is  fully covered\n","151.3\n","\n","episode  2484  is  fully covered\n","156.3\n","\n","episode  2485  is  fully covered\n","180.8\n","\n","episode  2486  is  fully covered\n","187.3\n","\n","episode  2487  is  fully covered\n","175.3\n","\n","episode  2488  is  fully covered\n","155.8\n","\n","episode  2489  is  fully covered\n","141.8\n","\n","episode  2490  is  fully covered\n","190.3\n","\n","episode  2491  is  fully covered\n","206.3\n","\n","episode  2492  is  fully covered\n","186.8\n","\n","episode  2493  is  fully covered\n","141.8\n","\n","episode  2494  is  fully covered\n","141.8\n","\n","episode  2495  is  fully covered\n","179.8\n","\n","episode  2496  is  fully covered\n","175.8\n","\n","episode  2497  is  fully covered\n","141.8\n","\n","episode  2498  is  fully covered\n","141.8\n","\n","episode  2499  is  fully covered\n","184.3\n","\n","episode  2500  is  fully covered\n","141.8\n","\n","episode  2501  is  fully covered\n","142.3\n","\n","episode  2502  is  fully covered\n","141.8\n","\n","episode  2503  is  fully covered\n","141.8\n","\n","episode  2504  is  fully covered\n","167.8\n","\n","episode  2505  is  fully covered\n","207.8\n","\n","episode  2506  is  fully covered\n","159.3\n","\n","episode  2507  is  fully covered\n","158.3\n","\n","episode  2508  is  fully covered\n","141.8\n","\n","episode  2509  not  fully covered\n","393.8\n","\n","episode  2510  is  fully covered\n","141.8\n","\n","episode  2511  is  fully covered\n","168.3\n","\n","episode  2512  is  fully covered\n","141.8\n","\n","episode  2513  is  fully covered\n","157.3\n","\n","episode  2514  is  fully covered\n","154.3\n","\n","episode  2515  is  fully covered\n","150.8\n","\n","episode  2516  is  fully covered\n","141.8\n","\n","episode  2517  is  fully covered\n","154.8\n","\n","episode  2518  is  fully covered\n","187.3\n","\n","episode  2519  is  fully covered\n","141.8\n","\n","episode  2520  is  fully covered\n","142.3\n","\n","episode  2521  is  fully covered\n","141.8\n","\n","episode  2522  is  fully covered\n","141.8\n","\n","episode  2523  is  fully covered\n","141.8\n","\n","episode  2524  is  fully covered\n","141.8\n","\n","episode  2525  is  fully covered\n","141.8\n","\n","episode  2526  is  fully covered\n","141.8\n","\n","episode  2527  is  fully covered\n","192.8\n","\n","episode  2528  is  fully covered\n","148.3\n","\n","episode  2529  is  fully covered\n","154.8\n","\n","episode  2530  is  fully covered\n","168.8\n","\n","episode  2531  is  fully covered\n","150.8\n","\n","episode  2532  is  fully covered\n","141.8\n","\n","episode  2533  is  fully covered\n","154.3\n","\n","episode  2534  is  fully covered\n","155.8\n","\n","episode  2535  is  fully covered\n","165.3\n","\n","episode  2536  is  fully covered\n","141.8\n","\n","episode  2537  is  fully covered\n","141.8\n","\n","episode  2538  is  fully covered\n","168.8\n","\n","episode  2539  is  fully covered\n","141.8\n","\n","episode  2540  is  fully covered\n","209.3\n","\n","episode  2541  is  fully covered\n","141.8\n","\n","episode  2542  is  fully covered\n","141.8\n","\n","episode  2543  is  fully covered\n","181.3\n","\n","episode  2544  is  fully covered\n","150.3\n","\n","episode  2545  is  fully covered\n","156.3\n","\n","episode  2546  is  fully covered\n","141.8\n","\n","episode  2547  is  fully covered\n","174.3\n","\n","episode  2548  is  fully covered\n","183.3\n","\n","episode  2549  is  fully covered\n","141.8\n","\n","episode  2550  is  fully covered\n","159.3\n","\n","episode  2551  is  fully covered\n","148.3\n","\n","episode  2552  is  fully covered\n","154.8\n","\n","episode  2553  is  fully covered\n","191.3\n","\n","episode  2554  is  fully covered\n","196.8\n","\n","episode  2555  not  fully covered\n","393.8\n","\n","episode  2556  is  fully covered\n","165.3\n","\n","episode  2557  is  fully covered\n","236.8\n","\n","episode  2558  is  fully covered\n","157.8\n","\n","episode  2559  is  fully covered\n","141.8\n","\n","episode  2560  is  fully covered\n","154.8\n","\n","episode  2561  is  fully covered\n","154.8\n","\n","episode  2562  is  fully covered\n","161.8\n","\n","episode  2563  is  fully covered\n","141.8\n","\n","episode  2564  is  fully covered\n","141.8\n","\n","episode  2565  is  fully covered\n","148.3\n","\n","episode  2566  is  fully covered\n","147.3\n","\n","episode  2567  is  fully covered\n","167.8\n","\n","episode  2568  is  fully covered\n","150.3\n","\n","episode  2569  is  fully covered\n","188.8\n","\n","episode  2570  is  fully covered\n","141.8\n","\n","episode  2571  is  fully covered\n","141.8\n","\n","episode  2572  is  fully covered\n","178.8\n","\n","episode  2573  is  fully covered\n","154.8\n","\n","episode  2574  is  fully covered\n","209.3\n","\n","episode  2575  is  fully covered\n","156.3\n","\n","episode  2576  is  fully covered\n","158.3\n","\n","episode  2577  is  fully covered\n","161.3\n","\n","episode  2578  is  fully covered\n","141.8\n","\n","episode  2579  is  fully covered\n","164.3\n","\n","episode  2580  is  fully covered\n","150.8\n","\n","episode  2581  is  fully covered\n","186.8\n","\n","episode  2582  is  fully covered\n","141.8\n","\n","episode  2583  is  fully covered\n","157.8\n","\n","episode  2584  is  fully covered\n","209.3\n","\n","episode  2585  is  fully covered\n","183.3\n","\n","episode  2586  is  fully covered\n","157.8\n","\n","episode  2587  is  fully covered\n","141.8\n","\n","episode  2588  is  fully covered\n","141.8\n","\n","episode  2589  is  fully covered\n","196.3\n","\n","episode  2590  is  fully covered\n","221.3\n","\n","episode  2591  is  fully covered\n","180.3\n","\n","episode  2592  is  fully covered\n","191.8\n","\n","episode  2593  is  fully covered\n","184.3\n","\n","episode  2594  not  fully covered\n","393.8\n","\n","episode  2595  is  fully covered\n","148.3\n","\n","episode  2596  is  fully covered\n","162.3\n","\n","episode  2597  is  fully covered\n","192.8\n","\n","episode  2598  is  fully covered\n","142.3\n","\n","episode  2599  is  fully covered\n","175.8\n","\n","episode  2600  is  fully covered\n","157.8\n","\n","episode  2601  is  fully covered\n","141.8\n","\n","episode  2602  is  fully covered\n","141.8\n","\n","episode  2603  is  fully covered\n","141.8\n","\n","episode  2604  is  fully covered\n","221.3\n","\n","episode  2605  is  fully covered\n","141.8\n","\n","episode  2606  is  fully covered\n","220.3\n","\n","episode  2607  is  fully covered\n","141.8\n","\n","episode  2608  is  fully covered\n","141.8\n","\n","episode  2609  is  fully covered\n","162.3\n","\n","episode  2610  is  fully covered\n","157.8\n","\n","episode  2611  is  fully covered\n","221.3\n","\n","episode  2612  is  fully covered\n","177.3\n","\n","episode  2613  is  fully covered\n","141.8\n","\n","episode  2614  is  fully covered\n","155.8\n","\n","episode  2615  is  fully covered\n","156.3\n","\n","episode  2616  is  fully covered\n","178.3\n","\n","episode  2617  is  fully covered\n","178.8\n","\n","episode  2618  is  fully covered\n","147.8\n","\n","episode  2619  is  fully covered\n","147.8\n","\n","episode  2620  is  fully covered\n","141.8\n","\n","episode  2621  is  fully covered\n","164.3\n","\n","episode  2622  is  fully covered\n","149.3\n","\n","episode  2623  is  fully covered\n","142.3\n","\n","episode  2624  is  fully covered\n","154.8\n","\n","episode  2625  is  fully covered\n","149.3\n","\n","episode  2626  is  fully covered\n","209.3\n","\n","episode  2627  is  fully covered\n","183.8\n","\n","episode  2628  is  fully covered\n","178.8\n","\n","episode  2629  is  fully covered\n","180.8\n","\n","episode  2630  is  fully covered\n","149.3\n","\n","episode  2631  is  fully covered\n","141.8\n","\n","episode  2632  is  fully covered\n","141.8\n","\n","episode  2633  is  fully covered\n","141.8\n","\n","episode  2634  is  fully covered\n","141.8\n","\n","episode  2635  is  fully covered\n","237.3\n","\n","episode  2636  is  fully covered\n","155.3\n","\n","episode  2637  is  fully covered\n","141.8\n","\n","episode  2638  is  fully covered\n","161.3\n","\n","episode  2639  is  fully covered\n","141.8\n","\n","episode  2640  is  fully covered\n","200.8\n","\n","episode  2641  is  fully covered\n","168.8\n","\n","episode  2642  is  fully covered\n","177.8\n","\n","episode  2643  is  fully covered\n","141.8\n","\n","episode  2644  is  fully covered\n","189.3\n","\n","episode  2645  is  fully covered\n","141.8\n","\n","episode  2646  is  fully covered\n","182.3\n","\n","episode  2647  is  fully covered\n","157.8\n","\n","episode  2648  is  fully covered\n","173.8\n","\n","episode  2649  is  fully covered\n","155.3\n","\n","episode  2650  is  fully covered\n","177.3\n","\n","episode  2651  is  fully covered\n","183.3\n","\n","episode  2652  is  fully covered\n","141.8\n","\n","episode  2653  is  fully covered\n","147.8\n","\n","episode  2654  is  fully covered\n","155.8\n","\n","episode  2655  is  fully covered\n","252.8\n","\n","episode  2656  is  fully covered\n","183.3\n","\n","episode  2657  is  fully covered\n","141.8\n","\n","episode  2658  is  fully covered\n","149.3\n","\n","episode  2659  is  fully covered\n","165.3\n","\n","episode  2660  is  fully covered\n","141.8\n","\n","episode  2661  is  fully covered\n","164.3\n","\n","episode  2662  is  fully covered\n","141.8\n","\n","episode  2663  is  fully covered\n","157.8\n","\n","episode  2664  is  fully covered\n","141.8\n","\n","episode  2665  is  fully covered\n","141.8\n","\n","episode  2666  is  fully covered\n","174.8\n","\n","episode  2667  is  fully covered\n","141.8\n","\n","episode  2668  is  fully covered\n","209.3\n","\n","episode  2669  is  fully covered\n","201.8\n","\n","episode  2670  is  fully covered\n","185.3\n","\n","episode  2671  is  fully covered\n","141.8\n","\n","episode  2672  is  fully covered\n","141.8\n","\n","episode  2673  is  fully covered\n","141.8\n","\n","episode  2674  is  fully covered\n","141.8\n","\n","episode  2675  is  fully covered\n","148.3\n","\n","episode  2676  is  fully covered\n","141.8\n","\n","episode  2677  is  fully covered\n","141.8\n","\n","episode  2678  is  fully covered\n","184.8\n","\n","episode  2679  is  fully covered\n","148.8\n","\n","episode  2680  is  fully covered\n","141.8\n","\n","episode  2681  is  fully covered\n","141.8\n","\n","episode  2682  is  fully covered\n","141.8\n","\n","episode  2683  is  fully covered\n","141.8\n","\n","episode  2684  is  fully covered\n","141.8\n","\n","episode  2685  is  fully covered\n","141.8\n","\n","episode  2686  is  fully covered\n","141.8\n","\n","episode  2687  is  fully covered\n","141.8\n","\n","episode  2688  is  fully covered\n","141.8\n","\n","episode  2689  is  fully covered\n","141.8\n","\n","episode  2690  is  fully covered\n","141.8\n","\n","episode  2691  is  fully covered\n","141.8\n","episode  2691\n","141.8\n","\n","chosen actions  [1 1 1]\n","[[1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 2. 1. 1.]\n"," [1. 1. 1. 2. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]]\n","\n","mean T =  182.37064601435586\n","min T =  141.8\n","diag_count =  2\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J4QecD3zIGLi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600187090667,"user_tz":-60,"elapsed":1442,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"8ec72ed9-c91a-4e6d-c1f2-b113fa6a4380"},"source":["# print(np.mean(c_Ts)) # r_c = 20\n","# print(np.var(c_Ts))\n","filename = 'rc_20_first_convergence'\n","with open(filename) as f:\n","    content = f.readlines()\n","# you may also want to remove whitespace characters like `\\n` at the end of each line\n","# content = [[float(strn) for strn in x.strip().split(\",\")] for x in content]\n","# content = [[float(strn) for strn in x.strip().split(\" \")] for x in content] \n","content = [float(x.strip()) for x in content] \n","# print(content)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[188.3, 231.3, 194.3, 194.8, 203.8, 230.3, 207.3, 214.3, 194.3, 193.8, 221.8, 192.3, 195.8, 212.8, 204.3, 196.8, 179.8, 179.3, 191.8, 174.3, 169.3, 183.8, 195.8, 164.8, 173.8, 193.8, 173.3, 164.3, 173.8, 168.8, 185.8, 183.8, 172.8, 161.3, 233.8, 192.3, 170.8, 207.3, 191.3, 172.3, 151.8, 164.3, 157.3, 182.8, 177.3, 190.3, 187.3, 170.3, 198.3, 177.3, 169.8, 183.3, 164.8, 161.3, 164.8, 180.8, 181.8, 164.8, 180.3, 174.8, 159.3, 204.8, 211.3, 176.3, 187.8, 184.3, 191.3, 174.3, 188.3, 193.3, 206.3, 198.3, 201.8, 215.8, 187.8, 208.8, 204.3, 193.3, 198.8, 190.3, 199.8, 185.8, 199.3, 213.3, 161.3, 201.3, 185.3, 185.3, 160.3, 160.3, 174.8, 171.8, 160.3, 181.8, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3, 160.3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_3HTd21o5Suu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596721461226,"user_tz":-60,"elapsed":290344,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"96db2911-efb9-4a35-995f-a9db9f97bec6"},"source":["action_n_ = [] \n","for i in range(env.num_agents):\n","    action_n_.append(sorted(T_memory[i].items())[0][1][0][1])\n","Ts, final_rs = train_initial_action_n(1.0, explore_stop, decay_rate, decay_step, action_n_, env, obs_prev_obs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["chosen actions  [1, 1, 1]\n","Explore P  0.9970762824113469\n","episode  0  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9940100119420313\n","episode  1  is  fully covered\n","191.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9911038073868191\n","episode  2  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9873842542131398\n","episode  3  is  fully covered\n","232.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9839541288882252\n","episode  4  is  fully covered\n","217.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9806771269925333\n","episode  5  is  fully covered\n","208.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9774579558165845\n","episode  6  is  fully covered\n","204.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9746625212042854\n","episode  7  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9714786371098341\n","episode  8  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9672212589816311\n","episode  9  is  fully covered\n","274.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9645399761091946\n","episode  10  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9580493392393024\n","episode  11  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9550496025569736\n","episode  12  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9516556707549222\n","episode  13  is  fully covered\n","222.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9488505256461078\n","episode  14  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.943038660349266\n","episode  15  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9404018453639863\n","episode  16  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9377123876207075\n","episode  17  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9352924666954265\n","episode  18  is  fully covered\n","160.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9324012790863442\n","episode  19  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.929786768785054\n","episode  20  is  fully covered\n","175.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9267717204491426\n","episode  21  is  fully covered\n","203.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9235004426732211\n","episode  22  is  fully covered\n","220.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.919902125430509\n","episode  23  is  fully covered\n","243.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9175208266178766\n","episode  24  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9145089723418021\n","episode  25  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9111935002961405\n","episode  26  is  fully covered\n","226.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9085148914781042\n","episode  27  is  fully covered\n","183.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.9056630061721753\n","episode  28  is  fully covered\n","196.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9028634095169165\n","episode  29  is  fully covered\n","192.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.9007135468032036\n","episode  30  is  fully covered\n","148.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.89803700795314\n","episode  31  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8949888669022612\n","episode  32  is  fully covered\n","212.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8922936469144596\n","episode  33  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8898058376501014\n","episode  34  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8870410660991631\n","episode  35  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8843768555538256\n","episode  36  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8814314899650553\n","episode  37  is  fully covered\n","214.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8783694394381145\n","episode  38  is  fully covered\n","210.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8744501413871533\n","episode  39  is  fully covered\n","279.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8712520186079137\n","episode  40  is  fully covered\n","228.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8686352301588407\n","episode  41  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.866123301560501\n","episode  42  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8635979103854343\n","episode  43  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8613141281808442\n","episode  44  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8590295131681693\n","episode  45  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8567166886889467\n","episode  46  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8541367236987343\n","episode  47  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8485926564289011\n","episode  48  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8463282434539048\n","episode  49  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8438808224370002\n","episode  50  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8411174277841127\n","episode  51  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8382021319570065\n","episode  52  is  fully covered\n","216.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8357514656409574\n","episode  53  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8329213991757106\n","episode  54  is  fully covered\n","207.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8300942752526446\n","episode  55  is  fully covered\n","212.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8266945493562922\n","episode  56  is  fully covered\n","255.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8243896373289625\n","episode  57  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8222095412903557\n","episode  58  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8196482452211209\n","episode  59  is  fully covered\n","195.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8162847716776623\n","episode  60  is  fully covered\n","256.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8141391350550464\n","episode  61  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.81199913832267\n","episode  62  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8094307954556035\n","episode  63  is  fully covered\n","198.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.8066511372724243\n","episode  64  is  fully covered\n","214.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8043377588077891\n","episode  65  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.8020759298288533\n","episode  66  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7995517666689911\n","episode  67  is  fully covered\n","202.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.797112066231044\n","episode  68  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7948705561799269\n","episode  69  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7920331753316328\n","episode  70  is  fully covered\n","223.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7895153522029081\n","episode  71  is  fully covered\n","203.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7868733272499395\n","episode  72  is  fully covered\n","204.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7846857184712458\n","episode  73  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7796672912314276\n","episode  74  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7772882649930205\n","episode  75  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7747615301587855\n","episode  76  is  fully covered\n","203.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7726137746150078\n","episode  77  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7703733588716775\n","episode  78  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7682992294788836\n","episode  79  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7658017153471636\n","episode  80  is  fully covered\n","203.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7633856013972361\n","episode  81  is  fully covered\n","203.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.761056256051893\n","episode  82  is  fully covered\n","184.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7587947194694395\n","episode  83  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7568062521356389\n","episode  84  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7541952441488682\n","episode  85  is  fully covered\n","215.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7519601112422458\n","episode  86  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7496476371378281\n","episode  87  is  fully covered\n","192.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7472286871035142\n","episode  88  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.744614979673582\n","episode  89  is  fully covered\n","221.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7426933783442428\n","episode  90  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7405930461708904\n","episode  91  is  fully covered\n","179.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7382978094058836\n","episode  92  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7363100392299174\n","episode  93  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.733875414344318\n","episode  94  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7315600282716943\n","episode  95  is  fully covered\n","196.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7287503944851875\n","episode  96  is  fully covered\n","246.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7265616068854832\n","episode  97  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7245590616341417\n","episode  98  is  fully covered\n","172.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7225273536420722\n","episode  99  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7200346090516355\n","episode  100  is  fully covered\n","215.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7178834851874035\n","episode  101  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7156357289018405\n","episode  102  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7137832030032687\n","episode  103  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7087417955150871\n","episode  104  is  fully covered\n","211.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7064661431111333\n","episode  105  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.7034826958207036\n","episode  106  is  fully covered\n","263.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.7015774354504567\n","episode  107  is  fully covered\n","171.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6990954457028551\n","episode  108  is  fully covered\n","218.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6968451913450618\n","episode  109  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6949078723317147\n","episode  110  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6929038735635118\n","episode  111  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6908006443161014\n","episode  112  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6890068955247867\n","episode  113  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6868880190314124\n","episode  114  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.684551089053085\n","episode  115  is  fully covered\n","212.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.682675255685117\n","episode  116  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6806956425321142\n","episode  117  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6781139074937521\n","episode  118  is  fully covered\n","237.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6760609799371243\n","episode  119  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6733513635930969\n","episode  120  is  fully covered\n","251.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6714256442341968\n","episode  121  is  fully covered\n","172.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6694304518357922\n","episode  122  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6676601449201224\n","episode  123  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6655376957508643\n","episode  124  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6633848431212066\n","episode  125  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6614241272766016\n","episode  126  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6597436028302596\n","episode  127  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6578410119688762\n","episode  128  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.655959650705914\n","episode  129  is  fully covered\n","182.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.65414646499947\n","episode  130  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6519052824007682\n","episode  131  is  fully covered\n","214.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6500148956307694\n","episode  132  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6479640905391639\n","episode  133  is  fully covered\n","196.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6461006386171966\n","episode  134  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6439333834989946\n","episode  135  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6420096142095646\n","episode  136  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6398816766213664\n","episode  137  is  fully covered\n","207.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6375771434099993\n","episode  138  is  fully covered\n","225.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6356265968059813\n","episode  139  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6336262559661451\n","episode  140  is  fully covered\n","196.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6316119983669343\n","episode  141  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6288691956178571\n","episode  142  is  fully covered\n","277.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6267547269310606\n","episode  143  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6248922775629503\n","episode  144  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6229157512983055\n","episode  145  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6210001223672289\n","episode  146  is  fully covered\n","198.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.618832896457645\n","episode  147  is  fully covered\n","211.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6167423052114467\n","episode  148  is  fully covered\n","217.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6144866962042955\n","episode  149  is  fully covered\n","222.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.612797953591984\n","episode  150  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6110014174058851\n","episode  151  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6091321742059218\n","episode  152  is  fully covered\n","188.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6070840679901081\n","episode  153  is  fully covered\n","216.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6051493451055262\n","episode  154  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.6032159622635554\n","episode  155  is  fully covered\n","193.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.6014378944622465\n","episode  156  is  fully covered\n","184.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5994732056552812\n","episode  157  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.597600983150534\n","episode  158  is  fully covered\n","195.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5955773546299238\n","episode  159  is  fully covered\n","211.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5936603051828178\n","episode  160  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5914039455759694\n","episode  161  is  fully covered\n","237.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5893352925473564\n","episode  162  is  fully covered\n","219.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5877720948558284\n","episode  163  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5861614590305079\n","episode  164  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5845225025550729\n","episode  165  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5826969727841158\n","episode  166  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5807238130232623\n","episode  167  is  fully covered\n","211.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5789610933231447\n","episode  168  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5771621669819249\n","episode  169  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5755713956752794\n","episode  170  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5735260044632382\n","episode  171  is  fully covered\n","222.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5718263028868565\n","episode  172  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5703232349627454\n","episode  173  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5684738301548528\n","episode  174  is  fully covered\n","202.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5669024703582327\n","episode  175  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5651771823032805\n","episode  176  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5637592389038633\n","episode  177  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5617648123724902\n","episode  178  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5583373105741853\n","episode  179  is  fully covered\n","208.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5563798692184161\n","episode  180  is  fully covered\n","214.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5548907674565486\n","episode  181  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5528790596619418\n","episode  182  is  fully covered\n","233.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5512449573099358\n","episode  183  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5487809035029257\n","episode  184  is  fully covered\n","279.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5473384113173517\n","episode  185  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5458429401614155\n","episode  186  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5439815217904843\n","episode  187  is  fully covered\n","206.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5421568111079025\n","episode  188  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5390300274878014\n","episode  189  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5370027840693642\n","episode  190  is  fully covered\n","235.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5357069513393112\n","episode  191  is  fully covered\n","151.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5342860015332178\n","episode  192  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.532510853205333\n","episode  193  is  fully covered\n","208.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5309624371887001\n","episode  194  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5294524074705724\n","episode  195  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5278242027779395\n","episode  196  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5260536895844693\n","episode  197  is  fully covered\n","210.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5243855934052518\n","episode  198  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5229110007750881\n","episode  199  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5213988411807862\n","episode  200  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5198993727891019\n","episode  201  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.518333718474276\n","episode  202  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5168513344916993\n","episode  203  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5153649440779519\n","episode  204  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5138458300784653\n","episode  205  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5124336703770881\n","episode  206  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5108659765444757\n","episode  207  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.50933160054546\n","episode  208  is  fully covered\n","193.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5078993403271793\n","episode  209  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5063171638641022\n","episode  210  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.504772220487183\n","episode  211  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.5034252694420267\n","episode  212  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.5018771050092471\n","episode  213  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.500525866621935\n","episode  214  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.49903452362629286\n","episode  215  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.49759141028258647\n","episode  216  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4960413444426284\n","episode  217  is  fully covered\n","194.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4944209495752749\n","episode  218  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.49271518004109766\n","episode  219  is  fully covered\n","215.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4912588994667049\n","episode  220  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.489622790325472\n","episode  221  is  fully covered\n","208.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4883553313893443\n","episode  222  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.48697036982449937\n","episode  223  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.48567092183197275\n","episode  224  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4843710663458212\n","episode  225  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4830785544214758\n","episode  226  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.48156984555406274\n","episode  227  is  fully covered\n","195.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.47984698843174284\n","episode  228  is  fully covered\n","223.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4784402238147021\n","episode  229  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4768696957218913\n","episode  230  is  fully covered\n","204.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4753309408326824\n","episode  231  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.474017044060494\n","episode  232  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.47260846634061493\n","episode  233  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4711965351117503\n","episode  234  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4697549984900458\n","episode  235  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4684265343274578\n","episode  236  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4670420418455856\n","episode  237  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.465654190875597\n","episode  238  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4640810802979716\n","episode  239  is  fully covered\n","211.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4625798908379717\n","episode  240  is  fully covered\n","202.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4612126788331054\n","episode  241  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4597759377567095\n","episode  242  is  fully covered\n","192.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4585453879168319\n","episode  243  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.45729252243701524\n","episode  244  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.45587528713446024\n","episode  245  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.45464426544942993\n","episode  246  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.45330413458417246\n","episode  247  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.45157762228716347\n","episode  248  is  fully covered\n","237.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4503618086706521\n","episode  249  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.448879859742717\n","episode  250  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.44760326863079325\n","episode  251  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.44600906591722556\n","episode  252  is  fully covered\n","222.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.44472996554725164\n","episode  253  is  fully covered\n","179.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4434829154816886\n","episode  254  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.44036471398212307\n","episode  255  is  fully covered\n","222.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4390666741957984\n","episode  256  is  fully covered\n","184.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4377059242156117\n","episode  257  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4365169817964578\n","episode  258  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.43516413379126895\n","episode  259  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.43385712680082855\n","episode  260  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4327132545617908\n","episode  261  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4314239627901835\n","episode  262  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4300662553305055\n","episode  263  is  fully covered\n","196.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.429055928108534\n","episode  264  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4276577725902869\n","episode  265  is  fully covered\n","204.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4263426130360996\n","episode  266  is  fully covered\n","192.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4251607271588939\n","episode  267  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4236091775013857\n","episode  268  is  fully covered\n","228.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4223605271241182\n","episode  269  is  fully covered\n","184.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4212941481824471\n","episode  270  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4197600669952464\n","episode  271  is  fully covered\n","227.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4182416096190935\n","episode  272  is  fully covered\n","223.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.41674198070328317\n","episode  273  is  fully covered\n","223.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4154936282261532\n","episode  274  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.41422250410587336\n","episode  275  is  fully covered\n","191.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4127372859199012\n","episode  276  is  fully covered\n","224.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.41126726336350466\n","episode  277  is  fully covered\n","222.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4098549345757933\n","episode  278  is  fully covered\n","214.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.40869913699485694\n","episode  279  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4073868718237021\n","episode  280  is  fully covered\n","207.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4062282846240388\n","episode  281  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4049757865334633\n","episode  282  is  fully covered\n","192.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.4037239203871007\n","episode  283  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.4022634728289767\n","episode  284  is  fully covered\n","226.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.40095583295441045\n","episode  285  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.39991790117608617\n","episode  286  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.39889222953191694\n","episode  287  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3976846199532293\n","episode  288  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.39649652584543893\n","episode  289  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3952708708942996\n","episode  290  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.39424450148839013\n","episode  291  is  fully covered\n","162.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.392799490343048\n","episode  292  is  fully covered\n","228.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.39168865587043805\n","episode  293  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.39069034081248144\n","episode  294  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3895075616983846\n","episode  295  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3883563239881254\n","episode  296  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.38688646542069627\n","episode  297  is  fully covered\n","236.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.38572137368249293\n","episode  298  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.38464901878879393\n","episode  299  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3836072639079611\n","episode  300  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.38236332875904466\n","episode  301  is  fully covered\n","200.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3812637065882033\n","episode  302  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.38018549523904055\n","episode  303  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3790163253595863\n","episode  304  is  fully covered\n","192.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.37779634439267024\n","episode  305  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3765712524923381\n","episode  306  is  fully covered\n","202.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3753441276813457\n","episode  307  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.37402523895785694\n","episode  308  is  fully covered\n","220.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3730152408445572\n","episode  309  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.37026215396460416\n","episode  310  is  fully covered\n","238.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.36922391620428396\n","episode  311  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.36819742635843405\n","episode  312  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3669770378687841\n","episode  313  is  fully covered\n","206.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3657841037840842\n","episode  314  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.36479635959942347\n","episode  315  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3638694971295471\n","episode  316  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3626025311966534\n","episode  317  is  fully covered\n","222.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.36146719080301615\n","episode  318  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3602489351395015\n","episode  319  is  fully covered\n","211.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3589744725949265\n","episode  320  is  fully covered\n","220.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.35778465353843336\n","episode  321  is  fully covered\n","207.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.356812801598626\n","episode  322  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.35583504936061155\n","episode  323  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.35487984911220155\n","episode  324  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.35395552830396076\n","episode  325  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3527569450982783\n","episode  326  is  fully covered\n","205.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3518719316944069\n","episode  327  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.35066076652261247\n","episode  328  is  fully covered\n","215.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3497894069562914\n","episode  329  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.347168856236109\n","episode  330  is  fully covered\n","231.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.34612614134595804\n","episode  331  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.34529643465301846\n","episode  332  is  fully covered\n","149.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.34436401429416674\n","episode  333  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3433736727094056\n","episode  334  is  fully covered\n","179.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3422821096261031\n","episode  335  is  fully covered\n","198.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.34128137340190795\n","episode  336  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3404360442372947\n","episode  337  is  fully covered\n","154.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3395683590895643\n","episode  338  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3384049580193438\n","episode  339  is  fully covered\n","213.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33745065053428097\n","episode  340  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33649634223034003\n","episode  341  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33527103988421325\n","episode  342  is  fully covered\n","221.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33419721372634087\n","episode  343  is  fully covered\n","199.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33309751301223606\n","episode  344  is  fully covered\n","205.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3321262869543732\n","episode  345  is  fully covered\n","182.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3310334007783807\n","episode  346  is  fully covered\n","205.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.33017383186017435\n","episode  347  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3292532722208776\n","episode  348  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3284009528224112\n","episode  349  is  fully covered\n","161.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.32743032324252397\n","episode  350  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3263711657568957\n","episode  351  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3253909163245584\n","episode  352  is  fully covered\n","188.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.32429424880740976\n","episode  353  is  fully covered\n","210.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3233512775873251\n","episode  354  is  fully covered\n","181.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.32233110027921946\n","episode  355  is  fully covered\n","197.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.32151984851305554\n","episode  356  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.32048493769410985\n","episode  357  is  fully covered\n","200.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.31960417563004706\n","episode  358  is  fully covered\n","172.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.31868758928565766\n","episode  359  is  fully covered\n","179.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3177888851010855\n","episode  360  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3168927152769294\n","episode  361  is  fully covered\n","176.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3159131326068645\n","episode  362  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3148433706031136\n","episode  363  is  fully covered\n","205.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.31388518902006435\n","episode  364  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.31274472383788826\n","episode  365  is  fully covered\n","227.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3119950364853502\n","episode  366  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.31097088143895607\n","episode  367  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3098757091890358\n","episode  368  is  fully covered\n","219.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.30888816288769905\n","episode  369  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3078692805180712\n","episode  370  is  fully covered\n","206.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3070944266570832\n","episode  371  is  fully covered\n","160.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.30620146846479035\n","episode  372  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.30538194717833933\n","episode  373  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3044842248215455\n","episode  374  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.3035187169615816\n","episode  375  is  fully covered\n","198.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.30251754596599173\n","episode  376  is  fully covered\n","206.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.3014039160522684\n","episode  377  is  fully covered\n","237.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.30046740503246727\n","episode  378  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2995697501180174\n","episode  379  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.29880622279097113\n","episode  380  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.29804464150146337\n","episode  381  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.29728500128953805\n","episode  382  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2963755140915036\n","episode  383  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.29547117305735365\n","episode  384  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.29462851128089934\n","episode  385  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2936966050759916\n","episode  386  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.29279575352164394\n","episode  387  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.29182294889868127\n","episode  388  is  fully covered\n","201.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2908533763882854\n","episode  389  is  fully covered\n","213.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2898220978303509\n","episode  390  is  fully covered\n","215.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2887251735182873\n","episode  391  is  fully covered\n","236.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2879409081307703\n","episode  392  is  fully covered\n","170.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.28713809835921966\n","episode  393  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.286385635651946\n","episode  394  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.28565799654422863\n","episode  395  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2848820625422601\n","episode  396  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.284037786106636\n","episode  397  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.28318468415460646\n","episode  398  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2822957496449137\n","episode  399  is  fully covered\n","196.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.28147940379328035\n","episode  400  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.28064745664726054\n","episode  401  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2798403547567076\n","episode  402  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.27907129281502335\n","episode  403  is  fully covered\n","171.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2782553671677319\n","episode  404  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.27754394443740893\n","episode  405  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.27675019577119014\n","episode  406  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.27574686221683353\n","episode  407  is  fully covered\n","226.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2749494545075259\n","episode  408  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2740776002773917\n","episode  409  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.27328939235281346\n","episode  410  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.27257104043795477\n","episode  411  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2716176228753361\n","episode  412  is  fully covered\n","218.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2709036652722909\n","episode  413  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.26998847664152614\n","episode  414  is  fully covered\n","211.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.269248643776529\n","episode  415  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2684421082533857\n","episode  416  is  fully covered\n","187.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.26764012982350277\n","episode  417  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.26692808538195784\n","episode  418  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.26607102355348083\n","episode  419  is  fully covered\n","200.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2652315661602735\n","episode  420  is  fully covered\n","197.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.26441167907155405\n","episode  421  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2636575966583826\n","episode  422  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.26272274615244473\n","episode  423  is  fully covered\n","221.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2620133034456524\n","episode  424  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.26130577648068903\n","episode  425  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2605959905151813\n","episode  426  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.25988189530390465\n","episode  427  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.25882580787948783\n","episode  428  is  fully covered\n","254.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2581103686679435\n","episode  429  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2574319154137531\n","episode  430  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.256740867609458\n","episode  431  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2559738469686506\n","episode  432  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.25528058638193485\n","episode  433  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.25459531358499027\n","episode  434  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.25376363908042066\n","episode  435  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2531512861486055\n","episode  436  is  fully covered\n","150.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2524979876503664\n","episode  437  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2517436427340566\n","episode  438  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.250949388405781\n","episode  439  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24947214832040257\n","episode  440  is  fully covered\n","172.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.24871689431036323\n","episode  441  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24786079522176874\n","episode  442  is  fully covered\n","215.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.24721323875462128\n","episode  443  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24650820501215345\n","episode  444  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24561058134130195\n","episode  445  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24434649488938057\n","episode  446  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2437237176361949\n","episode  447  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.24305974540438005\n","episode  448  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2424033996322029\n","episode  449  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24178364055922894\n","episode  450  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24106130505944942\n","episode  451  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.24040073954872632\n","episode  452  is  fully covered\n","171.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2395521836646303\n","episode  453  is  fully covered\n","220.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.23885562239123473\n","episode  454  is  fully covered\n","181.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.23823540440578797\n","episode  455  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2374704700190006\n","episode  456  is  fully covered\n","200.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2368614281963921\n","episode  457  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.23616324428682242\n","episode  458  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.23535788697776897\n","episode  459  is  fully covered\n","207.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.23464536259726762\n","episode  460  is  fully covered\n","189.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.23397804331830174\n","episode  461  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2332584996110138\n","episode  462  is  fully covered\n","192.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.23258954228267248\n","episode  463  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.23194848021856695\n","episode  464  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2313350931258765\n","episode  465  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.23061629734240854\n","episode  466  is  fully covered\n","200.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2298703096916033\n","episode  467  is  fully covered\n","196.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.22908457968862775\n","episode  468  is  fully covered\n","213.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22842393784848014\n","episode  469  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2276923279855148\n","episode  470  is  fully covered\n","200.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.22704296642648364\n","episode  471  is  fully covered\n","181.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22632845365999046\n","episode  472  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.22571187103948348\n","episode  473  is  fully covered\n","170.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2250393507156492\n","episode  474  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.22445680402525656\n","episode  475  is  fully covered\n","161.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2238238321875852\n","episode  476  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22317836146857706\n","episode  477  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.22253831276432584\n","episode  478  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22189832444909838\n","episode  479  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22129204040817843\n","episode  480  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.22045272636129076\n","episode  481  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21935496863426363\n","episode  482  is  fully covered\n","156.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.21870488828381163\n","episode  483  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21814921017027072\n","episode  484  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21759842545217276\n","episode  485  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.21684249900649552\n","episode  486  is  fully covered\n","216.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21613587892106062\n","episode  487  is  fully covered\n","203.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2155643082072859\n","episode  488  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21504585382272168\n","episode  489  is  fully covered\n","149.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21449775647345\n","episode  490  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.21391169271255475\n","episode  491  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.21329480695579717\n","episode  492  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21263376634079117\n","episode  493  is  fully covered\n","193.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21184932248562555\n","episode  494  is  fully covered\n","230.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.21127387529143107\n","episode  495  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2105971946755289\n","episode  496  is  fully covered\n","200.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.21001338756229276\n","episode  497  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.2094546564580956\n","episode  498  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20883725804064518\n","episode  499  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.20828832109539536\n","episode  500  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20760127211242532\n","episode  501  is  fully covered\n","205.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.20701252092203565\n","episode  502  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20646838037005957\n","episode  503  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20590590219746688\n","episode  504  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2053630275255341\n","episode  505  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2047757092559193\n","episode  506  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.20419823842556933\n","episode  507  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.20368593617166691\n","episode  508  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20305792415037643\n","episode  509  is  fully covered\n","192.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20248853729263858\n","episode  510  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.20179155909697666\n","episode  511  is  fully covered\n","215.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20120640646221122\n","episode  512  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.2005924582747053\n","episode  513  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.20004598777319793\n","episode  514  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19950739014599214\n","episode  515  is  fully covered\n","175.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1988954438741912\n","episode  516  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19823620594102984\n","episode  517  is  fully covered\n","207.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1977293702802664\n","episode  518  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19720332024759346\n","episode  519  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.19672902593197478\n","episode  520  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.19627471381120284\n","episode  521  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.19576036408412842\n","episode  522  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.19526142055771467\n","episode  523  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1947232420649311\n","episode  524  is  fully covered\n","172.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.19408559614194254\n","episode  525  is  fully covered\n","204.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1935289819903418\n","episode  526  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19301256279376172\n","episode  527  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1924066842365092\n","episode  528  is  fully covered\n","196.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1917873639672483\n","episode  529  is  fully covered\n","200.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1912342810961943\n","episode  530  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19075450343511177\n","episode  531  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.19018461893318564\n","episode  532  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.18967105437578743\n","episode  533  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1891588766211501\n","episode  534  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1886013030009334\n","episode  535  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1880754625777742\n","episode  536  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.18747008368157195\n","episode  537  is  fully covered\n","201.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1869339374897327\n","episode  538  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18637695804688262\n","episode  539  is  fully covered\n","186.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1858097459491237\n","episode  540  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.18529020635532994\n","episode  541  is  fully covered\n","175.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18482682008035242\n","episode  542  is  fully covered\n","156.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18420684410778962\n","episode  543  is  fully covered\n","209.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18370942161960632\n","episode  544  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18311223648779734\n","episode  545  is  fully covered\n","202.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1826440686728737\n","episode  546  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18204160848227213\n","episode  547  is  fully covered\n","205.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.18161540266035614\n","episode  548  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18113367217216925\n","episode  549  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.18066767229818062\n","episode  550  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1802086378792051\n","episode  551  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17972920096141218\n","episode  552  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17916501971124657\n","episode  553  is  fully covered\n","196.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17862118509876043\n","episode  554  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1780932481230565\n","episode  555  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17752994146187653\n","episode  556  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17701372624647418\n","episode  557  is  fully covered\n","181.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17655550078478927\n","episode  558  is  fully covered\n","161.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17603648575546643\n","episode  559  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17544599574208344\n","episode  560  is  fully covered\n","210.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17495403442298565\n","episode  561  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17453744078184913\n","episode  562  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1738754570725127\n","episode  563  is  fully covered\n","236.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1734392300548686\n","episode  564  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17298195435703062\n","episode  565  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.17243619413655592\n","episode  566  is  fully covered\n","197.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17195955072999045\n","episode  567  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.17148559672736524\n","episode  568  is  fully covered\n","171.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1709650721116033\n","episode  569  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1703411650050998\n","episode  570  is  fully covered\n","228.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16985944126857727\n","episode  571  is  fully covered\n","176.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16928019133935776\n","episode  572  is  fully covered\n","213.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16876231063149785\n","episode  573  is  fully covered\n","190.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16830255444066422\n","episode  574  is  fully covered\n","170.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16785076465183196\n","episode  575  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1673934917696578\n","episode  576  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16692811639167754\n","episode  577  is  fully covered\n","173.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16646137141369005\n","episode  578  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16598796388021964\n","episode  579  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16553841438682376\n","episode  580  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1651085735446565\n","episode  581  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16466667497612963\n","episode  582  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16428771968112213\n","episode  583  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1638925907782574\n","episode  584  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16339772819308176\n","episode  585  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1629695345899998\n","episode  586  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16241378042837554\n","episode  587  is  fully covered\n","213.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.16192208762907903\n","episode  588  is  fully covered\n","189.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16141509534168194\n","episode  589  is  fully covered\n","199.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1609792184949551\n","episode  590  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16049828850326203\n","episode  591  is  fully covered\n","186.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.16002519618342032\n","episode  592  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1595943492970771\n","episode  593  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.159183763315079\n","episode  594  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1587323227762575\n","episode  595  is  fully covered\n","176.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1582821625097191\n","episode  596  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15790147759098683\n","episode  597  is  fully covered\n","149.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15744485641847422\n","episode  598  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.15707372466505024\n","episode  599  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1566834109896373\n","episode  600  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15629281686006483\n","episode  601  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15586952498396495\n","episode  602  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1554710093177015\n","episode  603  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1550375396649059\n","episode  604  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15465723467318335\n","episode  605  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1542531800811122\n","episode  606  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.15383295085912127\n","episode  607  is  fully covered\n","170.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.15339177645394764\n","episode  608  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15297756535619966\n","episode  609  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15256081126893942\n","episode  610  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15217805944858642\n","episode  611  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15173677541847969\n","episode  612  is  fully covered\n","180.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15135245847587298\n","episode  613  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1508858029710245\n","episode  614  is  fully covered\n","192.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.15046391364339864\n","episode  615  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.15006721278691446\n","episode  616  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14963803516520763\n","episode  617  is  fully covered\n","178.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1491778590518978\n","episode  618  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14876074527834973\n","episode  619  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14839583717851787\n","episode  620  is  fully covered\n","152.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14801998212108222\n","episode  621  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1476226386779735\n","episode  622  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14715453292295214\n","episode  623  is  fully covered\n","198.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14670551511899718\n","episode  624  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14628361106890664\n","episode  625  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14588859448767824\n","episode  626  is  fully covered\n","175.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.145479513932252\n","episode  627  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14498688339472215\n","episode  628  is  fully covered\n","211.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14459652519638178\n","episode  629  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14425683380279378\n","episode  630  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14383046491200197\n","episode  631  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1434845378158714\n","episode  632  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14311997706955493\n","episode  633  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14273578715376414\n","episode  634  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14235490621403568\n","episode  635  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14199889544038044\n","episode  636  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1415735371105727\n","episode  637  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14124659718096555\n","episode  638  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.14081560646921143\n","episode  639  is  fully covered\n","191.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.14039154640412496\n","episode  640  is  fully covered\n","188.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13999340004777067\n","episode  641  is  fully covered\n","176.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13966563980313415\n","episode  642  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1393208127234083\n","episode  643  is  fully covered\n","154.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1389846199243012\n","episode  644  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13851176653942845\n","episode  645  is  fully covered\n","212.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13808138793839594\n","episode  646  is  fully covered\n","194.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13765124537935186\n","episode  647  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1372894236677602\n","episode  648  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13688583797411766\n","episode  649  is  fully covered\n","183.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1365030937198057\n","episode  650  is  fully covered\n","175.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13610181957725717\n","episode  651  is  fully covered\n","183.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13562032843493763\n","episode  652  is  fully covered\n","221.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1352627629619677\n","episode  653  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13484355828796776\n","episode  654  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1345192456287616\n","episode  655  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13406372933996596\n","episode  656  is  fully covered\n","212.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13368459796189974\n","episode  657  is  fully covered\n","176.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13329480830570756\n","episode  658  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13290083903529293\n","episode  659  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1325430209287922\n","episode  660  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13214070197981032\n","episode  661  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13181023557865623\n","episode  662  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13131450914611434\n","episode  663  is  fully covered\n","241.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.130963057417275\n","episode  664  is  fully covered\n","160.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.13055404500258477\n","episode  665  is  fully covered\n","195.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.13019317108593093\n","episode  666  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1298821211846855\n","episode  667  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12958218058544638\n","episode  668  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1292415687279801\n","episode  669  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12883896342441573\n","episode  670  is  fully covered\n","195.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1285362902656062\n","episode  671  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12819022313515524\n","episode  672  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1278082736613495\n","episode  673  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12748252278188116\n","episode  674  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12715760215879948\n","episode  675  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12676757339587855\n","episode  676  is  fully covered\n","191.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12646976642153987\n","episode  677  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1260848734483592\n","episode  678  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12579370194490552\n","episode  679  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12540284044039693\n","episode  680  is  fully covered\n","194.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12505020278856097\n","episode  681  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12456594806342165\n","episode  682  is  fully covered\n","242.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12421963875025406\n","episode  683  is  fully covered\n","177.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12387032830746854\n","episode  684  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12353682366860898\n","episode  685  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.123217030852193\n","episode  686  is  fully covered\n","161.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12290494834982278\n","episode  687  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12255443259209307\n","episode  688  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12224794018152317\n","episode  689  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12186517113667125\n","episode  690  is  fully covered\n","195.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12157888106326153\n","episode  691  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12125445591807615\n","episode  692  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12096669732340073\n","episode  693  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.12065452288059765\n","episode  694  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12033641502731622\n","episode  695  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.12001338509045811\n","episode  696  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11971516293055645\n","episode  697  is  fully covered\n","154.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11931645855567898\n","episode  698  is  fully covered\n","208.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11902092035730007\n","episode  699  is  fully covered\n","154.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11866154474392844\n","episode  700  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11827202630404735\n","episode  701  is  fully covered\n","205.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11794132782432393\n","episode  702  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11766613842888014\n","episode  703  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11727707282669042\n","episode  704  is  fully covered\n","206.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11696319106094963\n","episode  705  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11664828298467637\n","episode  706  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11637890367688936\n","episode  707  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11608599805571192\n","episode  708  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11581791724903492\n","episode  709  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11552272674750273\n","episode  710  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11522275778659088\n","episode  711  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11491437421428756\n","episode  712  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11463249080573484\n","episode  713  is  fully covered\n","153.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11436776662606378\n","episode  714  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11410000252321965\n","episode  715  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11381283250305689\n","episode  716  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.113509130541122\n","episode  717  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11324337672428533\n","episode  718  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11297734128254948\n","episode  719  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1127164393883115\n","episode  720  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11242915376736201\n","episode  721  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1121704151783942\n","episode  722  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11186393637665079\n","episode  723  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11157079018360554\n","episode  724  is  fully covered\n","163.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11127841219784204\n","episode  725  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11096726844821815\n","episode  726  is  fully covered\n","174.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.11064283149369128\n","episode  727  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11038820385361034\n","episode  728  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.11011918497271145\n","episode  729  is  fully covered\n","152.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10978932245896077\n","episode  730  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10950511702450885\n","episode  731  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10925310766115183\n","episode  732  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10900167825899612\n","episode  733  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10868211867465034\n","episode  734  is  fully covered\n","183.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10835829461830157\n","episode  735  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1080959521996137\n","episode  736  is  fully covered\n","151.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10782303075051324\n","episode  737  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10755510049345629\n","episode  738  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1072277716497254\n","episode  739  is  fully covered\n","189.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1069407859543065\n","episode  740  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10666224775622489\n","episode  741  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10641252423778665\n","episode  742  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10614470227411302\n","episode  743  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.1058385986004061\n","episode  744  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10556884285151671\n","episode  745  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10532589222418307\n","episode  746  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10508350071074826\n","episode  747  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10478464866803454\n","episode  748  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10448664654507821\n","episode  749  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10422367168327674\n","episode  750  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10396468550053693\n","episode  751  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10370219470375217\n","episode  752  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10344202170475383\n","episode  753  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1031998375565863\n","episode  754  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10278621811271554\n","episode  755  is  fully covered\n","250.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10247832153416761\n","episode  756  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10218933100084586\n","episode  757  is  fully covered\n","169.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.1019435551247109\n","episode  758  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10170487926888205\n","episode  759  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10144971722985537\n","episode  760  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10113045114881901\n","episode  761  is  fully covered\n","196.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10079928667568677\n","episode  762  is  fully covered\n","205.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.10056650792082338\n","episode  763  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10027809526888778\n","episode  764  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.10000330935416249\n","episode  765  is  fully covered\n","171.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0997524162946785\n","episode  766  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09946315548773364\n","episode  767  is  fully covered\n","187.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09920409354183676\n","episode  768  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09893699951018181\n","episode  769  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09870931106112572\n","episode  770  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09848214660238767\n","episode  771  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09821306771705221\n","episode  772  is  fully covered\n","170.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09798390975157643\n","episode  773  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09775450443518537\n","episode  774  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09749365305198003\n","episode  775  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09721716387777735\n","episode  776  is  fully covered\n","176.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09696938197974064\n","episode  777  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0967090782858756\n","episode  778  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09644947334878984\n","episode  779  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09622443033162746\n","episode  780  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09598916101331288\n","episode  781  is  fully covered\n","152.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09573608383709754\n","episode  782  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0954393797558254\n","episode  783  is  fully covered\n","193.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09517480743390944\n","episode  784  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09493830684174823\n","episode  785  is  fully covered\n","154.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09469178785751152\n","episode  786  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09440511716910532\n","episode  787  is  fully covered\n","189.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09418785815763739\n","episode  788  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09395155517874566\n","episode  789  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09371209648917767\n","episode  790  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0934455841321154\n","episode  791  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09321039771944306\n","episode  792  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09296018460745202\n","episode  793  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.092745508921359\n","episode  794  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09251356468239824\n","episode  795  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09229032169624297\n","episode  796  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09199031308786042\n","episode  797  is  fully covered\n","207.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09174851293240543\n","episode  798  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09153736769133078\n","episode  799  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0913040621534696\n","episode  800  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09107499417917014\n","episode  801  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09081307553981292\n","episode  802  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09056639960569894\n","episode  803  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.09034640973796683\n","episode  804  is  fully covered\n","151.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.09011613964992127\n","episode  805  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08990803180042535\n","episode  806  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08968031390012046\n","episode  807  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08945603530809762\n","episode  808  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08920091335881479\n","episode  809  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08899563103048863\n","episode  810  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08875531190207055\n","episode  811  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08849156873833272\n","episode  812  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08828156235738809\n","episode  813  is  fully covered\n","151.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08802204363680492\n","episode  814  is  fully covered\n","181.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08774854482188912\n","episode  815  is  fully covered\n","193.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08741713175725044\n","episode  816  is  fully covered\n","236.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08721246596243151\n","episode  817  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08698322456603372\n","episode  818  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08671850333765355\n","episode  819  is  fully covered\n","190.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08649886383379403\n","episode  820  is  fully covered\n","161.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08629427685082391\n","episode  821  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08609292868631785\n","episode  822  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08589411175612846\n","episode  823  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08564640741860585\n","episode  824  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08542811644462839\n","episode  825  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0852103818389886\n","episode  826  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08496804791881309\n","episode  827  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08474131634305664\n","episode  828  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08449761245403695\n","episode  829  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08430180521736418\n","episode  830  is  fully covered\n","144.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08408021460719603\n","episode  831  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08388000611891354\n","episode  832  is  fully covered\n","149.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08364413228790717\n","episode  833  is  fully covered\n","175.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08343294697219238\n","episode  834  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08320298952580209\n","episode  835  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08293517503650352\n","episode  836  is  fully covered\n","194.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08269269600653129\n","episode  837  is  fully covered\n","182.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08249975131579872\n","episode  838  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08228289747517631\n","episode  839  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0820849986238988\n","episode  840  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08188430030224035\n","episode  841  is  fully covered\n","152.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08167559798529346\n","episode  842  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08145439385194084\n","episode  843  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08124613728582032\n","episode  844  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08105851319344257\n","episode  845  is  fully covered\n","144.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08084997517574069\n","episode  846  is  fully covered\n","160.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08061681728947619\n","episode  847  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.08040169668279852\n","episode  848  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.08019998108159765\n","episode  849  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07999109204064563\n","episode  850  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0798044509313278\n","episode  851  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07960359689735559\n","episode  852  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07941976597176552\n","episode  853  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07920467136656877\n","episode  854  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0790027987442687\n","episode  855  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07879450642242485\n","episode  856  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07860248219158536\n","episode  857  is  fully covered\n","152.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07842159053981948\n","episode  858  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07824111518266369\n","episode  859  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07804169842073426\n","episode  860  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07780232219332417\n","episode  861  is  fully covered\n","191.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07760774888599765\n","episode  862  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0774155201309403\n","episode  863  is  fully covered\n","154.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07723365278742766\n","episode  864  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07698566833280113\n","episode  865  is  fully covered\n","200.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07678945138653115\n","episode  866  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07660905482638614\n","episode  867  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07643275074435991\n","episode  868  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07622757538957443\n","episode  869  is  fully covered\n","168.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07597127277155015\n","episode  870  is  fully covered\n","209.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07576794234499402\n","episode  871  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07553191475937675\n","episode  872  is  fully covered\n","194.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07535748668900057\n","episode  873  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07516782489677153\n","episode  874  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07496604509699394\n","episode  875  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07472773280390697\n","episode  876  is  fully covered\n","199.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07454144496676382\n","episode  877  is  fully covered\n","155.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07436216510686415\n","episode  878  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07419103189961652\n","episode  879  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07399365002003278\n","episode  880  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07379738364270606\n","episode  881  is  fully covered\n","165.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07362696117600953\n","episode  882  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07340464955000388\n","episode  883  is  fully covered\n","188.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07322517473470767\n","episode  884  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07303211522409102\n","episode  885  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07283723388962272\n","episode  886  is  fully covered\n","166.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0726661220219519\n","episode  887  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07249889199977606\n","episode  888  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07229270889945993\n","episode  889  is  fully covered\n","178.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07212576122236516\n","episode  890  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07193272296908841\n","episode  891  is  fully covered\n","167.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07170520068830379\n","episode  892  is  fully covered\n","197.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07152301449684638\n","episode  893  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07132588314324204\n","episode  894  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07115889106432995\n","episode  895  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.07099456174608085\n","episode  896  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07081814682761796\n","episode  897  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07065460440004799\n","episode  898  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07049200357958485\n","episode  899  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07030333793487814\n","episode  900  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.07012639656304812\n","episode  901  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06996221278806201\n","episode  902  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06980064699412251\n","episode  903  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06961383170448156\n","episode  904  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06945307043721612\n","episode  905  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06927660638347218\n","episode  906  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06911662388055043\n","episode  907  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06894542702517144\n","episode  908  is  fully covered\n","155.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06876365114755885\n","episode  909  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06860485322723735\n","episode  910  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06844696959718627\n","episode  911  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0682894493122845\n","episode  912  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.068095782413404\n","episode  913  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06793852682197664\n","episode  914  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0677664529997841\n","episode  915  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06759319274785032\n","episode  916  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06742199358333507\n","episode  917  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.067263603394306\n","episode  918  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06710773271433315\n","episode  919  is  fully covered\n","144.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06693722761675018\n","episode  920  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06678264750996067\n","episode  921  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06661030391261837\n","episode  922  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06645701043430283\n","episode  923  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06630406973760085\n","episode  924  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06615148101063924\n","episode  925  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06597812706431215\n","episode  926  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06580417452724292\n","episode  927  is  fully covered\n","165.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0656390818840036\n","episode  928  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06546602324574477\n","episode  929  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0653127505929834\n","episode  930  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06516244323618335\n","episode  931  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0650124817888926\n","episode  932  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06484626268674158\n","episode  933  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06468098600710805\n","episode  934  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06451148558163979\n","episode  935  is  fully covered\n","163.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06431978479280698\n","episode  936  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06416354913575609\n","episode  937  is  fully covered\n","151.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0640133259069874\n","episode  938  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06385272622995397\n","episode  939  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06370475961274441\n","episode  940  is  fully covered\n","144.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06355561052722994\n","episode  941  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06337283369388687\n","episode  942  is  fully covered\n","179.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06320271625718236\n","episode  943  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06305625591515479\n","episode  944  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06287541815549093\n","episode  945  is  fully covered\n","179.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06272871259748453\n","episode  946  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06258385133810276\n","episode  947  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.062433330723632795\n","episode  948  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.062256271607330146\n","episode  949  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.062092131810151455\n","episode  950  is  fully covered\n","164.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06194923621739615\n","episode  951  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06180666947710304\n","episode  952  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06160427570845147\n","episode  953  is  fully covered\n","205.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.061440380323545866\n","episode  954  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06129408091504\n","episode  955  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.061152043474456706\n","episode  956  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06099959830466099\n","episode  957  is  fully covered\n","156.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06084363904602666\n","episode  958  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.060671088249244505\n","episode  959  is  fully covered\n","177.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.06049612291792793\n","episode  960  is  fully covered\n","180.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06033227970439353\n","episode  961  is  fully covered\n","169.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.06019102645424997\n","episode  962  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.060040016343903456\n","episode  963  is  fully covered\n","157.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05988938509490505\n","episode  964  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05973722013511926\n","episode  965  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0595997440132354\n","episode  966  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05938794614843436\n","episode  967  is  fully covered\n","222.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.059248429834594205\n","episode  968  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05910971415335288\n","episode  969  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05896283198172931\n","episode  970  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.058805493592373186\n","episode  971  is  fully covered\n","166.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05864294505706722\n","episode  972  is  fully covered\n","172.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.058507519180733124\n","episode  973  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05837287302847129\n","episode  974  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05821990339357665\n","episode  975  is  fully covered\n","164.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.058084989778010875\n","episode  976  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05795131601287276\n","episode  977  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05779575204706712\n","episode  978  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05766274391855326\n","episode  979  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.057516696466417966\n","episode  980  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.057382035214544054\n","episode  981  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05723349357240048\n","episode  982  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05710177939620677\n","episode  983  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05695943107943586\n","episode  984  is  fully covered\n","155.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05681425593359744\n","episode  985  is  fully covered\n","159.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05668305310279507\n","episode  986  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05652591915021635\n","episode  987  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05637869162121974\n","episode  988  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.056247144704211466\n","episode  989  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05610333616723462\n","episode  990  is  fully covered\n","159.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05596840185985098\n","episode  991  is  fully covered\n","150.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.055827539048850607\n","episode  992  is  fully covered\n","157.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.055698614872346376\n","episode  993  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05557043298591721\n","episode  994  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.055440328433103966\n","episode  995  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.055312298453453465\n","episode  996  is  fully covered\n","144.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05515808190265622\n","episode  997  is  fully covered\n","173.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.055018378226085934\n","episode  998  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05485532576905155\n","episode  999  is  fully covered\n","185.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05472689546456901\n","episode  1000  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05459265112883175\n","episode  1001  is  fully covered\n","153.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05446657871529029\n","episode  1002  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05434079744447828\n","episode  1003  is  fully covered\n","144.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.054194491962974455\n","episode  1004  is  fully covered\n","168.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.054067608838929505\n","episode  1005  is  fully covered\n","145.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.053943180464694845\n","episode  1006  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.053808275712049464\n","episode  1007  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05368186736206753\n","episode  1008  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.053557898248243474\n","episode  1009  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05343250555240159\n","episode  1010  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0532984515407842\n","episode  1011  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0531753678603835\n","episode  1012  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05301820149249364\n","episode  1013  is  fully covered\n","184.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05289449551863885\n","episode  1014  is  fully covered\n","146.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0527575705177162\n","episode  1015  is  fully covered\n","161.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.0526012181900498\n","episode  1016  is  fully covered\n","185.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.052466731288165466\n","episode  1017  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.052334681577835\n","episode  1018  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05221424127199093\n","episode  1019  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05208366036736714\n","episode  1020  is  fully covered\n","156.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.051948003155025166\n","episode  1021  is  fully covered\n","162.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.051816015394614716\n","episode  1022  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05168353604112835\n","episode  1023  is  fully covered\n","160.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05153861224016676\n","episode  1024  is  fully covered\n","174.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.051408075909552736\n","episode  1025  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.051277870199668854\n","episode  1026  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.05115986198439586\n","episode  1027  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05102987670688525\n","episode  1028  is  fully covered\n","158.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.050912439210898545\n","episode  1029  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.0507952719794383\n","episode  1030  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05065608081074724\n","episode  1031  not  fully covered\n","393.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.050341373760846235\n","episode  1032  is  fully covered\n","167.8\n","chosen actions  [1, 1, 1]\n","Explore P  0.05021628010396738\n","episode  1033  is  fully covered\n","155.3\n","chosen actions  [1, 1, 1]\n","Explore P  0.050088692247947444\n","episode  1034  is  fully covered\n","158.3\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1035  is  fully covered\n","146.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1036  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1037  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1038  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1039  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1040  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1041  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1042  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1043  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1044  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1045  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1046  is  fully covered\n","143.8\n","chosen actions  [1, 1, 1]\n","Explore P  0\n","episode  1047  is  fully covered\n","143.8\n","[[1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 1. 1. 1. 1. 1. 1. 1.]\n"," [1. 2. 1. 1. 1. 1. 1. 1.]]\n","\n","mean T =  177.26179883945846\n","min T =  143.8\n","diag_count =  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AF6Kn3utA4Sn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":476},"executionInfo":{"status":"ok","timestamp":1596720760146,"user_tz":-60,"elapsed":764,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"72358d02-7614-4c9e-fc38-58ea31a58a0e"},"source":["# print(np.min(Ts))\n","# print(env.index_to_pos([2, 1]))\n","x = T_memory[2]\n","print(len(T_memory[1]))\n","# print(T_memory[0])\n","print(len(Ts))\n","x = sorted(T_memory[0].items())\n","# memory = [v for k, v in sorted(T_memory[0].items(), key=lambda item: item[1])]\n","# y = [k for k, v in sorted(T_memory[0].items(), key=lambda item: item[1])]\n","first = 0\n","second = 3\n","print(x[first][0])\n","print(x[second][0])\n","print(len(x[first][1]))\n","print(len(x[second][1]))\n","for i in range(len(x[first][1])):\n","    if i == len(x[second][1]):\n","        break\n","    else:\n","        if x[first][1][i][0] == x[second][1][i][0]:\n","            print(\"state \", i, \" is the same\")\n","        else:\n","            print(\"state \", i, \" is different\")\n","# print(x[0][1][12][0], x[0][1][12][1])\n","# print(x[1][1][12][0], x[1][1][12][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["185\n","999\n","142.8\n","147.3\n","21\n","21\n","state  0  is the same\n","state  1  is the same\n","state  2  is the same\n","state  3  is different\n","state  4  is different\n","state  5  is different\n","state  6  is different\n","state  7  is different\n","state  8  is different\n","state  9  is different\n","state  10  is different\n","state  11  is different\n","state  12  is different\n","state  13  is different\n","state  14  is different\n","state  15  is different\n","state  16  is different\n","state  17  is different\n","state  18  is different\n","state  19  is different\n","state  20  is different\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vPLUSCunwIeO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1596046922744,"user_tz":-60,"elapsed":511,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"7de6520b-4eb6-4974-cc3d-78d1deda05f6"},"source":["print(env.uavs[0].Q[o_obs_n[0]])\n","print(env.uavs[1].Q[o_obs_n[1]])\n","print(env.uavs[2].Q[o_obs_n[2]])\n","# print(memory[0][0][0])\n","# print(o_obs_n[0])\n","# print(list(range(10, 70, 10)))\n","# Ts1 = Ts_s\n","\n","x = {}\n","for i in range(9):\n","    if env.uavs[0].Q[o_obs_n[0]][i] == 0:\n","        continue\n","    for j in range(9):\n","        if env.uavs[1].Q[o_obs_n[1]][j] == 0:\n","            continue\n","        for k in range(9):\n","            if env.uavs[2].Q[o_obs_n[2]][k] == 0:\n","                continue\n","            # x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = str(i + 1) + str(j + 1) + str(k + 1)\n","            if i == 3 and j == 0 and k ==0:\n","                print(env.uavs[0].Q[o_obs_n[0]][3])\n","                print(env.uavs[1].Q[o_obs_n[1]][0])\n","                print(env.uavs[2].Q[o_obs_n[2]][0])\n","            x[env.uavs[0].Q[o_obs_n[0]][i] + env.uavs[1].Q[o_obs_n[1]][j] + env.uavs[2].Q[o_obs_n[2]][k]] = [i + 1, j + 1, k + 1]\n","\n","# x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n","# x = {k:v for k, v in sorted(x.items(), key=lambda item: item[1])}\n","x = x.items()\n","x = sorted(x, reverse=True)\n","print(x)\n","print(env.uavs[0].Q[o_obs_n[0]][3])\n","print(env.uavs[1].Q[o_obs_n[1]][0])\n","print(env.uavs[2].Q[o_obs_n[2]][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[41.19808398  0.          0.         57.64887133  0.         48.93034292\n","  0.         48.20225515  0.        ]\n","[57.64887133 38.72599612  0.         48.93034292  0.         15.74273432\n","  0.          0.          0.        ]\n","[57.64887133 41.19808398  0.         26.88380199  0.         36.21111406\n","  0.         48.93034292  0.        ]\n","57.64887133293179\n","57.64887133293179\n","57.64887133293179\n","[(172.94661399879539, [4, 1, 1]), (164.2280855832882, [6, 1, 1]), (163.49999781337817, [8, 1, 1]), (156.49582664547322, [4, 1, 2]), (156.4958266454732, [1, 1, 1]), (155.509557167781, [6, 4, 1]), (154.78146939787098, [8, 4, 1]), (154.02373878189536, [4, 2, 1]), (151.5088567271109, [4, 1, 6]), (147.77729822996605, [1, 4, 1]), (147.77729822996602, [6, 1, 2]), (147.049210460056, [8, 1, 2]), (146.79102875227383, [6, 4, 8]), (146.0629409823638, [8, 4, 8]), (145.3052103663882, [6, 2, 1]), (144.57712259647815, [8, 2, 1]), (142.79032831160373, [6, 1, 6]), (142.18154465972486, [4, 1, 4]), (142.06224054169368, [8, 1, 6]), (140.04503929215105, [1, 1, 2]), (139.05876981445886, [6, 4, 2]), (138.33068204454884, [8, 4, 2]), (137.57295142857322, [4, 2, 2]), (136.58668195088103, [6, 2, 8]), (135.85859418097098, [8, 2, 8]), (135.05806937378873, [1, 1, 6]), (134.07179989609654, [6, 4, 6]), (133.46301624421767, [6, 1, 4]), (133.34371212618652, [8, 4, 6]), (132.73492847430765, [8, 1, 4]), (132.5859815102109, [4, 2, 6]), (131.32651087664388, [1, 4, 2]), (131.04047698372665, [4, 6, 1]), (128.85442301306603, [6, 2, 2]), (128.126335243156, [8, 2, 2]), (126.33954095828156, [1, 4, 6]), (125.73075730640268, [1, 1, 4]), (124.74448782871049, [6, 4, 4]), (124.01640005880047, [8, 4, 4]), (123.86745309470373, [6, 2, 6]), (123.25866944282485, [4, 2, 4]), (123.13936532479369, [8, 2, 6]), (122.32194856821948, [6, 6, 1]), (121.59386079830946, [8, 6, 1]), (121.12216407525105, [1, 2, 2]), (117.01222889089551, [1, 4, 4]), (116.13519415688873, [1, 2, 6]), (114.58968963040451, [4, 6, 2]), (114.54014102731767, [6, 2, 4]), (113.81205325740764, [8, 2, 4]), (113.6034201527123, [6, 6, 8]), (112.87533238280228, [8, 6, 8]), (109.60271971204219, [4, 6, 6]), (106.80788208950268, [1, 2, 4]), (105.87116121489733, [1, 6, 8]), (105.87116121489731, [6, 6, 2]), (105.1430734449873, [8, 6, 2]), (100.88419129653502, [6, 6, 6]), (100.27540764465614, [4, 6, 4]), (100.156103526625, [8, 6, 6]), (98.13890227708234, [1, 6, 2]), (93.15193235872005, [1, 6, 6]), (91.55687922914896, [6, 6, 4]), (90.82879145923894, [8, 6, 4]), (83.82462029133399, [1, 6, 4])]\n","57.64887133293179\n","57.64887133293179\n","57.64887133293179\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7tw9473yvpbi","colab_type":"code","colab":{}},"source":["Ts2 = Ts_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmfC7uno3Okw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595946319345,"user_tz":-60,"elapsed":1437,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"de805311-e78b-4179-b3f0-fba00b4c277f"},"source":["print(len(Ts1[1]))\n","print(len(Ts3[1]))\n","print(Ts3[0][-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100\n","1292\n","184.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oCMG2wGZvp_W","colab_type":"code","colab":{}},"source":["Ts3 = Ts_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1JLq8HQwN0d","colab_type":"code","colab":{}},"source":["mean_Ts = [[251.477, 204.340, 204.6, 200.795, 201.03, 200.542], [240.553, 200.585, 190.335, 191.698, 193.635, 190.685], [156.45, 157.4, 156.2, 155.35, 154.618, 152.613], [149.3, 142.65, 142, 142.15, 142.65, 142.4]]\n","var_Ts = [[956.005, 437.973, 379.03, 392.477, 336.167, 453.987], [1300.926, 590.036, 483.415, 508.617, 672.72, 551.694], [131.453, 36.073, 41.990, 46.6225, 51.467, 41.934], [25.05, 4.702, 0.41, 1.652, 4.402, 1.44]]\n","# Ts_s = [Ts1, Ts2, Ts3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLJ8JxiWD_ff","colab_type":"code","colab":{}},"source":["filename = 'rc_60_sec_convergence'\n","with open(filename) as f:\n","    content = f.readlines()\n","# you may also want to remove whitespace characters like `\\n` at the end of each line\n","# content = [[float(strn) for strn in x.strip().split(\",\")] for x in content]\n","Ts = [float(x.strip()) for x in content]  \n","# cac_T = [content[i][0] for i in range(len(content))]\n","# cac = [content[i][1] for i in range(len(content))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8u4szrcKC5g5","colab_type":"code","colab":{}},"source":["# cac_Ts = []\n","# cacs = []\n","Ts_s = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AO6NC9zRDvUQ","colab_type":"code","colab":{}},"source":["# cac_Ts.append(cac_T) \n","# cacs.append(cac) \n","Ts_s.append(Ts)\n","# np.concatenate((Ts_s, Ts))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5-XWLWry79Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600190703560,"user_tz":-60,"elapsed":1338,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"fc7e5f9c-c8c0-4fee-9ac7-faf2b4bf0921"},"source":["print(len(Ts_s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fn4nhmeWqK-_","colab_type":"code","colab":{}},"source":["Ts_sc = np.copy(Ts_s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_q6zgD85qY0k","colab_type":"code","colab":{}},"source":["Ts_s = np.copy(Ts_sc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46xg7wO8pfot","colab_type":"code","colab":{}},"source":["et_len = 11\n","for i in range(len(Ts_s)):\n","# for j in range(et_len):\n","    while len(Ts_s[i]) < 3100:\n","        Ts_s[i].append(Ts_s[i][-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTEQHRWbT03X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"status":"ok","timestamp":1600190802183,"user_tz":-60,"elapsed":1609,"user":{"displayName":"Zefeng Liao","photoUrl":"","userId":"06427631148021087626"}},"outputId":"5d4c078c-e226-4e25-c5ef-1edec58124e4"},"source":["# plt.plot(mean_Ts)\n","# plt.grid()\n","# plt.show()\n","\n","# log_episode_interval = 10\n","# episodes = np.arange(log_episode_interval - 1, len(final_rs) - 1, log_episode_interval)\n","# episode_rewards = [np.mean(final_rs[episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","# plt.plot(episode_rewards)\n","# plt.grid()\n","# plt.show()\n","\n","# log_episode_interval = 50\n","# episodes = np.arange(log_episode_interval - 1, len(Ts) - 1, log_episode_interval)\n","# min_Ts = [np.min(Ts[episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","# plt.plot(min_Ts)\n","# plt.grid()\n","# plt.show()\n","\n","# xs_ = [[] for _ in range(env.num_agents)]\n","# ys_ = [[] for _ in range(env.num_agents)]\n","# for i in range(env.num_agents):\n","#     for pos in pos_tra[i]:\n","#         xs_[i].append(pos[0])\n","#         ys_[i].append(pos[1])\n","#         # print(pos)\n","# fig, ax = plt.subplots()\n","# ax.set_xlabel(\"x (m)\")\n","# ax.set_ylabel(\"y (m)\")\n","# colors = ['blue', 'orange', 'green']\n","# for i in range(env.num_agents):\n","#     ax.plot(xs_[i], ys_[i], label='uav ' + str(i), color=colors[i])\n","#     # ax.plot(xs_[i][0], ys_[i][0], label='uav ' + str(i), color=colors[i], marker='o', fillstyle='none')\n","#     ax.plot(xs_[i][0], ys_[i][0], label='uav ' + str(i), color=colors[i], marker='o')\n","#     ax.plot(xs_[i][-1], ys_[i][-1], label='uav ' + str(i), color=colors[i], marker='x')\n","    # marker='o', markerfacecolor='none'\n","    # ax.scatter(xs_[i], ys_[i])\n","# ax.plot(a, d, 'k:', label='Data length')\n","# ax.plot(a, c + d, 'k', label='Total message length')\n","\n","# legend = ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n","\n","# print(env.uavs[0].tra_map)\n","\n","# Section A\n","# r_c_set = np.array(range(10, 70, 10))\n","# labels = [\"Untrained MC-Q algorithm\", \"Untrained MI-Q algorithm\", \"Trained MC-Q algorithm\", \"Trained MI-Q algorithm\"]\n","# fig, ax = plt.subplots()\n","# ax.set_xlabel(\"time(s)\")\n","# # ax.set_ylabel(r'$\\overline{T}$', rotation=0)\n","# ax.set_ylabel('cumulative area coverage(%)')\n","# for i in range(4):\n","#     # if i != 2:\n","#     #     # tmp = [np.mean(Ts_s[i][j]) for j in range(len(r_c_set))]\n","#     #     tmp = [np.var(Ts_s[i][j]) for j in range(len(r_c_set))]\n","#     # else:\n","#     #     # tmp = [Ts_s[i][j][-1] for j in range(len(r_c_set))]\n","#     #     # tmp[0] = 164.8\n","#     #     tmp = [0 for j in range(len(r_c_set))]\n","#     ax.plot(cac_Ts[i], np.array(cacs[i]) * 100, label=labels[i])\n","#     # ax.scatter(xs_[i], ys_[i])\n","# # ax.plot(a, d, 'k:', label='Data length')\n","# # ax.plot(a, c + d, 'k', label='Total message length')\n","\n","# legend = ax.legend(loc='bottom right', shadow=True)\n","\n","# Section B\n","# r_c_set = np.array(range(10, 70, 10))\n","# labels = [\"Untrained MC-Q algorithm\", \"Untrained MI-Q algorithm\", \"Trained MC-Q algorithm\", \"Trained MI-Q algorithm\"]\n","# fig, ax = plt.subplots()\n","# ax.set_xlabel(\"$r_c$\")\n","# # ax.set_ylabel(r'$\\overline{T}$', rotation=0)\n","# ax.set_ylabel(r'$s^2$', rotation=0)\n","# for i in range(4):\n","    # if i != 2:\n","    #     # tmp = [np.mean(Ts_s[i][j]) for j in range(len(r_c_set))]\n","    #     tmp = [np.var(Ts_s[i][j]) for j in range(len(r_c_set))]\n","    # else:\n","    #     # tmp = [Ts_s[i][j][-1] for j in range(len(r_c_set))]\n","    #     # tmp[0] = 164.8\n","    #     tmp = [0 for j in range(len(r_c_set))]\n","    # ax.plot(r_c_set, mean_Ts[i], label=labels[i], marker='o', markerfacecolor='none')\n","    # ax.plot(r_c_set, var_Ts[i], label=labels[i], marker='o', markerfacecolor='none')\n","    # ax.scatter(xs_[i], ys_[i])\n","# ax.plot(a, d, 'k:', label='Data length')\n","# ax.plot(a, c + d, 'k', label='Total message length')\n","\n","# legend = ax.legend(loc='upper right', shadow=True)\n","# ax.grid()\n","\n","# Section C\n","# mean T\n","fig, ax = plt.subplots()\n","labels = \"rc=\"\n","ax.set_xlabel(\"$iterations$\")\n","# ax.set_ylabel(r'$\\overline{T}$', rotation=0)\n","ax.set_ylabel('$T$', rotation=0)\n","tmp = [[] for _ in range(3)]\n","for i in range(3):\n","    print(len(Ts_s[0]))\n","    print(len(Ts_s[1]))\n","    print(len(Ts_s[2]))\n","    # if i in [2]:\n","    for j in range(0, len(Ts_s[i]) - 99):\n","        tmp[i].append(np.mean(Ts_s[i][j:j+100]))\n","    ax.plot(tmp[i], label= labels + str((i + 1) * 20))\n","    # ax.set_ylim([138,250])\n","    # else:\n","        # ax.plot(Ts_s[i], label= labels + str((i + 1) * 20))\n","    # r_c_set = np.array(range(len(Ts_s[2][i * 2 + 1]))) + 1\n","legend = ax.legend(loc='upper right', shadow=True)\n","\n","# learning curves\n","# log_episode_interval = 10\n","# fig, ax = plt.subplots()\n","# labels = \"rc=\"\n","# ax.set_xlabel(\"$episode$\")\n","# ax.set_ylabel(\"$r$\", rotation=0)\n","# for i in range(3):\n","#     episodes = np.arange(log_episode_interval - 1, len(final_rs_s[i * 2 + 1]) - 1, log_episode_interval)\n","#     episode_rewards = [np.mean(final_rs_s[i * 2 + 1][episode - log_episode_interval + 1: episode + 1]) for episode in episodes]\n","#     ax.plot(episode_rewards, label = labels + str((i + 1) * 20))\n","# legend = ax.legend(loc='upper left', shadow=True)\n","\n","\n","# plt.xlabel(\"$r_c$\")\n","# plt.ylabel(r'$\\overline{T}$', rotation=0)\n","# plt.show()\n","\n","# plt.plot(r_c_set, var_Ts) \n","# plt.xlabel(\"$r_c$\")\n","# plt.ylabel(r'$s^2$', rotation=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3100\n","3100\n","3100\n","3100\n","3100\n","3100\n","3100\n","3100\n","3100\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}